{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial objective**:  \n",
    "The tutorial shows how to do \n",
    "- Post-training static quantization\n",
    "- Two more advanced techniques\n",
    "    - Per-channel quantization\n",
    "    - Quantization-aware training\n",
    "\n",
    "**Task**\n",
    "- Classify MNIST digits with a simple LeNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Initial Setup](#intial-setup)\n",
    "- [Train CNN](#train-cnn)\n",
    "- [Post-training quantization](#post-training-quantization)\n",
    "- [Quantization aware training](#quantization-aware-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the MNIST dataset, and train a simple convolutional neural network (CNN) to classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaushik/miniconda3/envs/py3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.quantization\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from collections import OrderedDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 2\n",
    "\n",
    "def load_data(train=True):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "        root=\"~/data\",\n",
    "        train=train,\n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    # Shuffle train dataset only\n",
    "    shuffle = train\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper functions and classes that help us to track the statistics and accuracy wrt to the train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=\":f\") -> None:\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    \"\"\"Computes the top 1 accuracy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(1, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        correct_one = correct[:1].view(-1).float().sum(0, keepdim=True)\n",
    "\n",
    "        return correct_one.mul_(100.0/batch_size).item()\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print(\"Size (MB): \", os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove(\"temp.p\")\n",
    "\n",
    "def load_model(quantized_model, model):\n",
    "    \"\"\"Loads in the weights into an object meant for quantization\"\"\"\n",
    "    state_dict = model.state_dict()\n",
    "    model = model.to(\"cpu\")\n",
    "    '''\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, value in state_dict:\n",
    "        key_split = key.split(\".\")\n",
    "        if len(key_split) == 3:\n",
    "            key = \".\".join([key_split[0], key_split[2]])\n",
    "        new_state_dict[key] = value\n",
    "    quantized_model.load_state_dict(new_state_dict)\n",
    "    '''\n",
    "    quantized_model.load_state_dict(state_dict)\n",
    "\n",
    "def fuse_modules(model):\n",
    "    \"\"\"Fuse together convolutions/linear layers and ReLU\"\"\"\n",
    "    torch.quantization.fuse_modules(\n",
    "        model=model,\n",
    "        modules_to_fuse=[[\"conv1\", \"relu1\"],\n",
    "                         [\"conv2\", \"relu2\"],\n",
    "                         [\"fc1\", \"relu3\"],\n",
    "                         [\"fc2\", \"relu4\"]],\n",
    "        inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple CNN to classify MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, q=False) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "        self.q = q\n",
    "        if q:\n",
    "            self.quant = QuantStub()\n",
    "            self.dequant = DeQuantStub()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.q:\n",
    "            x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        # Flatten image tensor\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc3(x)\n",
    "        if self.q:\n",
    "            x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for availability of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the model size without quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB):  0.181134\n"
     ]
    }
   ],
   "source": [
    "net = Net(q=False)\n",
    "if torch.cuda.is_available():\n",
    "    net = net.to(device)\n",
    "print_size_of_model(model=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "n_epoch = 20\n",
    "\n",
    "def train(model: nn.Module, dataloader: DataLoader, cuda: bool=False, q: bool=False):\n",
    "    # Define a loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    # Set train mode\n",
    "    model.train()\n",
    "\n",
    "    # Loop over the dataset\n",
    "    for epoch in range(n_epoch):\n",
    "        running_loss = AverageMeter(name=\"loss\")\n",
    "        acc = AverageMeter(name=\"train_acc\")\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            if cuda:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if q and epoch >= 3:\n",
    "                # Freeze quantizer parameters\n",
    "                model.apply(torch.quantization.disable_observer)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Compute gradient and update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update statistics\n",
    "            running_loss.update(val=loss.item(), n=outputs.shape[0])\n",
    "            acc.update(accuracy(output=outputs, target=labels), n=outputs.shape[0])\n",
    "\n",
    "            # Print every 100th mini-batch\n",
    "            if i % 100 == 99:\n",
    "                print(\"[%d, %5d] \" % (epoch+1, i+1), running_loss, acc)\n",
    "    \n",
    "    print(\"Finished training\")\n",
    "\n",
    "def test(model: nn.Module, dataloader: DataLoader, cuda: bool=False) -> float:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Set eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            if cuda:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # update count\n",
    "            total += outputs.shape[0]\n",
    "            correct += torch.sum(predicted == labels).item()\n",
    "    \n",
    "    return 100*correct/total       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test without quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100]  loss 2.301452 (2.300698) train_acc 6.250000 (10.312500)\n",
      "[1,   200]  loss 2.287138 (2.297951) train_acc 18.750000 (10.281250)\n",
      "[1,   300]  loss 2.307468 (2.294516) train_acc 6.250000 (14.854167)\n",
      "[1,   400]  loss 2.276615 (2.290724) train_acc 37.500000 (19.703125)\n",
      "[1,   500]  loss 2.220406 (2.284420) train_acc 56.250000 (24.037500)\n",
      "[1,   600]  loss 2.199768 (2.273662) train_acc 43.750000 (26.791667)\n",
      "[1,   700]  loss 2.012336 (2.249737) train_acc 37.500000 (29.392857)\n",
      "[1,   800]  loss 1.320231 (2.182185) train_acc 56.250000 (32.304688)\n",
      "[1,   900]  loss 0.652150 (2.059625) train_acc 81.250000 (36.381944)\n",
      "[1,  1000]  loss 0.620984 (1.924953) train_acc 68.750000 (40.487500)\n",
      "[1,  1100]  loss 0.763521 (1.799508) train_acc 81.250000 (44.471591)\n",
      "[1,  1200]  loss 0.261930 (1.686431) train_acc 87.500000 (47.979167)\n",
      "[1,  1300]  loss 0.247878 (1.591323) train_acc 87.500000 (50.908654)\n",
      "[1,  1400]  loss 0.285040 (1.501963) train_acc 81.250000 (53.683036)\n",
      "[1,  1500]  loss 0.270009 (1.423879) train_acc 93.750000 (56.050000)\n",
      "[1,  1600]  loss 0.151503 (1.350205) train_acc 100.000000 (58.343750)\n",
      "[1,  1700]  loss 0.107209 (1.284332) train_acc 93.750000 (60.397059)\n",
      "[1,  1800]  loss 0.457840 (1.230062) train_acc 81.250000 (62.131944)\n",
      "[1,  1900]  loss 0.195339 (1.178546) train_acc 87.500000 (63.726974)\n",
      "[1,  2000]  loss 0.075556 (1.130934) train_acc 100.000000 (65.218750)\n",
      "[1,  2100]  loss 0.302211 (1.088066) train_acc 93.750000 (66.568452)\n",
      "[1,  2200]  loss 0.067822 (1.048541) train_acc 100.000000 (67.795455)\n",
      "[1,  2300]  loss 0.531409 (1.011867) train_acc 81.250000 (68.910326)\n",
      "[1,  2400]  loss 0.140301 (0.978267) train_acc 93.750000 (69.937500)\n",
      "[1,  2500]  loss 0.776275 (0.945457) train_acc 75.000000 (70.927500)\n",
      "[1,  2600]  loss 0.039710 (0.916591) train_acc 100.000000 (71.819712)\n",
      "[1,  2700]  loss 0.622681 (0.888249) train_acc 93.750000 (72.703704)\n",
      "[1,  2800]  loss 0.082128 (0.863100) train_acc 100.000000 (73.466518)\n",
      "[1,  2900]  loss 0.152921 (0.838889) train_acc 93.750000 (74.215517)\n",
      "[1,  3000]  loss 0.162297 (0.815959) train_acc 93.750000 (74.933333)\n",
      "[1,  3100]  loss 0.099868 (0.795162) train_acc 100.000000 (75.576613)\n",
      "[1,  3200]  loss 0.124292 (0.775550) train_acc 93.750000 (76.156250)\n",
      "[1,  3300]  loss 0.067958 (0.755839) train_acc 100.000000 (76.757576)\n",
      "[1,  3400]  loss 0.018598 (0.737587) train_acc 100.000000 (77.316176)\n",
      "[1,  3500]  loss 0.007112 (0.719933) train_acc 100.000000 (77.855357)\n",
      "[1,  3600]  loss 0.055847 (0.704114) train_acc 100.000000 (78.342014)\n",
      "[1,  3700]  loss 0.197862 (0.688703) train_acc 93.750000 (78.812500)\n",
      "[2,   100]  loss 0.364959 (0.132877) train_acc 87.500000 (96.500000)\n",
      "[2,   200]  loss 0.004639 (0.125135) train_acc 100.000000 (96.437500)\n",
      "[2,   300]  loss 0.136289 (0.119680) train_acc 93.750000 (96.604167)\n",
      "[2,   400]  loss 0.422608 (0.115602) train_acc 81.250000 (96.546875)\n",
      "[2,   500]  loss 0.545497 (0.118806) train_acc 87.500000 (96.425000)\n",
      "[2,   600]  loss 0.014179 (0.113799) train_acc 100.000000 (96.583333)\n",
      "[2,   700]  loss 0.030844 (0.115683) train_acc 100.000000 (96.482143)\n",
      "[2,   800]  loss 0.085080 (0.115312) train_acc 93.750000 (96.476562)\n",
      "[2,   900]  loss 0.064877 (0.118277) train_acc 100.000000 (96.409722)\n",
      "[2,  1000]  loss 0.416448 (0.117878) train_acc 93.750000 (96.437500)\n",
      "[2,  1100]  loss 0.198753 (0.116414) train_acc 93.750000 (96.471591)\n",
      "[2,  1200]  loss 0.011017 (0.113797) train_acc 100.000000 (96.557292)\n",
      "[2,  1300]  loss 0.015997 (0.113342) train_acc 100.000000 (96.543269)\n",
      "[2,  1400]  loss 0.115983 (0.112799) train_acc 93.750000 (96.482143)\n",
      "[2,  1500]  loss 0.259360 (0.112832) train_acc 93.750000 (96.483333)\n",
      "[2,  1600]  loss 0.063320 (0.111266) train_acc 100.000000 (96.550781)\n",
      "[2,  1700]  loss 0.025273 (0.110299) train_acc 100.000000 (96.547794)\n",
      "[2,  1800]  loss 0.224248 (0.110166) train_acc 87.500000 (96.545139)\n",
      "[2,  1900]  loss 0.003897 (0.109231) train_acc 100.000000 (96.578947)\n",
      "[2,  2000]  loss 0.022398 (0.108282) train_acc 100.000000 (96.621875)\n",
      "[2,  2100]  loss 0.023439 (0.108057) train_acc 100.000000 (96.607143)\n",
      "[2,  2200]  loss 0.090036 (0.107655) train_acc 93.750000 (96.644886)\n",
      "[2,  2300]  loss 0.378309 (0.107094) train_acc 93.750000 (96.652174)\n",
      "[2,  2400]  loss 0.005049 (0.106956) train_acc 100.000000 (96.651042)\n",
      "[2,  2500]  loss 0.017846 (0.106072) train_acc 100.000000 (96.665000)\n",
      "[2,  2600]  loss 0.021712 (0.105682) train_acc 100.000000 (96.682692)\n",
      "[2,  2700]  loss 0.027624 (0.104634) train_acc 100.000000 (96.706019)\n",
      "[2,  2800]  loss 0.095449 (0.104281) train_acc 93.750000 (96.707589)\n",
      "[2,  2900]  loss 0.013392 (0.103673) train_acc 100.000000 (96.719828)\n",
      "[2,  3000]  loss 0.172024 (0.103020) train_acc 93.750000 (96.750000)\n",
      "[2,  3100]  loss 0.093279 (0.103028) train_acc 93.750000 (96.756048)\n",
      "[2,  3200]  loss 0.076974 (0.102586) train_acc 100.000000 (96.773438)\n",
      "[2,  3300]  loss 0.016196 (0.101472) train_acc 100.000000 (96.821970)\n",
      "[2,  3400]  loss 0.022065 (0.100994) train_acc 100.000000 (96.843750)\n",
      "[2,  3500]  loss 0.015915 (0.100391) train_acc 100.000000 (96.860714)\n",
      "[2,  3600]  loss 0.004054 (0.099964) train_acc 100.000000 (96.880208)\n",
      "[2,  3700]  loss 0.009086 (0.099189) train_acc 100.000000 (96.908784)\n",
      "[3,   100]  loss 0.028205 (0.085776) train_acc 100.000000 (97.562500)\n",
      "[3,   200]  loss 0.019065 (0.080041) train_acc 100.000000 (97.562500)\n",
      "[3,   300]  loss 0.001346 (0.072233) train_acc 100.000000 (97.770833)\n",
      "[3,   400]  loss 0.011281 (0.076912) train_acc 100.000000 (97.656250)\n",
      "[3,   500]  loss 0.091038 (0.078344) train_acc 93.750000 (97.612500)\n",
      "[3,   600]  loss 0.007588 (0.076333) train_acc 100.000000 (97.697917)\n",
      "[3,   700]  loss 0.011637 (0.074694) train_acc 100.000000 (97.785714)\n",
      "[3,   800]  loss 0.035153 (0.075711) train_acc 100.000000 (97.757812)\n",
      "[3,   900]  loss 0.414929 (0.075583) train_acc 87.500000 (97.687500)\n",
      "[3,  1000]  loss 0.008070 (0.075947) train_acc 100.000000 (97.687500)\n",
      "[3,  1100]  loss 0.007221 (0.075410) train_acc 100.000000 (97.687500)\n",
      "[3,  1200]  loss 0.008303 (0.074412) train_acc 100.000000 (97.734375)\n",
      "[3,  1300]  loss 0.032283 (0.074381) train_acc 100.000000 (97.764423)\n",
      "[3,  1400]  loss 0.018156 (0.074594) train_acc 100.000000 (97.732143)\n",
      "[3,  1500]  loss 0.932325 (0.075210) train_acc 93.750000 (97.745833)\n",
      "[3,  1600]  loss 0.087021 (0.074718) train_acc 93.750000 (97.742188)\n",
      "[3,  1700]  loss 0.003331 (0.073846) train_acc 100.000000 (97.768382)\n",
      "[3,  1800]  loss 0.136230 (0.072854) train_acc 93.750000 (97.791667)\n",
      "[3,  1900]  loss 0.182279 (0.072977) train_acc 87.500000 (97.756579)\n",
      "[3,  2000]  loss 0.162165 (0.073705) train_acc 93.750000 (97.731250)\n",
      "[3,  2100]  loss 0.146628 (0.072983) train_acc 93.750000 (97.758929)\n",
      "[3,  2200]  loss 0.090060 (0.072498) train_acc 93.750000 (97.786932)\n",
      "[3,  2300]  loss 0.033473 (0.073019) train_acc 100.000000 (97.774457)\n",
      "[3,  2400]  loss 0.003155 (0.072295) train_acc 100.000000 (97.783854)\n",
      "[3,  2500]  loss 0.004065 (0.072004) train_acc 100.000000 (97.800000)\n",
      "[3,  2600]  loss 0.001974 (0.071678) train_acc 100.000000 (97.812500)\n",
      "[3,  2700]  loss 0.342438 (0.071706) train_acc 87.500000 (97.810185)\n",
      "[3,  2800]  loss 0.500823 (0.071554) train_acc 93.750000 (97.808036)\n",
      "[3,  2900]  loss 0.016502 (0.071138) train_acc 100.000000 (97.829741)\n",
      "[3,  3000]  loss 0.006011 (0.071125) train_acc 100.000000 (97.839583)\n",
      "[3,  3100]  loss 0.058453 (0.070811) train_acc 100.000000 (97.856855)\n",
      "[3,  3200]  loss 0.121581 (0.071390) train_acc 93.750000 (97.847656)\n",
      "[3,  3300]  loss 0.036881 (0.071867) train_acc 100.000000 (97.842803)\n",
      "[3,  3400]  loss 0.226884 (0.071763) train_acc 87.500000 (97.841912)\n",
      "[3,  3500]  loss 0.229164 (0.071093) train_acc 93.750000 (97.862500)\n",
      "[3,  3600]  loss 0.008634 (0.070619) train_acc 100.000000 (97.869792)\n",
      "[3,  3700]  loss 0.274217 (0.070710) train_acc 87.500000 (97.864865)\n",
      "[4,   100]  loss 0.028426 (0.046779) train_acc 100.000000 (98.625000)\n",
      "[4,   200]  loss 0.273821 (0.058238) train_acc 87.500000 (98.281250)\n",
      "[4,   300]  loss 0.039262 (0.054437) train_acc 100.000000 (98.437500)\n",
      "[4,   400]  loss 0.476090 (0.054693) train_acc 87.500000 (98.484375)\n",
      "[4,   500]  loss 0.083483 (0.054065) train_acc 93.750000 (98.362500)\n",
      "[4,   600]  loss 0.018703 (0.056750) train_acc 100.000000 (98.260417)\n",
      "[4,   700]  loss 0.003871 (0.056142) train_acc 100.000000 (98.214286)\n",
      "[4,   800]  loss 0.042867 (0.055340) train_acc 100.000000 (98.250000)\n",
      "[4,   900]  loss 0.067362 (0.055503) train_acc 93.750000 (98.229167)\n",
      "[4,  1000]  loss 0.011016 (0.056138) train_acc 100.000000 (98.193750)\n",
      "[4,  1100]  loss 0.155710 (0.056114) train_acc 87.500000 (98.198864)\n",
      "[4,  1200]  loss 0.047358 (0.056154) train_acc 100.000000 (98.213542)\n",
      "[4,  1300]  loss 0.012378 (0.057187) train_acc 100.000000 (98.182692)\n",
      "[4,  1400]  loss 0.013510 (0.057333) train_acc 100.000000 (98.174107)\n",
      "[4,  1500]  loss 0.008233 (0.058576) train_acc 100.000000 (98.120833)\n",
      "[4,  1600]  loss 0.027782 (0.058948) train_acc 100.000000 (98.089844)\n",
      "[4,  1700]  loss 0.001302 (0.058761) train_acc 100.000000 (98.102941)\n",
      "[4,  1800]  loss 0.061638 (0.058841) train_acc 93.750000 (98.083333)\n",
      "[4,  1900]  loss 0.008859 (0.058873) train_acc 100.000000 (98.078947)\n",
      "[4,  2000]  loss 0.240175 (0.058168) train_acc 93.750000 (98.103125)\n",
      "[4,  2100]  loss 0.045635 (0.058461) train_acc 93.750000 (98.074405)\n",
      "[4,  2200]  loss 0.041898 (0.057699) train_acc 100.000000 (98.107955)\n",
      "[4,  2300]  loss 0.016814 (0.058264) train_acc 100.000000 (98.100543)\n",
      "[4,  2400]  loss 0.005709 (0.058082) train_acc 100.000000 (98.109375)\n",
      "[4,  2500]  loss 0.004538 (0.057291) train_acc 100.000000 (98.142500)\n",
      "[4,  2600]  loss 0.207671 (0.057917) train_acc 93.750000 (98.122596)\n",
      "[4,  2700]  loss 0.099887 (0.058595) train_acc 93.750000 (98.101852)\n",
      "[4,  2800]  loss 0.014115 (0.059337) train_acc 100.000000 (98.073661)\n",
      "[4,  2900]  loss 0.003647 (0.059245) train_acc 100.000000 (98.092672)\n",
      "[4,  3000]  loss 0.001349 (0.059156) train_acc 100.000000 (98.093750)\n",
      "[4,  3100]  loss 0.005665 (0.059168) train_acc 100.000000 (98.100806)\n",
      "[4,  3200]  loss 0.033942 (0.058811) train_acc 100.000000 (98.119141)\n",
      "[4,  3300]  loss 0.136553 (0.059052) train_acc 87.500000 (98.109848)\n",
      "[4,  3400]  loss 0.023809 (0.058821) train_acc 100.000000 (98.125000)\n",
      "[4,  3500]  loss 0.386429 (0.058463) train_acc 87.500000 (98.132143)\n",
      "[4,  3600]  loss 0.557996 (0.058855) train_acc 93.750000 (98.131944)\n",
      "[4,  3700]  loss 0.002726 (0.058426) train_acc 100.000000 (98.138514)\n",
      "[5,   100]  loss 0.004578 (0.047602) train_acc 100.000000 (98.312500)\n",
      "[5,   200]  loss 0.023941 (0.050265) train_acc 100.000000 (98.250000)\n",
      "[5,   300]  loss 0.001420 (0.047829) train_acc 100.000000 (98.291667)\n",
      "[5,   400]  loss 0.371280 (0.047257) train_acc 87.500000 (98.343750)\n",
      "[5,   500]  loss 0.003234 (0.047043) train_acc 100.000000 (98.325000)\n",
      "[5,   600]  loss 0.009462 (0.047136) train_acc 100.000000 (98.375000)\n",
      "[5,   700]  loss 0.005436 (0.048513) train_acc 100.000000 (98.339286)\n",
      "[5,   800]  loss 0.003861 (0.048309) train_acc 100.000000 (98.351562)\n",
      "[5,   900]  loss 0.569693 (0.047669) train_acc 93.750000 (98.375000)\n",
      "[5,  1000]  loss 0.006138 (0.049122) train_acc 100.000000 (98.393750)\n",
      "[5,  1100]  loss 0.010788 (0.048852) train_acc 100.000000 (98.403409)\n",
      "[5,  1200]  loss 0.126824 (0.048671) train_acc 93.750000 (98.411458)\n",
      "[5,  1300]  loss 0.007909 (0.047985) train_acc 100.000000 (98.437500)\n",
      "[5,  1400]  loss 0.006928 (0.047081) train_acc 100.000000 (98.459821)\n",
      "[5,  1500]  loss 0.052381 (0.046943) train_acc 93.750000 (98.458333)\n",
      "[5,  1600]  loss 0.011522 (0.048873) train_acc 100.000000 (98.429688)\n",
      "[5,  1700]  loss 0.043433 (0.048363) train_acc 100.000000 (98.452206)\n",
      "[5,  1800]  loss 0.029438 (0.049103) train_acc 100.000000 (98.440972)\n",
      "[5,  1900]  loss 0.002375 (0.049471) train_acc 100.000000 (98.430921)\n",
      "[5,  2000]  loss 0.322625 (0.050121) train_acc 87.500000 (98.409375)\n",
      "[5,  2100]  loss 0.000369 (0.049610) train_acc 100.000000 (98.428571)\n",
      "[5,  2200]  loss 0.028111 (0.050053) train_acc 100.000000 (98.417614)\n",
      "[5,  2300]  loss 0.024121 (0.050114) train_acc 100.000000 (98.426630)\n",
      "[5,  2400]  loss 0.065213 (0.049935) train_acc 93.750000 (98.434896)\n",
      "[5,  2500]  loss 0.004137 (0.050023) train_acc 100.000000 (98.427500)\n",
      "[5,  2600]  loss 0.409128 (0.049654) train_acc 93.750000 (98.437500)\n",
      "[5,  2700]  loss 0.001119 (0.049462) train_acc 100.000000 (98.449074)\n",
      "[5,  2800]  loss 0.355243 (0.049448) train_acc 93.750000 (98.448661)\n",
      "[5,  2900]  loss 0.013788 (0.049344) train_acc 100.000000 (98.463362)\n",
      "[5,  3000]  loss 0.012992 (0.049174) train_acc 100.000000 (98.472917)\n",
      "[5,  3100]  loss 0.038939 (0.048765) train_acc 100.000000 (98.483871)\n",
      "[5,  3200]  loss 0.000861 (0.048704) train_acc 100.000000 (98.480469)\n",
      "[5,  3300]  loss 0.015521 (0.048561) train_acc 100.000000 (98.479167)\n",
      "[5,  3400]  loss 0.003033 (0.049037) train_acc 100.000000 (98.461397)\n",
      "[5,  3500]  loss 0.008886 (0.049119) train_acc 100.000000 (98.457143)\n",
      "[5,  3600]  loss 0.005813 (0.049050) train_acc 100.000000 (98.463542)\n",
      "[5,  3700]  loss 0.001237 (0.049190) train_acc 100.000000 (98.462838)\n",
      "[6,   100]  loss 0.038529 (0.046830) train_acc 100.000000 (98.312500)\n",
      "[6,   200]  loss 0.001283 (0.041145) train_acc 100.000000 (98.500000)\n",
      "[6,   300]  loss 0.030233 (0.039265) train_acc 100.000000 (98.583333)\n",
      "[6,   400]  loss 0.004944 (0.040041) train_acc 100.000000 (98.531250)\n",
      "[6,   500]  loss 0.003366 (0.043209) train_acc 100.000000 (98.487500)\n",
      "[6,   600]  loss 0.007910 (0.041058) train_acc 100.000000 (98.583333)\n",
      "[6,   700]  loss 0.005828 (0.042493) train_acc 100.000000 (98.571429)\n",
      "[6,   800]  loss 0.444348 (0.044754) train_acc 93.750000 (98.515625)\n",
      "[6,   900]  loss 0.250955 (0.045553) train_acc 87.500000 (98.513889)\n",
      "[6,  1000]  loss 0.126272 (0.045713) train_acc 93.750000 (98.525000)\n",
      "[6,  1100]  loss 0.003937 (0.044372) train_acc 100.000000 (98.585227)\n",
      "[6,  1200]  loss 0.060407 (0.043239) train_acc 93.750000 (98.630208)\n",
      "[6,  1300]  loss 0.147320 (0.042993) train_acc 93.750000 (98.629808)\n",
      "[6,  1400]  loss 0.034901 (0.043078) train_acc 100.000000 (98.638393)\n",
      "[6,  1500]  loss 0.021235 (0.042423) train_acc 100.000000 (98.675000)\n",
      "[6,  1600]  loss 0.014049 (0.042621) train_acc 100.000000 (98.667969)\n",
      "[6,  1700]  loss 0.358439 (0.043885) train_acc 93.750000 (98.628676)\n",
      "[6,  1800]  loss 0.047788 (0.043707) train_acc 100.000000 (98.649306)\n",
      "[6,  1900]  loss 0.014185 (0.042941) train_acc 100.000000 (98.674342)\n",
      "[6,  2000]  loss 0.042415 (0.042938) train_acc 100.000000 (98.681250)\n",
      "[6,  2100]  loss 0.173789 (0.043421) train_acc 93.750000 (98.657738)\n",
      "[6,  2200]  loss 0.002430 (0.042473) train_acc 100.000000 (98.693182)\n",
      "[6,  2300]  loss 0.549103 (0.042357) train_acc 93.750000 (98.701087)\n",
      "[6,  2400]  loss 0.108775 (0.042269) train_acc 93.750000 (98.700521)\n",
      "[6,  2500]  loss 0.000324 (0.042449) train_acc 100.000000 (98.695000)\n",
      "[6,  2600]  loss 0.005198 (0.043165) train_acc 100.000000 (98.668269)\n",
      "[6,  2700]  loss 0.004479 (0.042732) train_acc 100.000000 (98.687500)\n",
      "[6,  2800]  loss 0.002919 (0.042542) train_acc 100.000000 (98.698661)\n",
      "[6,  2900]  loss 0.004652 (0.042585) train_acc 100.000000 (98.709052)\n",
      "[6,  3000]  loss 0.002993 (0.042018) train_acc 100.000000 (98.725000)\n",
      "[6,  3100]  loss 0.004428 (0.042067) train_acc 100.000000 (98.729839)\n",
      "[6,  3200]  loss 0.012910 (0.042055) train_acc 100.000000 (98.728516)\n",
      "[6,  3300]  loss 0.044912 (0.042016) train_acc 100.000000 (98.721591)\n",
      "[6,  3400]  loss 0.002936 (0.041767) train_acc 100.000000 (98.720588)\n",
      "[6,  3500]  loss 0.000401 (0.041239) train_acc 100.000000 (98.737500)\n",
      "[6,  3600]  loss 0.015875 (0.041348) train_acc 100.000000 (98.739583)\n",
      "[6,  3700]  loss 0.149014 (0.041503) train_acc 93.750000 (98.733108)\n",
      "[7,   100]  loss 0.000857 (0.029426) train_acc 100.000000 (99.125000)\n",
      "[7,   200]  loss 0.228517 (0.033980) train_acc 93.750000 (98.937500)\n",
      "[7,   300]  loss 0.004176 (0.036556) train_acc 100.000000 (98.729167)\n",
      "[7,   400]  loss 0.045344 (0.036100) train_acc 93.750000 (98.765625)\n",
      "[7,   500]  loss 0.013943 (0.036768) train_acc 100.000000 (98.762500)\n",
      "[7,   600]  loss 0.031987 (0.040821) train_acc 100.000000 (98.687500)\n",
      "[7,   700]  loss 0.009018 (0.039188) train_acc 100.000000 (98.732143)\n",
      "[7,   800]  loss 0.007372 (0.037375) train_acc 100.000000 (98.789062)\n",
      "[7,   900]  loss 0.003569 (0.037005) train_acc 100.000000 (98.805556)\n",
      "[7,  1000]  loss 0.011558 (0.036291) train_acc 100.000000 (98.812500)\n",
      "[7,  1100]  loss 0.053311 (0.035720) train_acc 100.000000 (98.869318)\n",
      "[7,  1200]  loss 0.029062 (0.034711) train_acc 100.000000 (98.875000)\n",
      "[7,  1300]  loss 0.014927 (0.035501) train_acc 100.000000 (98.841346)\n",
      "[7,  1400]  loss 0.735398 (0.036031) train_acc 81.250000 (98.830357)\n",
      "[7,  1500]  loss 0.047780 (0.035878) train_acc 100.000000 (98.825000)\n",
      "[7,  1600]  loss 0.016586 (0.037477) train_acc 100.000000 (98.789062)\n",
      "[7,  1700]  loss 0.018245 (0.038652) train_acc 100.000000 (98.750000)\n",
      "[7,  1800]  loss 0.003769 (0.038000) train_acc 100.000000 (98.770833)\n",
      "[7,  1900]  loss 0.002504 (0.037777) train_acc 100.000000 (98.776316)\n",
      "[7,  2000]  loss 0.213802 (0.037589) train_acc 93.750000 (98.771875)\n",
      "[7,  2100]  loss 0.001489 (0.038038) train_acc 100.000000 (98.776786)\n",
      "[7,  2200]  loss 0.010143 (0.037900) train_acc 100.000000 (98.786932)\n",
      "[7,  2300]  loss 0.044022 (0.038258) train_acc 100.000000 (98.785326)\n",
      "[7,  2400]  loss 0.007039 (0.037805) train_acc 100.000000 (98.789062)\n",
      "[7,  2500]  loss 0.000129 (0.037549) train_acc 100.000000 (98.787500)\n",
      "[7,  2600]  loss 0.066270 (0.037551) train_acc 93.750000 (98.778846)\n",
      "[7,  2700]  loss 0.003860 (0.037617) train_acc 100.000000 (98.780093)\n",
      "[7,  2800]  loss 0.015495 (0.037329) train_acc 100.000000 (98.787946)\n",
      "[7,  2900]  loss 0.052571 (0.037226) train_acc 100.000000 (98.790948)\n",
      "[7,  3000]  loss 0.085475 (0.036921) train_acc 93.750000 (98.804167)\n",
      "[7,  3100]  loss 0.102219 (0.036829) train_acc 93.750000 (98.806452)\n",
      "[7,  3200]  loss 0.222724 (0.037152) train_acc 93.750000 (98.806641)\n",
      "[7,  3300]  loss 0.009467 (0.036885) train_acc 100.000000 (98.820076)\n",
      "[7,  3400]  loss 0.022779 (0.036925) train_acc 100.000000 (98.814338)\n",
      "[7,  3500]  loss 0.088839 (0.036976) train_acc 93.750000 (98.801786)\n",
      "[7,  3600]  loss 0.001826 (0.036552) train_acc 100.000000 (98.819444)\n",
      "[7,  3700]  loss 0.008506 (0.036565) train_acc 100.000000 (98.826014)\n",
      "[8,   100]  loss 0.000163 (0.023289) train_acc 100.000000 (99.375000)\n",
      "[8,   200]  loss 0.112093 (0.023635) train_acc 93.750000 (99.281250)\n",
      "[8,   300]  loss 0.001644 (0.025300) train_acc 100.000000 (99.291667)\n",
      "[8,   400]  loss 0.004688 (0.028937) train_acc 100.000000 (99.171875)\n",
      "[8,   500]  loss 0.006839 (0.029996) train_acc 100.000000 (99.187500)\n",
      "[8,   600]  loss 0.034937 (0.028431) train_acc 100.000000 (99.218750)\n",
      "[8,   700]  loss 0.022836 (0.029805) train_acc 100.000000 (99.160714)\n",
      "[8,   800]  loss 0.010319 (0.029298) train_acc 100.000000 (99.179688)\n",
      "[8,   900]  loss 0.001738 (0.029213) train_acc 100.000000 (99.152778)\n",
      "[8,  1000]  loss 0.005284 (0.029953) train_acc 100.000000 (99.143750)\n",
      "[8,  1100]  loss 0.000545 (0.029320) train_acc 100.000000 (99.147727)\n",
      "[8,  1200]  loss 0.001719 (0.029114) train_acc 100.000000 (99.166667)\n",
      "[8,  1300]  loss 0.096924 (0.029489) train_acc 100.000000 (99.153846)\n",
      "[8,  1400]  loss 0.001490 (0.029215) train_acc 100.000000 (99.174107)\n",
      "[8,  1500]  loss 0.001474 (0.028531) train_acc 100.000000 (99.187500)\n",
      "[8,  1600]  loss 0.017344 (0.028724) train_acc 100.000000 (99.175781)\n",
      "[8,  1700]  loss 0.000108 (0.028759) train_acc 100.000000 (99.172794)\n",
      "[8,  1800]  loss 0.000490 (0.029051) train_acc 100.000000 (99.156250)\n",
      "[8,  1900]  loss 0.003398 (0.028897) train_acc 100.000000 (99.154605)\n",
      "[8,  2000]  loss 0.447425 (0.029332) train_acc 93.750000 (99.143750)\n",
      "[8,  2100]  loss 0.000307 (0.029179) train_acc 100.000000 (99.157738)\n",
      "[8,  2200]  loss 0.006701 (0.029971) train_acc 100.000000 (99.136364)\n",
      "[8,  2300]  loss 0.149617 (0.030392) train_acc 93.750000 (99.125000)\n",
      "[8,  2400]  loss 0.141798 (0.030384) train_acc 93.750000 (99.125000)\n",
      "[8,  2500]  loss 0.071029 (0.030897) train_acc 93.750000 (99.102500)\n",
      "[8,  2600]  loss 0.009942 (0.031844) train_acc 100.000000 (99.081731)\n",
      "[8,  2700]  loss 0.022703 (0.032239) train_acc 100.000000 (99.069444)\n",
      "[8,  2800]  loss 0.019912 (0.032439) train_acc 100.000000 (99.066964)\n",
      "[8,  2900]  loss 0.174783 (0.032715) train_acc 93.750000 (99.058190)\n",
      "[8,  3000]  loss 0.062892 (0.032820) train_acc 93.750000 (99.054167)\n",
      "[8,  3100]  loss 0.000851 (0.032541) train_acc 100.000000 (99.056452)\n",
      "[8,  3200]  loss 0.005403 (0.032516) train_acc 100.000000 (99.052734)\n",
      "[8,  3300]  loss 0.001102 (0.032620) train_acc 100.000000 (99.045455)\n",
      "[8,  3400]  loss 0.078222 (0.032722) train_acc 93.750000 (99.038603)\n",
      "[8,  3500]  loss 0.002614 (0.032497) train_acc 100.000000 (99.048214)\n",
      "[8,  3600]  loss 0.004299 (0.032322) train_acc 100.000000 (99.053819)\n",
      "[8,  3700]  loss 0.016403 (0.032305) train_acc 100.000000 (99.045608)\n",
      "[9,   100]  loss 0.013062 (0.025335) train_acc 100.000000 (99.062500)\n",
      "[9,   200]  loss 0.000563 (0.024304) train_acc 100.000000 (99.187500)\n",
      "[9,   300]  loss 0.000287 (0.024464) train_acc 100.000000 (99.166667)\n",
      "[9,   400]  loss 0.074981 (0.028863) train_acc 100.000000 (99.078125)\n",
      "[9,   500]  loss 0.056687 (0.029622) train_acc 93.750000 (99.012500)\n",
      "[9,   600]  loss 0.000675 (0.028900) train_acc 100.000000 (99.062500)\n",
      "[9,   700]  loss 0.002145 (0.029337) train_acc 100.000000 (99.062500)\n",
      "[9,   800]  loss 0.014122 (0.028467) train_acc 100.000000 (99.070312)\n",
      "[9,   900]  loss 0.011639 (0.030250) train_acc 100.000000 (99.027778)\n",
      "[9,  1000]  loss 0.048280 (0.030462) train_acc 100.000000 (99.018750)\n",
      "[9,  1100]  loss 0.000408 (0.030069) train_acc 100.000000 (99.022727)\n",
      "[9,  1200]  loss 0.001094 (0.031415) train_acc 100.000000 (98.973958)\n",
      "[9,  1300]  loss 0.036197 (0.030802) train_acc 100.000000 (99.004808)\n",
      "[9,  1400]  loss 0.010693 (0.030894) train_acc 100.000000 (99.013393)\n",
      "[9,  1500]  loss 0.002250 (0.030819) train_acc 100.000000 (99.008333)\n",
      "[9,  1600]  loss 0.000177 (0.030638) train_acc 100.000000 (99.031250)\n",
      "[9,  1700]  loss 0.000243 (0.029960) train_acc 100.000000 (99.051471)\n",
      "[9,  1800]  loss 0.015647 (0.029432) train_acc 100.000000 (99.062500)\n",
      "[9,  1900]  loss 0.006585 (0.029550) train_acc 100.000000 (99.059211)\n",
      "[9,  2000]  loss 0.003261 (0.029232) train_acc 100.000000 (99.071875)\n",
      "[9,  2100]  loss 0.004757 (0.029032) train_acc 100.000000 (99.080357)\n",
      "[9,  2200]  loss 0.173141 (0.028875) train_acc 93.750000 (99.076705)\n",
      "[9,  2300]  loss 0.178239 (0.028722) train_acc 93.750000 (99.084239)\n",
      "[9,  2400]  loss 0.000733 (0.028693) train_acc 100.000000 (99.080729)\n",
      "[9,  2500]  loss 0.000758 (0.028656) train_acc 100.000000 (99.082500)\n",
      "[9,  2600]  loss 0.470084 (0.028856) train_acc 81.250000 (99.076923)\n",
      "[9,  2700]  loss 0.004853 (0.028877) train_acc 100.000000 (99.076389)\n",
      "[9,  2800]  loss 0.014809 (0.028887) train_acc 100.000000 (99.064732)\n",
      "[9,  2900]  loss 0.002924 (0.028747) train_acc 100.000000 (99.058190)\n",
      "[9,  3000]  loss 0.000803 (0.028503) train_acc 100.000000 (99.066667)\n",
      "[9,  3100]  loss 0.020178 (0.028294) train_acc 100.000000 (99.078629)\n",
      "[9,  3200]  loss 0.030775 (0.028019) train_acc 100.000000 (99.083984)\n",
      "[9,  3300]  loss 0.077770 (0.028029) train_acc 100.000000 (99.081439)\n",
      "[9,  3400]  loss 0.000153 (0.027798) train_acc 100.000000 (99.097426)\n",
      "[9,  3500]  loss 0.003556 (0.027917) train_acc 100.000000 (99.089286)\n",
      "[9,  3600]  loss 0.004662 (0.027990) train_acc 100.000000 (99.085069)\n",
      "[9,  3700]  loss 0.000716 (0.028302) train_acc 100.000000 (99.076014)\n",
      "[10,   100]  loss 0.005901 (0.014159) train_acc 100.000000 (99.625000)\n",
      "[10,   200]  loss 0.000422 (0.021493) train_acc 100.000000 (99.406250)\n",
      "[10,   300]  loss 0.001742 (0.023047) train_acc 100.000000 (99.354167)\n",
      "[10,   400]  loss 0.004992 (0.023293) train_acc 100.000000 (99.296875)\n",
      "[10,   500]  loss 0.007780 (0.023734) train_acc 100.000000 (99.287500)\n",
      "[10,   600]  loss 0.002886 (0.025169) train_acc 100.000000 (99.218750)\n",
      "[10,   700]  loss 0.004687 (0.024802) train_acc 100.000000 (99.241071)\n",
      "[10,   800]  loss 0.120439 (0.024662) train_acc 93.750000 (99.234375)\n",
      "[10,   900]  loss 0.012174 (0.025538) train_acc 100.000000 (99.201389)\n",
      "[10,  1000]  loss 0.075116 (0.026357) train_acc 93.750000 (99.156250)\n",
      "[10,  1100]  loss 0.001827 (0.026063) train_acc 100.000000 (99.164773)\n",
      "[10,  1200]  loss 0.002670 (0.026037) train_acc 100.000000 (99.161458)\n",
      "[10,  1300]  loss 0.000116 (0.025800) train_acc 100.000000 (99.149038)\n",
      "[10,  1400]  loss 0.000744 (0.025573) train_acc 100.000000 (99.165179)\n",
      "[10,  1500]  loss 0.017743 (0.025613) train_acc 100.000000 (99.154167)\n",
      "[10,  1600]  loss 0.002173 (0.025184) train_acc 100.000000 (99.171875)\n",
      "[10,  1700]  loss 0.036553 (0.025488) train_acc 100.000000 (99.158088)\n",
      "[10,  1800]  loss 0.004706 (0.025840) train_acc 100.000000 (99.145833)\n",
      "[10,  1900]  loss 0.001101 (0.025947) train_acc 100.000000 (99.148026)\n",
      "[10,  2000]  loss 0.024498 (0.025587) train_acc 100.000000 (99.159375)\n",
      "[10,  2100]  loss 0.005468 (0.024992) train_acc 100.000000 (99.187500)\n",
      "[10,  2200]  loss 0.000092 (0.024845) train_acc 100.000000 (99.196023)\n",
      "[10,  2300]  loss 0.003118 (0.024971) train_acc 100.000000 (99.195652)\n",
      "[10,  2400]  loss 0.000779 (0.024875) train_acc 100.000000 (99.192708)\n",
      "[10,  2500]  loss 0.106839 (0.025453) train_acc 93.750000 (99.172500)\n",
      "[10,  2600]  loss 0.046152 (0.025366) train_acc 93.750000 (99.175481)\n",
      "[10,  2700]  loss 0.000215 (0.025309) train_acc 100.000000 (99.178241)\n",
      "[10,  2800]  loss 0.000124 (0.025339) train_acc 100.000000 (99.176339)\n",
      "[10,  2900]  loss 0.215196 (0.025588) train_acc 93.750000 (99.165948)\n",
      "[10,  3000]  loss 0.047627 (0.025555) train_acc 100.000000 (99.172917)\n",
      "[10,  3100]  loss 0.002786 (0.025382) train_acc 100.000000 (99.173387)\n",
      "[10,  3200]  loss 0.000335 (0.025525) train_acc 100.000000 (99.166016)\n",
      "[10,  3300]  loss 0.021814 (0.025726) train_acc 100.000000 (99.159091)\n",
      "[10,  3400]  loss 0.023360 (0.025533) train_acc 100.000000 (99.159926)\n",
      "[10,  3500]  loss 0.005095 (0.025557) train_acc 100.000000 (99.158929)\n",
      "[10,  3600]  loss 0.008266 (0.025372) train_acc 100.000000 (99.170139)\n",
      "[10,  3700]  loss 0.005178 (0.025564) train_acc 100.000000 (99.168919)\n",
      "[11,   100]  loss 0.000112 (0.031506) train_acc 100.000000 (99.062500)\n",
      "[11,   200]  loss 0.005911 (0.021674) train_acc 100.000000 (99.343750)\n",
      "[11,   300]  loss 0.000754 (0.022300) train_acc 100.000000 (99.270833)\n",
      "[11,   400]  loss 0.000824 (0.022683) train_acc 100.000000 (99.250000)\n",
      "[11,   500]  loss 0.019299 (0.023107) train_acc 100.000000 (99.212500)\n",
      "[11,   600]  loss 0.012427 (0.022252) train_acc 100.000000 (99.218750)\n",
      "[11,   700]  loss 0.031971 (0.021157) train_acc 100.000000 (99.250000)\n",
      "[11,   800]  loss 0.002375 (0.020784) train_acc 100.000000 (99.250000)\n",
      "[11,   900]  loss 0.003696 (0.021562) train_acc 100.000000 (99.222222)\n",
      "[11,  1000]  loss 0.003071 (0.021340) train_acc 100.000000 (99.218750)\n",
      "[11,  1100]  loss 0.003439 (0.021665) train_acc 100.000000 (99.204545)\n",
      "[11,  1200]  loss 0.008359 (0.022381) train_acc 100.000000 (99.197917)\n",
      "[11,  1300]  loss 0.005116 (0.022923) train_acc 100.000000 (99.177885)\n",
      "[11,  1400]  loss 0.004736 (0.022470) train_acc 100.000000 (99.191964)\n",
      "[11,  1500]  loss 0.047213 (0.023394) train_acc 100.000000 (99.187500)\n",
      "[11,  1600]  loss 0.003220 (0.023607) train_acc 100.000000 (99.179688)\n",
      "[11,  1700]  loss 0.001366 (0.023539) train_acc 100.000000 (99.198529)\n",
      "[11,  1800]  loss 0.001136 (0.023188) train_acc 100.000000 (99.201389)\n",
      "[11,  1900]  loss 0.248802 (0.023535) train_acc 93.750000 (99.203947)\n",
      "[11,  2000]  loss 0.075319 (0.024864) train_acc 93.750000 (99.168750)\n",
      "[11,  2100]  loss 0.000611 (0.025132) train_acc 100.000000 (99.175595)\n",
      "[11,  2200]  loss 0.040399 (0.025144) train_acc 100.000000 (99.181818)\n",
      "[11,  2300]  loss 0.004304 (0.025040) train_acc 100.000000 (99.184783)\n",
      "[11,  2400]  loss 0.009686 (0.024856) train_acc 100.000000 (99.190104)\n",
      "[11,  2500]  loss 0.989541 (0.025082) train_acc 93.750000 (99.195000)\n",
      "[11,  2600]  loss 0.002006 (0.024799) train_acc 100.000000 (99.206731)\n",
      "[11,  2700]  loss 0.003511 (0.024422) train_acc 100.000000 (99.219907)\n",
      "[11,  2800]  loss 0.008627 (0.023942) train_acc 100.000000 (99.238839)\n",
      "[11,  2900]  loss 0.000151 (0.024013) train_acc 100.000000 (99.237069)\n",
      "[11,  3000]  loss 0.239413 (0.024094) train_acc 93.750000 (99.225000)\n",
      "[11,  3100]  loss 0.000142 (0.023839) train_acc 100.000000 (99.233871)\n",
      "[11,  3200]  loss 0.031210 (0.023530) train_acc 100.000000 (99.242188)\n",
      "[11,  3300]  loss 0.005525 (0.023478) train_acc 100.000000 (99.240530)\n",
      "[11,  3400]  loss 0.007800 (0.023618) train_acc 100.000000 (99.238971)\n",
      "[11,  3500]  loss 0.004572 (0.023346) train_acc 100.000000 (99.250000)\n",
      "[11,  3600]  loss 0.000797 (0.023584) train_acc 100.000000 (99.241319)\n",
      "[11,  3700]  loss 0.015254 (0.023555) train_acc 100.000000 (99.243243)\n",
      "[12,   100]  loss 0.005085 (0.017786) train_acc 100.000000 (99.500000)\n",
      "[12,   200]  loss 0.021603 (0.017644) train_acc 100.000000 (99.468750)\n",
      "[12,   300]  loss 0.065548 (0.017664) train_acc 93.750000 (99.375000)\n",
      "[12,   400]  loss 0.002534 (0.018015) train_acc 100.000000 (99.406250)\n",
      "[12,   500]  loss 0.004511 (0.018114) train_acc 100.000000 (99.400000)\n",
      "[12,   600]  loss 0.003663 (0.018928) train_acc 100.000000 (99.375000)\n",
      "[12,   700]  loss 0.000787 (0.018976) train_acc 100.000000 (99.357143)\n",
      "[12,   800]  loss 0.000537 (0.018981) train_acc 100.000000 (99.367188)\n",
      "[12,   900]  loss 0.000065 (0.019664) train_acc 100.000000 (99.333333)\n",
      "[12,  1000]  loss 0.001068 (0.020009) train_acc 100.000000 (99.318750)\n",
      "[12,  1100]  loss 0.005811 (0.020721) train_acc 100.000000 (99.278409)\n",
      "[12,  1200]  loss 0.141913 (0.020326) train_acc 93.750000 (99.296875)\n",
      "[12,  1300]  loss 0.306306 (0.020159) train_acc 87.500000 (99.298077)\n",
      "[12,  1400]  loss 0.000095 (0.019515) train_acc 100.000000 (99.321429)\n",
      "[12,  1500]  loss 0.001598 (0.019277) train_acc 100.000000 (99.337500)\n",
      "[12,  1600]  loss 0.004710 (0.019410) train_acc 100.000000 (99.332031)\n",
      "[12,  1700]  loss 0.040869 (0.019459) train_acc 100.000000 (99.327206)\n",
      "[12,  1800]  loss 0.000577 (0.019573) train_acc 100.000000 (99.329861)\n",
      "[12,  1900]  loss 0.009474 (0.019937) train_acc 100.000000 (99.319079)\n",
      "[12,  2000]  loss 0.000037 (0.020174) train_acc 100.000000 (99.300000)\n",
      "[12,  2100]  loss 0.007807 (0.020081) train_acc 100.000000 (99.306548)\n",
      "[12,  2200]  loss 0.000676 (0.020129) train_acc 100.000000 (99.309659)\n",
      "[12,  2300]  loss 0.005925 (0.020043) train_acc 100.000000 (99.315217)\n",
      "[12,  2400]  loss 0.007534 (0.019716) train_acc 100.000000 (99.330729)\n",
      "[12,  2500]  loss 0.001773 (0.019636) train_acc 100.000000 (99.342500)\n",
      "[12,  2600]  loss 0.000843 (0.019866) train_acc 100.000000 (99.334135)\n",
      "[12,  2700]  loss 0.000096 (0.019415) train_acc 100.000000 (99.351852)\n",
      "[12,  2800]  loss 0.002651 (0.019517) train_acc 100.000000 (99.352679)\n",
      "[12,  2900]  loss 0.085712 (0.019929) train_acc 93.750000 (99.344828)\n",
      "[12,  3000]  loss 0.001991 (0.019973) train_acc 100.000000 (99.343750)\n",
      "[12,  3100]  loss 0.038446 (0.019774) train_acc 100.000000 (99.342742)\n",
      "[12,  3200]  loss 0.016738 (0.020045) train_acc 100.000000 (99.337891)\n",
      "[12,  3300]  loss 0.014403 (0.019905) train_acc 100.000000 (99.339015)\n",
      "[12,  3400]  loss 0.008678 (0.020258) train_acc 100.000000 (99.325368)\n",
      "[12,  3500]  loss 0.001381 (0.020249) train_acc 100.000000 (99.328571)\n",
      "[12,  3600]  loss 0.000963 (0.020241) train_acc 100.000000 (99.326389)\n",
      "[12,  3700]  loss 0.001494 (0.020328) train_acc 100.000000 (99.322635)\n",
      "[13,   100]  loss 0.001512 (0.021973) train_acc 100.000000 (99.375000)\n",
      "[13,   200]  loss 0.000726 (0.022611) train_acc 100.000000 (99.343750)\n",
      "[13,   300]  loss 0.003523 (0.021057) train_acc 100.000000 (99.375000)\n",
      "[13,   400]  loss 0.000318 (0.018721) train_acc 100.000000 (99.421875)\n",
      "[13,   500]  loss 0.000164 (0.020268) train_acc 100.000000 (99.337500)\n",
      "[13,   600]  loss 0.023876 (0.018571) train_acc 100.000000 (99.406250)\n",
      "[13,   700]  loss 0.013687 (0.018706) train_acc 100.000000 (99.383929)\n",
      "[13,   800]  loss 0.003659 (0.018334) train_acc 100.000000 (99.406250)\n",
      "[13,   900]  loss 0.000225 (0.018379) train_acc 100.000000 (99.423611)\n",
      "[13,  1000]  loss 0.002339 (0.018330) train_acc 100.000000 (99.437500)\n",
      "[13,  1100]  loss 0.010841 (0.018024) train_acc 100.000000 (99.454545)\n",
      "[13,  1200]  loss 0.004621 (0.018189) train_acc 100.000000 (99.442708)\n",
      "[13,  1300]  loss 0.000465 (0.018104) train_acc 100.000000 (99.451923)\n",
      "[13,  1400]  loss 0.003857 (0.018178) train_acc 100.000000 (99.459821)\n",
      "[13,  1500]  loss 0.000049 (0.018900) train_acc 100.000000 (99.441667)\n",
      "[13,  1600]  loss 0.000496 (0.019000) train_acc 100.000000 (99.441406)\n",
      "[13,  1700]  loss 0.001630 (0.018925) train_acc 100.000000 (99.441176)\n",
      "[13,  1800]  loss 0.001646 (0.018817) train_acc 100.000000 (99.440972)\n",
      "[13,  1900]  loss 0.001557 (0.018443) train_acc 100.000000 (99.453947)\n",
      "[13,  2000]  loss 0.001702 (0.018077) train_acc 100.000000 (99.465625)\n",
      "[13,  2100]  loss 0.004041 (0.017986) train_acc 100.000000 (99.470238)\n",
      "[13,  2200]  loss 0.014427 (0.017773) train_acc 100.000000 (99.468750)\n",
      "[13,  2300]  loss 0.002443 (0.018233) train_acc 100.000000 (99.456522)\n",
      "[13,  2400]  loss 0.131847 (0.018665) train_acc 87.500000 (99.437500)\n",
      "[13,  2500]  loss 0.000038 (0.018282) train_acc 100.000000 (99.450000)\n",
      "[13,  2600]  loss 0.001376 (0.018058) train_acc 100.000000 (99.451923)\n",
      "[13,  2700]  loss 0.000086 (0.017863) train_acc 100.000000 (99.456019)\n",
      "[13,  2800]  loss 0.056367 (0.017765) train_acc 93.750000 (99.453125)\n",
      "[13,  2900]  loss 0.000020 (0.017796) train_acc 100.000000 (99.448276)\n",
      "[13,  3000]  loss 0.000690 (0.017836) train_acc 100.000000 (99.452083)\n",
      "[13,  3100]  loss 0.036282 (0.017831) train_acc 100.000000 (99.463710)\n",
      "[13,  3200]  loss 0.000748 (0.017751) train_acc 100.000000 (99.468750)\n",
      "[13,  3300]  loss 0.082730 (0.018083) train_acc 93.750000 (99.456439)\n",
      "[13,  3400]  loss 0.005864 (0.018543) train_acc 100.000000 (99.450368)\n",
      "[13,  3500]  loss 0.000728 (0.018634) train_acc 100.000000 (99.446429)\n",
      "[13,  3600]  loss 0.019859 (0.018564) train_acc 100.000000 (99.446181)\n",
      "[13,  3700]  loss 0.000617 (0.018793) train_acc 100.000000 (99.437500)\n",
      "[14,   100]  loss 0.001312 (0.022824) train_acc 100.000000 (99.187500)\n",
      "[14,   200]  loss 0.002529 (0.017331) train_acc 100.000000 (99.406250)\n",
      "[14,   300]  loss 0.028036 (0.015870) train_acc 100.000000 (99.437500)\n",
      "[14,   400]  loss 0.001520 (0.015160) train_acc 100.000000 (99.515625)\n",
      "[14,   500]  loss 0.003057 (0.013822) train_acc 100.000000 (99.562500)\n",
      "[14,   600]  loss 0.003351 (0.012726) train_acc 100.000000 (99.604167)\n",
      "[14,   700]  loss 0.015416 (0.011909) train_acc 100.000000 (99.633929)\n",
      "[14,   800]  loss 0.000130 (0.012555) train_acc 100.000000 (99.601562)\n",
      "[14,   900]  loss 0.000469 (0.012239) train_acc 100.000000 (99.597222)\n",
      "[14,  1000]  loss 0.025308 (0.012233) train_acc 100.000000 (99.600000)\n",
      "[14,  1100]  loss 0.015867 (0.013478) train_acc 100.000000 (99.545455)\n",
      "[14,  1200]  loss 0.000632 (0.013307) train_acc 100.000000 (99.557292)\n",
      "[14,  1300]  loss 0.000214 (0.013116) train_acc 100.000000 (99.567308)\n",
      "[14,  1400]  loss 0.000691 (0.013228) train_acc 100.000000 (99.553571)\n",
      "[14,  1500]  loss 0.052163 (0.013307) train_acc 93.750000 (99.550000)\n",
      "[14,  1600]  loss 0.000728 (0.013923) train_acc 100.000000 (99.535156)\n",
      "[14,  1700]  loss 0.001819 (0.014277) train_acc 100.000000 (99.518382)\n",
      "[14,  1800]  loss 0.000345 (0.014370) train_acc 100.000000 (99.517361)\n",
      "[14,  1900]  loss 0.132140 (0.014397) train_acc 93.750000 (99.509868)\n",
      "[14,  2000]  loss 0.001612 (0.014340) train_acc 100.000000 (99.515625)\n",
      "[14,  2100]  loss 0.003119 (0.014512) train_acc 100.000000 (99.505952)\n",
      "[14,  2200]  loss 0.000172 (0.014463) train_acc 100.000000 (99.505682)\n",
      "[14,  2300]  loss 0.037525 (0.014448) train_acc 100.000000 (99.508152)\n",
      "[14,  2400]  loss 0.001287 (0.014435) train_acc 100.000000 (99.507812)\n",
      "[14,  2500]  loss 0.011286 (0.015004) train_acc 100.000000 (99.495000)\n",
      "[14,  2600]  loss 0.000880 (0.015250) train_acc 100.000000 (99.483173)\n",
      "[14,  2700]  loss 0.007416 (0.015778) train_acc 100.000000 (99.469907)\n",
      "[14,  2800]  loss 0.099121 (0.016234) train_acc 93.750000 (99.453125)\n",
      "[14,  2900]  loss 0.000739 (0.016346) train_acc 100.000000 (99.452586)\n",
      "[14,  3000]  loss 0.011306 (0.016299) train_acc 100.000000 (99.456250)\n",
      "[14,  3100]  loss 0.006241 (0.016466) train_acc 100.000000 (99.461694)\n",
      "[14,  3200]  loss 0.000873 (0.016506) train_acc 100.000000 (99.460938)\n",
      "[14,  3300]  loss 0.001934 (0.016296) train_acc 100.000000 (99.465909)\n",
      "[14,  3400]  loss 0.009397 (0.016432) train_acc 100.000000 (99.468750)\n",
      "[14,  3500]  loss 0.000114 (0.016836) train_acc 100.000000 (99.451786)\n",
      "[14,  3600]  loss 0.325723 (0.017128) train_acc 93.750000 (99.439236)\n",
      "[14,  3700]  loss 0.014715 (0.017221) train_acc 100.000000 (99.440878)\n",
      "[15,   100]  loss 0.019432 (0.017952) train_acc 100.000000 (99.500000)\n",
      "[15,   200]  loss 0.001443 (0.016919) train_acc 100.000000 (99.468750)\n",
      "[15,   300]  loss 0.001242 (0.015592) train_acc 100.000000 (99.541667)\n",
      "[15,   400]  loss 0.012851 (0.013449) train_acc 100.000000 (99.593750)\n",
      "[15,   500]  loss 0.000244 (0.012657) train_acc 100.000000 (99.625000)\n",
      "[15,   600]  loss 0.001356 (0.012385) train_acc 100.000000 (99.635417)\n",
      "[15,   700]  loss 0.000160 (0.013196) train_acc 100.000000 (99.625000)\n",
      "[15,   800]  loss 0.000066 (0.012303) train_acc 100.000000 (99.656250)\n",
      "[15,   900]  loss 0.000401 (0.013042) train_acc 100.000000 (99.625000)\n",
      "[15,  1000]  loss 0.000029 (0.013605) train_acc 100.000000 (99.593750)\n",
      "[15,  1100]  loss 0.000107 (0.013639) train_acc 100.000000 (99.590909)\n",
      "[15,  1200]  loss 0.011253 (0.013070) train_acc 100.000000 (99.614583)\n",
      "[15,  1300]  loss 0.000125 (0.012905) train_acc 100.000000 (99.629808)\n",
      "[15,  1400]  loss 0.003553 (0.012847) train_acc 100.000000 (99.625000)\n",
      "[15,  1500]  loss 0.000644 (0.013583) train_acc 100.000000 (99.591667)\n",
      "[15,  1600]  loss 0.003941 (0.014352) train_acc 100.000000 (99.566406)\n",
      "[15,  1700]  loss 0.000055 (0.013967) train_acc 100.000000 (99.580882)\n",
      "[15,  1800]  loss 0.031390 (0.013687) train_acc 100.000000 (99.583333)\n",
      "[15,  1900]  loss 0.000165 (0.013639) train_acc 100.000000 (99.569079)\n",
      "[15,  2000]  loss 0.031995 (0.014181) train_acc 100.000000 (99.550000)\n",
      "[15,  2100]  loss 0.000111 (0.014278) train_acc 100.000000 (99.544643)\n",
      "[15,  2200]  loss 0.004866 (0.014904) train_acc 100.000000 (99.536932)\n",
      "[15,  2300]  loss 0.000315 (0.015250) train_acc 100.000000 (99.527174)\n",
      "[15,  2400]  loss 0.004449 (0.015296) train_acc 100.000000 (99.526042)\n",
      "[15,  2500]  loss 0.000050 (0.015242) train_acc 100.000000 (99.530000)\n",
      "[15,  2600]  loss 0.000032 (0.015468) train_acc 100.000000 (99.519231)\n",
      "[15,  2700]  loss 0.002923 (0.015696) train_acc 100.000000 (99.509259)\n",
      "[15,  2800]  loss 0.001791 (0.015429) train_acc 100.000000 (99.517857)\n",
      "[15,  2900]  loss 0.002389 (0.015462) train_acc 100.000000 (99.510776)\n",
      "[15,  3000]  loss 0.032881 (0.015435) train_acc 100.000000 (99.508333)\n",
      "[15,  3100]  loss 0.003729 (0.015535) train_acc 100.000000 (99.497984)\n",
      "[15,  3200]  loss 0.060013 (0.015631) train_acc 93.750000 (99.484375)\n",
      "[15,  3300]  loss 0.003393 (0.015540) train_acc 100.000000 (99.490530)\n",
      "[15,  3400]  loss 0.000143 (0.015769) train_acc 100.000000 (99.476103)\n",
      "[15,  3500]  loss 0.000525 (0.015706) train_acc 100.000000 (99.482143)\n",
      "[15,  3600]  loss 0.000911 (0.015710) train_acc 100.000000 (99.482639)\n",
      "[15,  3700]  loss 0.000469 (0.015631) train_acc 100.000000 (99.484797)\n",
      "[16,   100]  loss 0.000730 (0.008715) train_acc 100.000000 (99.687500)\n",
      "[16,   200]  loss 0.000018 (0.007802) train_acc 100.000000 (99.781250)\n",
      "[16,   300]  loss 0.000980 (0.008833) train_acc 100.000000 (99.729167)\n",
      "[16,   400]  loss 0.033953 (0.010950) train_acc 100.000000 (99.671875)\n",
      "[16,   500]  loss 0.004092 (0.011130) train_acc 100.000000 (99.700000)\n",
      "[16,   600]  loss 0.001578 (0.011639) train_acc 100.000000 (99.666667)\n",
      "[16,   700]  loss 0.000757 (0.011827) train_acc 100.000000 (99.651786)\n",
      "[16,   800]  loss 0.002206 (0.011909) train_acc 100.000000 (99.632812)\n",
      "[16,   900]  loss 0.007116 (0.012294) train_acc 100.000000 (99.638889)\n",
      "[16,  1000]  loss 0.000243 (0.012787) train_acc 100.000000 (99.631250)\n",
      "[16,  1100]  loss 0.000192 (0.012347) train_acc 100.000000 (99.636364)\n",
      "[16,  1200]  loss 0.000291 (0.012627) train_acc 100.000000 (99.630208)\n",
      "[16,  1300]  loss 0.002185 (0.012731) train_acc 100.000000 (99.620192)\n",
      "[16,  1400]  loss 0.001305 (0.012619) train_acc 100.000000 (99.620536)\n",
      "[16,  1500]  loss 0.000699 (0.012375) train_acc 100.000000 (99.629167)\n",
      "[16,  1600]  loss 0.000002 (0.012259) train_acc 100.000000 (99.636719)\n",
      "[16,  1700]  loss 0.000896 (0.012241) train_acc 100.000000 (99.636029)\n",
      "[16,  1800]  loss 0.000724 (0.012234) train_acc 100.000000 (99.638889)\n",
      "[16,  1900]  loss 0.002173 (0.012508) train_acc 100.000000 (99.631579)\n",
      "[16,  2000]  loss 0.001373 (0.012781) train_acc 100.000000 (99.625000)\n",
      "[16,  2100]  loss 0.001572 (0.013185) train_acc 100.000000 (99.613095)\n",
      "[16,  2200]  loss 0.033342 (0.013058) train_acc 100.000000 (99.613636)\n",
      "[16,  2300]  loss 0.014853 (0.012885) train_acc 100.000000 (99.614130)\n",
      "[16,  2400]  loss 0.000047 (0.013057) train_acc 100.000000 (99.611979)\n",
      "[16,  2500]  loss 0.000729 (0.013601) train_acc 100.000000 (99.612500)\n",
      "[16,  2600]  loss 0.004323 (0.013334) train_acc 100.000000 (99.622596)\n",
      "[16,  2700]  loss 0.000266 (0.013158) train_acc 100.000000 (99.627315)\n",
      "[16,  2800]  loss 0.000236 (0.013098) train_acc 100.000000 (99.627232)\n",
      "[16,  2900]  loss 0.000180 (0.013101) train_acc 100.000000 (99.627155)\n",
      "[16,  3000]  loss 0.000096 (0.013092) train_acc 100.000000 (99.631250)\n",
      "[16,  3100]  loss 0.000819 (0.013260) train_acc 100.000000 (99.622984)\n",
      "[16,  3200]  loss 0.000848 (0.013239) train_acc 100.000000 (99.623047)\n",
      "[16,  3300]  loss 0.000400 (0.013479) train_acc 100.000000 (99.609848)\n",
      "[16,  3400]  loss 0.000890 (0.013491) train_acc 100.000000 (99.602941)\n",
      "[16,  3500]  loss 0.001020 (0.013573) train_acc 100.000000 (99.591071)\n",
      "[16,  3600]  loss 0.000780 (0.013473) train_acc 100.000000 (99.592014)\n",
      "[16,  3700]  loss 0.000200 (0.013360) train_acc 100.000000 (99.596284)\n",
      "[17,   100]  loss 0.000227 (0.009202) train_acc 100.000000 (99.625000)\n",
      "[17,   200]  loss 0.008422 (0.008653) train_acc 100.000000 (99.718750)\n",
      "[17,   300]  loss 0.000647 (0.010322) train_acc 100.000000 (99.687500)\n",
      "[17,   400]  loss 0.011423 (0.010717) train_acc 100.000000 (99.703125)\n",
      "[17,   500]  loss 0.000286 (0.010220) train_acc 100.000000 (99.700000)\n",
      "[17,   600]  loss 0.000035 (0.010160) train_acc 100.000000 (99.677083)\n",
      "[17,   700]  loss 0.034725 (0.010478) train_acc 100.000000 (99.660714)\n",
      "[17,   800]  loss 0.004509 (0.010346) train_acc 100.000000 (99.664062)\n",
      "[17,   900]  loss 0.002948 (0.009917) train_acc 100.000000 (99.666667)\n",
      "[17,  1000]  loss 0.000073 (0.009871) train_acc 100.000000 (99.662500)\n",
      "[17,  1100]  loss 0.000374 (0.009760) train_acc 100.000000 (99.659091)\n",
      "[17,  1200]  loss 0.112489 (0.010151) train_acc 93.750000 (99.651042)\n",
      "[17,  1300]  loss 0.022233 (0.009844) train_acc 100.000000 (99.668269)\n",
      "[17,  1400]  loss 0.108177 (0.009850) train_acc 93.750000 (99.665179)\n",
      "[17,  1500]  loss 0.000146 (0.009992) train_acc 100.000000 (99.658333)\n",
      "[17,  1600]  loss 0.000143 (0.010315) train_acc 100.000000 (99.640625)\n",
      "[17,  1700]  loss 0.000188 (0.010944) train_acc 100.000000 (99.606618)\n",
      "[17,  1800]  loss 0.000345 (0.010877) train_acc 100.000000 (99.611111)\n",
      "[17,  1900]  loss 0.004720 (0.010825) train_acc 100.000000 (99.615132)\n",
      "[17,  2000]  loss 0.000066 (0.011167) train_acc 100.000000 (99.618750)\n",
      "[17,  2100]  loss 0.000008 (0.011172) train_acc 100.000000 (99.616071)\n",
      "[17,  2200]  loss 0.000775 (0.011060) train_acc 100.000000 (99.622159)\n",
      "[17,  2300]  loss 0.001743 (0.010963) train_acc 100.000000 (99.622283)\n",
      "[17,  2400]  loss 0.015665 (0.011009) train_acc 100.000000 (99.617188)\n",
      "[17,  2500]  loss 0.002440 (0.011525) train_acc 100.000000 (99.597500)\n",
      "[17,  2600]  loss 0.002657 (0.011460) train_acc 100.000000 (99.600962)\n",
      "[17,  2700]  loss 0.010235 (0.011562) train_acc 100.000000 (99.604167)\n",
      "[17,  2800]  loss 0.002044 (0.011808) train_acc 100.000000 (99.600446)\n",
      "[17,  2900]  loss 0.000060 (0.011753) train_acc 100.000000 (99.601293)\n",
      "[17,  3000]  loss 0.065617 (0.011778) train_acc 93.750000 (99.593750)\n",
      "[17,  3100]  loss 0.000043 (0.011843) train_acc 100.000000 (99.588710)\n",
      "[17,  3200]  loss 0.001304 (0.011991) train_acc 100.000000 (99.580078)\n",
      "[17,  3300]  loss 0.031314 (0.011882) train_acc 100.000000 (99.585227)\n",
      "[17,  3400]  loss 0.000221 (0.011857) train_acc 100.000000 (99.590074)\n",
      "[17,  3500]  loss 0.000840 (0.011739) train_acc 100.000000 (99.596429)\n",
      "[17,  3600]  loss 0.000201 (0.011599) train_acc 100.000000 (99.600694)\n",
      "[17,  3700]  loss 0.004756 (0.011655) train_acc 100.000000 (99.599662)\n",
      "[18,   100]  loss 0.000019 (0.010182) train_acc 100.000000 (99.750000)\n",
      "[18,   200]  loss 0.007166 (0.010391) train_acc 100.000000 (99.687500)\n",
      "[18,   300]  loss 0.000261 (0.010244) train_acc 100.000000 (99.687500)\n",
      "[18,   400]  loss 0.000007 (0.011864) train_acc 100.000000 (99.640625)\n",
      "[18,   500]  loss 0.000073 (0.011616) train_acc 100.000000 (99.637500)\n",
      "[18,   600]  loss 0.017626 (0.012169) train_acc 100.000000 (99.604167)\n",
      "[18,   700]  loss 0.000874 (0.011467) train_acc 100.000000 (99.633929)\n",
      "[18,   800]  loss 0.000031 (0.011406) train_acc 100.000000 (99.648438)\n",
      "[18,   900]  loss 0.000041 (0.011052) train_acc 100.000000 (99.659722)\n",
      "[18,  1000]  loss 0.002699 (0.010436) train_acc 100.000000 (99.675000)\n",
      "[18,  1100]  loss 0.008351 (0.010426) train_acc 100.000000 (99.676136)\n",
      "[18,  1200]  loss 0.001500 (0.010238) train_acc 100.000000 (99.677083)\n",
      "[18,  1300]  loss 0.000245 (0.010178) train_acc 100.000000 (99.668269)\n",
      "[18,  1400]  loss 0.000494 (0.009753) train_acc 100.000000 (99.687500)\n",
      "[18,  1500]  loss 0.001026 (0.010019) train_acc 100.000000 (99.687500)\n",
      "[18,  1600]  loss 0.002216 (0.010024) train_acc 100.000000 (99.687500)\n",
      "[18,  1700]  loss 0.090677 (0.010880) train_acc 93.750000 (99.650735)\n",
      "[18,  1800]  loss 0.098287 (0.010846) train_acc 93.750000 (99.649306)\n",
      "[18,  1900]  loss 0.000350 (0.011600) train_acc 100.000000 (99.638158)\n",
      "[18,  2000]  loss 0.000786 (0.011631) train_acc 100.000000 (99.625000)\n",
      "[18,  2100]  loss 0.000077 (0.011455) train_acc 100.000000 (99.627976)\n",
      "[18,  2200]  loss 0.012682 (0.011480) train_acc 100.000000 (99.633523)\n",
      "[18,  2300]  loss 0.006986 (0.011201) train_acc 100.000000 (99.644022)\n",
      "[18,  2400]  loss 0.002494 (0.011065) train_acc 100.000000 (99.648438)\n",
      "[18,  2500]  loss 0.000133 (0.010889) train_acc 100.000000 (99.660000)\n",
      "[18,  2600]  loss 0.000115 (0.010702) train_acc 100.000000 (99.673077)\n",
      "[18,  2700]  loss 0.027657 (0.010749) train_acc 100.000000 (99.664352)\n",
      "[18,  2800]  loss 0.000022 (0.010629) train_acc 100.000000 (99.667411)\n",
      "[18,  2900]  loss 0.000303 (0.010664) train_acc 100.000000 (99.661638)\n",
      "[18,  3000]  loss 0.000378 (0.011226) train_acc 100.000000 (99.652083)\n",
      "[18,  3100]  loss 0.000427 (0.011316) train_acc 100.000000 (99.655242)\n",
      "[18,  3200]  loss 0.001624 (0.011279) train_acc 100.000000 (99.652344)\n",
      "[18,  3300]  loss 0.000361 (0.011191) train_acc 100.000000 (99.655303)\n",
      "[18,  3400]  loss 0.000304 (0.011171) train_acc 100.000000 (99.652574)\n",
      "[18,  3500]  loss 0.000187 (0.011129) train_acc 100.000000 (99.655357)\n",
      "[18,  3600]  loss 0.000045 (0.011028) train_acc 100.000000 (99.657986)\n",
      "[18,  3700]  loss 0.000032 (0.011136) train_acc 100.000000 (99.657095)\n",
      "[19,   100]  loss 0.036123 (0.012221) train_acc 100.000000 (99.750000)\n",
      "[19,   200]  loss 0.001875 (0.011322) train_acc 100.000000 (99.781250)\n",
      "[19,   300]  loss 0.011332 (0.008815) train_acc 100.000000 (99.833333)\n",
      "[19,   400]  loss 0.000219 (0.008470) train_acc 100.000000 (99.828125)\n",
      "[19,   500]  loss 0.000002 (0.009005) train_acc 100.000000 (99.787500)\n",
      "[19,   600]  loss 0.000013 (0.008956) train_acc 100.000000 (99.791667)\n",
      "[19,   700]  loss 0.000023 (0.009101) train_acc 100.000000 (99.776786)\n",
      "[19,   800]  loss 0.000408 (0.009043) train_acc 100.000000 (99.765625)\n",
      "[19,   900]  loss 0.000014 (0.008761) train_acc 100.000000 (99.777778)\n",
      "[19,  1000]  loss 0.000312 (0.009245) train_acc 100.000000 (99.750000)\n",
      "[19,  1100]  loss 0.000694 (0.010069) train_acc 100.000000 (99.710227)\n",
      "[19,  1200]  loss 0.002852 (0.009832) train_acc 100.000000 (99.708333)\n",
      "[19,  1300]  loss 0.001610 (0.009870) train_acc 100.000000 (99.706731)\n",
      "[19,  1400]  loss 0.000184 (0.010345) train_acc 100.000000 (99.687500)\n",
      "[19,  1500]  loss 0.003584 (0.010316) train_acc 100.000000 (99.683333)\n",
      "[19,  1600]  loss 0.000010 (0.010446) train_acc 100.000000 (99.683594)\n",
      "[19,  1700]  loss 0.003271 (0.010468) train_acc 100.000000 (99.672794)\n",
      "[19,  1800]  loss 0.000134 (0.010321) train_acc 100.000000 (99.677083)\n",
      "[19,  1900]  loss 0.000016 (0.010214) train_acc 100.000000 (99.684211)\n",
      "[19,  2000]  loss 0.000096 (0.010210) train_acc 100.000000 (99.684375)\n",
      "[19,  2100]  loss 0.077078 (0.010119) train_acc 100.000000 (99.690476)\n",
      "[19,  2200]  loss 0.000035 (0.010034) train_acc 100.000000 (99.701705)\n",
      "[19,  2300]  loss 0.001007 (0.010597) train_acc 100.000000 (99.695652)\n",
      "[19,  2400]  loss 0.000039 (0.010418) train_acc 100.000000 (99.703125)\n",
      "[19,  2500]  loss 0.015549 (0.010334) train_acc 100.000000 (99.705000)\n",
      "[19,  2600]  loss 0.000087 (0.010142) train_acc 100.000000 (99.713942)\n",
      "[19,  2700]  loss 0.000436 (0.009986) train_acc 100.000000 (99.719907)\n",
      "[19,  2800]  loss 0.000152 (0.009952) train_acc 100.000000 (99.720982)\n",
      "[19,  2900]  loss 0.000126 (0.009940) train_acc 100.000000 (99.721983)\n",
      "[19,  3000]  loss 0.000708 (0.009833) train_acc 100.000000 (99.718750)\n",
      "[19,  3100]  loss 0.001957 (0.009728) train_acc 100.000000 (99.719758)\n",
      "[19,  3200]  loss 0.017497 (0.009699) train_acc 100.000000 (99.716797)\n",
      "[19,  3300]  loss 0.006741 (0.009639) train_acc 100.000000 (99.717803)\n",
      "[19,  3400]  loss 0.000238 (0.009631) train_acc 100.000000 (99.715074)\n",
      "[19,  3500]  loss 0.000123 (0.009719) train_acc 100.000000 (99.712500)\n",
      "[19,  3600]  loss 0.013852 (0.009733) train_acc 100.000000 (99.711806)\n",
      "[19,  3700]  loss 0.001197 (0.009782) train_acc 100.000000 (99.704392)\n",
      "[20,   100]  loss 0.000052 (0.008502) train_acc 100.000000 (99.625000)\n",
      "[20,   200]  loss 0.002708 (0.008303) train_acc 100.000000 (99.718750)\n",
      "[20,   300]  loss 0.000095 (0.007205) train_acc 100.000000 (99.770833)\n",
      "[20,   400]  loss 0.000005 (0.007020) train_acc 100.000000 (99.750000)\n",
      "[20,   500]  loss 0.000072 (0.006563) train_acc 100.000000 (99.775000)\n",
      "[20,   600]  loss 0.000956 (0.008456) train_acc 100.000000 (99.697917)\n",
      "[20,   700]  loss 0.079741 (0.008333) train_acc 93.750000 (99.705357)\n",
      "[20,   800]  loss 0.001352 (0.008531) train_acc 100.000000 (99.703125)\n",
      "[20,   900]  loss 0.009675 (0.009090) train_acc 100.000000 (99.687500)\n",
      "[20,  1000]  loss 0.000148 (0.008616) train_acc 100.000000 (99.712500)\n",
      "[20,  1100]  loss 0.000571 (0.008192) train_acc 100.000000 (99.727273)\n",
      "[20,  1200]  loss 0.000159 (0.008423) train_acc 100.000000 (99.708333)\n",
      "[20,  1300]  loss 0.000005 (0.008455) train_acc 100.000000 (99.706731)\n",
      "[20,  1400]  loss 0.014062 (0.008322) train_acc 100.000000 (99.714286)\n",
      "[20,  1500]  loss 0.004506 (0.008157) train_acc 100.000000 (99.725000)\n",
      "[20,  1600]  loss 0.000427 (0.008100) train_acc 100.000000 (99.726562)\n",
      "[20,  1700]  loss 0.000005 (0.008371) train_acc 100.000000 (99.713235)\n",
      "[20,  1800]  loss 0.000022 (0.009012) train_acc 100.000000 (99.704861)\n",
      "[20,  1900]  loss 0.000255 (0.008911) train_acc 100.000000 (99.713816)\n",
      "[20,  2000]  loss 0.001027 (0.008928) train_acc 100.000000 (99.709375)\n",
      "[20,  2100]  loss 0.027766 (0.008718) train_acc 100.000000 (99.717262)\n",
      "[20,  2200]  loss 0.000045 (0.008611) train_acc 100.000000 (99.724432)\n",
      "[20,  2300]  loss 0.000146 (0.008525) train_acc 100.000000 (99.730978)\n",
      "[20,  2400]  loss 0.030528 (0.008369) train_acc 100.000000 (99.736979)\n",
      "[20,  2500]  loss 0.001923 (0.008522) train_acc 100.000000 (99.727500)\n",
      "[20,  2600]  loss 0.000201 (0.008757) train_acc 100.000000 (99.725962)\n",
      "[20,  2700]  loss 0.005755 (0.008995) train_acc 100.000000 (99.715278)\n",
      "[20,  2800]  loss 0.000014 (0.009294) train_acc 100.000000 (99.709821)\n",
      "[20,  2900]  loss 0.001007 (0.009268) train_acc 100.000000 (99.713362)\n",
      "[20,  3000]  loss 0.007941 (0.009176) train_acc 100.000000 (99.718750)\n",
      "[20,  3100]  loss 0.000042 (0.009438) train_acc 100.000000 (99.707661)\n",
      "[20,  3200]  loss 0.000907 (0.009281) train_acc 100.000000 (99.712891)\n",
      "[20,  3300]  loss 0.000013 (0.009187) train_acc 100.000000 (99.719697)\n",
      "[20,  3400]  loss 0.000000 (0.009156) train_acc 100.000000 (99.720588)\n",
      "[20,  3500]  loss 0.003950 (0.009291) train_acc 100.000000 (99.712500)\n",
      "[20,  3600]  loss 0.000783 (0.009397) train_acc 100.000000 (99.704861)\n",
      "[20,  3700]  loss 0.000032 (0.009394) train_acc 100.000000 (99.704392)\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_data(train=True)\n",
    "\n",
    "train(model=net, dataloader=trainloader, cuda=torch.cuda.is_available(), q=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.88% - FP32\n"
     ]
    }
   ],
   "source": [
    "testloader = load_data(train=False)\n",
    "\n",
    "score = test(model=net, dataloader=testloader, cuda=torch.cuda.is_available())\n",
    "print(f\"Accuracy of the network on the test images: {score}% - FP32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new quantized network architecture. Next we'll \"fuse models\", this can both make the model faster by saving on memory access while also improving numercial accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 0.4428,  0.2118,  0.2768,  0.0157,  0.1945],\n",
       "                        [ 0.1524,  0.5733,  0.5056,  0.5796,  0.4472],\n",
       "                        [ 0.2082,  0.2719,  0.3475,  0.1671,  0.1293],\n",
       "                        [-0.6373, -0.5048, -0.3163, -0.5148, -0.1139],\n",
       "                        [-0.5653, -0.6353, -0.5980, -0.4194, -0.1856]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1060, -0.1101, -0.2690, -0.2815, -0.3092],\n",
       "                        [-0.0744, -0.3842, -0.5379, -0.5686, -0.1313],\n",
       "                        [-0.2139, -0.4757, -0.2699, -0.2235, -0.0287],\n",
       "                        [-0.0538, -0.2168, -0.3401, -0.0597,  0.3398],\n",
       "                        [ 0.0278,  0.1061,  0.4902,  0.4629,  0.4011]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2204, -0.2499, -0.2926,  0.0900, -0.0302],\n",
       "                        [ 0.0470,  0.0654,  0.1472,  0.1214, -0.1123],\n",
       "                        [ 0.3897,  0.1482,  0.4610,  0.3757,  0.5284],\n",
       "                        [-0.1360,  0.1774, -0.0484, -0.1533,  0.1665],\n",
       "                        [-0.2194, -0.0612, -0.2165, -0.5327, -0.4153]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4647,  0.0852,  0.0218, -0.2276,  0.0860],\n",
       "                        [ 0.2887,  0.2762, -0.2456, -0.1757, -0.1273],\n",
       "                        [ 0.6765,  0.1084, -0.0660, -0.4578, -0.3067],\n",
       "                        [ 0.5517,  0.0103, -0.3145, -0.2681, -0.0729],\n",
       "                        [ 0.3570,  0.1031, -0.0201, -0.2431, -0.1076]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.6527,  0.0383,  0.3695,  0.2189,  0.0906],\n",
       "                        [-0.1196,  0.7292,  0.4463, -0.0447, -0.0227],\n",
       "                        [ 0.4558,  0.5622, -0.2620, -0.3094, -0.2524],\n",
       "                        [ 0.3099, -0.1591, -0.4565, -0.0753, -0.2735],\n",
       "                        [-0.2583,  0.0294,  0.1495, -0.0312,  0.1902]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1729,  0.5543,  0.5594, -0.0561, -0.2682],\n",
       "                        [ 0.2519,  0.5972,  0.0806,  0.0633,  0.0106],\n",
       "                        [ 0.1794,  0.4413,  0.0484, -0.2959, -0.2221],\n",
       "                        [ 0.1889,  0.0728, -0.2175, -0.2404, -0.0937],\n",
       "                        [-0.1862,  0.0809, -0.2449, -0.3597, -0.3535]]]], device='cuda:0')),\n",
       "             ('conv1.bias',\n",
       "              tensor([ 0.2607, -0.0862,  0.0012,  0.1082,  0.4752,  0.4794], device='cuda:0')),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[ 9.4076e-02,  7.8537e-02, -6.2959e-02, -1.6783e-01, -2.4314e-01],\n",
       "                        [ 4.1892e-02,  8.3288e-02,  3.6569e-02, -1.3824e-01, -1.8680e-01],\n",
       "                        [ 4.5581e-03,  3.2515e-02, -5.5086e-02, -1.1820e-01, -2.6430e-01],\n",
       "                        [-8.8884e-02, -3.7494e-02, -2.0413e-01, -3.7985e-02, -8.7395e-02],\n",
       "                        [ 1.5336e-01, -3.3057e-02, -7.8466e-02,  1.0611e-01,  1.8435e-01]],\n",
       "              \n",
       "                       [[-4.0320e-02, -1.0804e-01, -9.4611e-02,  1.6191e-02, -4.2234e-02],\n",
       "                        [-5.1107e-02, -1.8279e-01, -1.1766e-01, -3.6080e-02, -1.3344e-02],\n",
       "                        [-7.4426e-03,  1.4192e-02,  3.9562e-02, -1.1262e-01,  3.4835e-02],\n",
       "                        [ 2.0714e-01,  2.0739e-01,  1.2514e-01,  2.4670e-02,  1.8235e-02],\n",
       "                        [-2.8873e-02, -4.4889e-02, -4.1747e-02,  4.1646e-02, -1.2029e-01]],\n",
       "              \n",
       "                       [[-5.8023e-02, -1.1298e-01, -7.2197e-02, -1.1074e-01, -3.5236e-02],\n",
       "                        [ 6.0664e-02, -9.3002e-02, -1.7178e-01, -1.1636e-02, -5.5738e-02],\n",
       "                        [ 4.5919e-03, -9.1669e-02, -1.6908e-01, -7.1388e-02, -2.2860e-02],\n",
       "                        [-7.9193e-02, -9.9413e-02,  3.7017e-02,  4.9899e-02, -7.8206e-03],\n",
       "                        [ 3.9150e-02,  2.9515e-02, -8.9181e-03, -4.3358e-02,  1.0748e-01]],\n",
       "              \n",
       "                       [[ 4.1736e-02, -5.6073e-02,  1.5886e-01,  2.0347e-01,  5.9496e-02],\n",
       "                        [ 2.3539e-03,  1.3599e-02, -1.1790e-02,  2.1278e-01,  2.0883e-01],\n",
       "                        [-4.5591e-02, -6.8870e-02,  3.2308e-02,  1.4676e-01,  2.2922e-01],\n",
       "                        [-1.3594e-01, -1.4740e-01,  1.0137e-02,  3.7157e-02,  1.3586e-01],\n",
       "                        [-2.9241e-02, -1.2407e-01,  1.7894e-02,  8.4867e-02,  6.1236e-02]],\n",
       "              \n",
       "                       [[-7.5946e-02,  4.0760e-02,  7.0116e-02, -1.2989e-02, -1.7136e-01],\n",
       "                        [-1.1943e-01, -8.4188e-02, -5.6343e-02, -3.2304e-02, -1.1776e-01],\n",
       "                        [-1.6524e-01, -1.7759e-01, -2.3767e-01,  4.2606e-02, -5.1466e-02],\n",
       "                        [-1.3373e-01, -6.3293e-02,  8.9493e-03, -3.4418e-02,  3.7586e-02],\n",
       "                        [-1.2784e-02,  1.4515e-01,  8.2455e-03,  8.0320e-02,  1.8621e-03]],\n",
       "              \n",
       "                       [[-5.8497e-02, -1.4006e-01,  8.3637e-02, -6.8302e-02, -6.2722e-02],\n",
       "                        [-1.9398e-01, -1.5567e-01,  9.7357e-02,  3.5807e-02, -2.9650e-02],\n",
       "                        [-1.0326e-01, -1.0073e-01, -1.0477e-02,  2.1124e-01,  1.2026e-01],\n",
       "                        [-1.5533e-01, -8.3763e-02,  8.0374e-02,  7.8031e-02,  1.5389e-01],\n",
       "                        [-1.5508e-02, -4.0241e-03,  1.5766e-01,  1.9433e-01,  2.2445e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0739e-01, -2.2853e-01, -2.0129e-01, -1.7986e-01, -6.1320e-02],\n",
       "                        [ 1.8369e-01, -2.1208e-02, -1.9789e-02, -3.5179e-02,  9.0667e-02],\n",
       "                        [ 1.4866e-01,  1.6560e-01,  2.8847e-02,  1.3607e-01,  2.1937e-01],\n",
       "                        [-6.5579e-02,  1.8874e-01,  3.8285e-01,  2.4055e-01,  7.9761e-02],\n",
       "                        [-9.1545e-02, -1.5908e-01, -3.5561e-02, -4.1941e-02, -1.2550e-01]],\n",
       "              \n",
       "                       [[ 7.4849e-03, -1.9230e-02,  4.9244e-03, -5.8354e-02,  6.5317e-03],\n",
       "                        [-7.6727e-02, -8.2249e-02,  4.9484e-02,  1.5726e-01,  1.0293e-01],\n",
       "                        [-1.7279e-01, -8.9176e-02, -1.1023e-02,  1.0373e-01,  3.3543e-04],\n",
       "                        [-1.1574e-03, -1.0197e-01, -5.9413e-02, -1.6698e-02, -6.9214e-02],\n",
       "                        [ 8.1743e-02,  1.2198e-01,  1.1937e-01,  1.8711e-02,  2.4200e-02]],\n",
       "              \n",
       "                       [[-2.4211e-02,  7.3654e-03, -1.0590e-01, -7.7405e-02,  5.8162e-02],\n",
       "                        [ 1.3717e-01,  8.6320e-03,  8.8264e-02,  1.0883e-01,  3.4934e-02],\n",
       "                        [ 1.1112e-01,  1.5872e-01,  1.4641e-01,  1.9192e-01,  7.6612e-02],\n",
       "                        [-9.8480e-03, -2.1855e-02,  1.3533e-01,  7.8251e-02,  8.6571e-02],\n",
       "                        [-5.7495e-02,  1.7087e-02,  6.4482e-02, -9.4104e-02, -2.5465e-02]],\n",
       "              \n",
       "                       [[-5.8461e-02,  5.6473e-03, -1.8675e-02, -8.4152e-02, -7.5491e-02],\n",
       "                        [-5.0306e-02,  2.7080e-02,  3.5796e-02, -4.0931e-02, -8.9537e-02],\n",
       "                        [-9.4124e-03,  1.7144e-02,  6.7012e-02,  2.2479e-02, -9.3459e-02],\n",
       "                        [ 9.5729e-02, -1.0205e-01, -6.1791e-02,  4.7090e-02, -5.1470e-02],\n",
       "                        [ 7.1560e-02,  2.9045e-02, -5.3138e-02, -8.4519e-02, -2.9554e-02]],\n",
       "              \n",
       "                       [[ 1.2571e-01,  6.5503e-02, -1.4863e-02, -2.2501e-03, -6.0843e-02],\n",
       "                        [-1.1951e-01, -7.6924e-02, -1.3871e-01, -7.0525e-02, -6.6745e-02],\n",
       "                        [-1.0472e-01, -1.4294e-01,  6.1542e-02, -2.3196e-02,  7.7154e-02],\n",
       "                        [-3.6393e-02, -1.3619e-01, -8.0599e-02, -3.4209e-03, -1.3144e-02],\n",
       "                        [-1.9961e-01, -2.1328e-01, -9.7060e-02, -5.3746e-02, -2.9494e-02]],\n",
       "              \n",
       "                       [[-3.6586e-02, -9.9914e-02, -4.9842e-02, -2.1699e-01,  4.3268e-03],\n",
       "                        [-7.7048e-02,  4.9422e-02, -5.0911e-02,  2.1594e-02, -5.3786e-02],\n",
       "                        [-8.2101e-02,  8.6607e-02,  6.4018e-02,  2.7329e-02, -2.2020e-02],\n",
       "                        [-5.7828e-02, -2.7048e-02,  1.3038e-01,  1.3460e-01, -3.3609e-03],\n",
       "                        [-1.3194e-01, -4.5858e-02, -3.9626e-02, -7.4508e-02, -1.2196e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5770e-01, -4.0732e-02, -1.2289e-01,  2.4850e-02,  5.6609e-02],\n",
       "                        [-1.0838e-01, -1.4037e-01,  4.6267e-02,  2.4073e-02,  8.4907e-02],\n",
       "                        [ 1.7708e-02, -5.2570e-02, -1.1149e-02, -5.2020e-02,  2.1980e-04],\n",
       "                        [ 1.3595e-01, -6.4608e-02, -9.7863e-02, -2.9302e-02, -1.1371e-02],\n",
       "                        [-1.1988e-02,  9.3964e-02,  6.6041e-02, -4.4957e-02,  3.5403e-02]],\n",
       "              \n",
       "                       [[-1.1477e-01, -2.5944e-02,  1.3235e-02, -4.0207e-02,  4.9558e-02],\n",
       "                        [-1.5104e-01, -1.0048e-01, -7.7928e-02, -3.6021e-02, -9.8592e-02],\n",
       "                        [-2.4801e-01, -2.3442e-01, -3.8792e-02, -9.0369e-03,  1.9725e-02],\n",
       "                        [-9.6714e-02, -8.2997e-02,  3.6388e-02,  2.0309e-02,  1.4855e-01],\n",
       "                        [ 5.6102e-02, -1.1070e-01, -1.4261e-01,  6.3161e-02,  1.0609e-01]],\n",
       "              \n",
       "                       [[-1.1315e-01,  4.2101e-03,  6.8092e-02,  1.1863e-01,  2.1206e-02],\n",
       "                        [-1.7410e-01, -6.1973e-02,  3.4488e-02,  1.2037e-01,  6.6028e-02],\n",
       "                        [ 4.3104e-02, -8.6863e-04,  2.7398e-02, -4.4761e-02,  9.0448e-04],\n",
       "                        [ 3.6574e-02, -7.3163e-02,  2.7630e-02,  2.3108e-02,  2.8618e-02],\n",
       "                        [ 4.2544e-02, -7.3836e-04, -5.6317e-02, -7.5655e-02, -3.1691e-02]],\n",
       "              \n",
       "                       [[ 7.7397e-02,  1.3413e-01, -3.5482e-02,  4.2018e-02, -5.4142e-02],\n",
       "                        [ 7.9122e-02,  1.2793e-01,  6.1826e-02,  8.4649e-02,  2.8496e-02],\n",
       "                        [ 1.6939e-02,  2.3355e-01,  1.0821e-01,  8.3875e-02,  1.3562e-02],\n",
       "                        [ 6.8392e-02,  1.7991e-01, -2.5611e-02,  3.6763e-02,  4.2401e-03],\n",
       "                        [-6.2798e-03,  1.0461e-01, -3.8649e-02, -1.0609e-01, -5.1882e-02]],\n",
       "              \n",
       "                       [[-3.4654e-04,  6.1132e-02, -5.9425e-02,  6.9586e-02,  3.8754e-02],\n",
       "                        [ 4.8079e-02,  1.4038e-01,  1.3533e-01,  5.3257e-02, -2.1577e-02],\n",
       "                        [-6.8271e-02,  1.3775e-01,  7.7522e-02, -1.4413e-01, -5.5893e-02],\n",
       "                        [ 5.6817e-02,  1.6072e-02, -1.1185e-01, -4.3575e-02, -5.8207e-02],\n",
       "                        [ 6.9395e-02,  4.1672e-03,  7.4426e-03, -4.5580e-02, -9.0454e-02]],\n",
       "              \n",
       "                       [[ 2.2662e-02,  6.1408e-02,  4.7782e-02,  4.0639e-02,  1.0416e-02],\n",
       "                        [ 1.1207e-01,  6.5563e-02,  9.2985e-02,  9.6984e-02, -2.0392e-02],\n",
       "                        [-3.7811e-02,  1.8122e-01,  7.8338e-02, -3.3589e-02, -1.1835e-01],\n",
       "                        [ 1.4186e-01,  2.0920e-01, -6.4761e-02, -1.0933e-01, -9.1657e-02],\n",
       "                        [-8.7068e-03,  5.1539e-02, -3.2257e-03,  3.0629e-02, -9.8704e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.2702e-01, -4.1668e-02, -7.2188e-02,  5.8947e-02, -1.1596e-02],\n",
       "                        [-1.6639e-01,  4.8748e-02, -1.3232e-02, -3.3999e-02, -9.4325e-02],\n",
       "                        [-1.0085e-01, -1.3402e-01, -6.1796e-03, -1.0312e-01, -5.1508e-03],\n",
       "                        [ 1.3795e-01, -4.8287e-02, -1.2833e-01, -1.8351e-01, -4.4193e-02],\n",
       "                        [ 2.1913e-01,  2.5307e-01,  1.1328e-01, -1.2378e-01, -7.4681e-02]],\n",
       "              \n",
       "                       [[-6.5648e-02,  8.0942e-02, -1.7076e-02,  7.1267e-02,  1.5460e-01],\n",
       "                        [-6.8478e-02, -7.2956e-02, -6.3238e-03,  1.3515e-01,  9.0655e-02],\n",
       "                        [-1.6285e-02,  2.9528e-02,  1.2609e-01,  1.2431e-01, -7.5148e-02],\n",
       "                        [-1.5283e-01,  1.0149e-01,  1.2371e-01,  3.7647e-02,  2.4090e-02],\n",
       "                        [-2.9817e-01, -2.2393e-01, -8.9318e-02,  7.8897e-03,  4.5263e-02]],\n",
       "              \n",
       "                       [[ 8.2167e-02,  5.5843e-02,  1.2089e-01, -3.0625e-02,  2.3629e-02],\n",
       "                        [-4.1671e-02,  4.3612e-02,  9.3745e-02,  7.0511e-02,  2.7947e-02],\n",
       "                        [-2.5083e-02, -9.3537e-02, -4.6375e-02, -2.0714e-02,  9.5125e-02],\n",
       "                        [ 1.3625e-02, -3.2170e-02,  1.3518e-03, -4.5221e-02,  3.7307e-02],\n",
       "                        [ 2.7213e-02,  7.1000e-02,  9.4463e-02,  9.5065e-02,  8.4927e-02]],\n",
       "              \n",
       "                       [[-4.6768e-02, -1.2243e-01,  7.7978e-03,  3.2117e-02, -4.0894e-03],\n",
       "                        [ 2.2275e-02,  6.0235e-02, -1.1978e-01, -3.7049e-02, -1.3382e-02],\n",
       "                        [ 1.7360e-01,  1.8236e-01, -4.4263e-02, -1.8721e-02,  2.0699e-02],\n",
       "                        [ 3.2069e-04,  9.8897e-02,  5.9899e-02, -1.0898e-02, -6.3890e-03],\n",
       "                        [-9.8679e-02, -1.0018e-01,  3.5770e-02,  8.2347e-02,  3.0960e-02]],\n",
       "              \n",
       "                       [[ 2.6710e-02,  1.9522e-02,  7.1214e-02,  1.8043e-01,  4.1209e-02],\n",
       "                        [ 2.2361e-02, -4.2318e-02,  1.7617e-02, -1.8469e-02, -3.5619e-02],\n",
       "                        [ 3.3144e-02, -5.2401e-02, -3.9902e-02,  1.1489e-02,  1.2445e-02],\n",
       "                        [ 3.9174e-02, -6.4772e-02,  5.9191e-02,  6.5496e-02,  7.1315e-02],\n",
       "                        [-6.5663e-02, -1.8048e-02,  2.8649e-02, -1.2478e-03, -5.4991e-03]],\n",
       "              \n",
       "                       [[ 6.2794e-02, -1.1720e-01,  1.6871e-02, -3.3571e-02, -1.3581e-03],\n",
       "                        [ 5.7024e-02, -8.3740e-02, -1.0195e-02,  3.6731e-02, -1.1049e-01],\n",
       "                        [-4.8568e-02, -8.7889e-02, -8.0013e-02, -2.7151e-02, -1.6447e-02],\n",
       "                        [-1.0766e-01, -1.1387e-01,  1.1098e-02, -8.6575e-02, -5.8627e-03],\n",
       "                        [ 1.3522e-02,  9.4565e-02,  6.2227e-02,  1.2117e-01, -9.1636e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3074e-01, -9.4922e-02, -4.1044e-02, -1.3627e-01, -2.0539e-01],\n",
       "                        [ 2.7539e-02,  9.4864e-02, -1.0623e-01, -2.2396e-01, -1.0953e-01],\n",
       "                        [ 1.8690e-01,  1.3576e-01, -2.2978e-01, -1.7932e-01,  3.4072e-02],\n",
       "                        [ 1.2769e-01, -2.6095e-01, -2.0454e-01, -6.0848e-02,  6.2998e-02],\n",
       "                        [-1.4292e-01, -2.7428e-01, -3.4749e-02,  4.8263e-02, -2.2134e-02]],\n",
       "              \n",
       "                       [[ 2.2102e-02,  1.4873e-01, -2.8040e-02, -1.4715e-01,  5.2388e-02],\n",
       "                        [ 3.8099e-02, -9.9669e-02,  3.1811e-02, -8.9303e-02, -6.0944e-03],\n",
       "                        [ 5.5204e-03,  2.4838e-02,  1.3327e-01, -1.2279e-02, -1.5405e-01],\n",
       "                        [ 9.6594e-02,  8.1819e-02, -1.0888e-01, -3.6020e-02, -3.3981e-02],\n",
       "                        [ 1.7106e-02, -1.0168e-01,  4.9698e-03, -6.3871e-02,  1.6733e-02]],\n",
       "              \n",
       "                       [[-1.0603e-01, -2.4230e-02, -6.4560e-02, -6.0392e-02, -4.5004e-03],\n",
       "                        [ 1.4958e-02,  7.8216e-02, -1.1012e-01,  6.3502e-02, -9.8566e-03],\n",
       "                        [ 4.6535e-02, -1.3406e-01, -5.9399e-02,  4.1064e-02,  1.0890e-01],\n",
       "                        [-3.8873e-02, -7.7181e-02,  1.3044e-01, -1.9347e-02, -1.6559e-02],\n",
       "                        [-1.0478e-01,  1.8242e-02,  6.8461e-02, -3.0893e-03,  5.7627e-02]],\n",
       "              \n",
       "                       [[ 9.6429e-02,  1.3427e-01, -1.0496e-01, -7.3113e-02,  1.3228e-01],\n",
       "                        [ 1.8655e-02,  4.5402e-02, -1.7307e-01, -6.0999e-02,  1.2076e-01],\n",
       "                        [ 3.8291e-02, -6.3115e-02, -1.1575e-01, -1.7527e-02,  9.6210e-02],\n",
       "                        [-4.0154e-02, -1.1101e-01,  2.9446e-02,  6.1171e-02, -5.4311e-02],\n",
       "                        [ 2.1762e-02,  3.7312e-02,  1.3700e-01,  6.3951e-02, -1.9650e-02]],\n",
       "              \n",
       "                       [[-8.8339e-02, -6.2771e-03, -1.1028e-01, -1.2598e-01, -1.6672e-02],\n",
       "                        [ 4.8316e-02, -1.2905e-01, -1.6228e-01, -4.1055e-02,  3.0739e-02],\n",
       "                        [ 1.9290e-01,  8.9118e-02, -4.2497e-02,  1.2321e-01,  2.6334e-01],\n",
       "                        [ 4.4793e-02,  4.9106e-02,  1.9783e-01,  4.0368e-01, -5.6211e-03],\n",
       "                        [-1.8631e-02,  8.3333e-02,  2.8789e-01, -1.8002e-02,  9.3813e-02]],\n",
       "              \n",
       "                       [[ 1.7040e-02,  1.8057e-02, -7.7769e-02, -9.2173e-02,  1.7991e-01],\n",
       "                        [ 7.2045e-02,  3.8497e-02, -4.9923e-02,  3.8812e-02,  1.5532e-01],\n",
       "                        [ 1.5248e-01, -9.8601e-03, -2.1943e-02,  1.2505e-01,  8.3257e-02],\n",
       "                        [ 2.8543e-02, -3.1945e-02,  2.1369e-02,  9.8517e-02,  1.3764e-02],\n",
       "                        [-3.6308e-02,  1.3805e-01,  6.2545e-02, -5.6190e-02, -8.9285e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4446e-01,  1.1019e-01,  2.1530e-01,  2.5246e-01,  2.0487e-01],\n",
       "                        [ 1.0618e-01,  5.1912e-02, -9.9469e-02, -9.4255e-02, -6.8209e-02],\n",
       "                        [ 5.6489e-02, -1.8884e-02,  1.0091e-02, -8.9743e-02, -5.3717e-02],\n",
       "                        [ 6.7243e-02,  1.0770e-02, -8.2329e-02, -1.0033e-01, -2.5529e-02],\n",
       "                        [-7.8601e-02, -3.2634e-02,  1.5728e-02,  1.9481e-01, -4.8090e-02]],\n",
       "              \n",
       "                       [[-5.6843e-02, -7.9023e-02,  5.8523e-04, -5.2132e-02, -1.9588e-01],\n",
       "                        [-9.0991e-02,  5.6029e-02,  7.0927e-02, -1.8100e-02, -1.1561e-01],\n",
       "                        [ 1.4492e-01,  1.4739e-01,  1.6231e-01,  1.2442e-01,  4.3291e-02],\n",
       "                        [-1.3651e-02,  1.1911e-01,  1.2387e-01,  1.4514e-01,  1.7580e-01],\n",
       "                        [-3.0547e-02,  1.2411e-02, -2.0530e-01, -8.8593e-02, -7.5812e-02]],\n",
       "              \n",
       "                       [[ 1.3660e-01,  5.8045e-02,  5.9765e-02,  7.8915e-02,  1.8679e-03],\n",
       "                        [ 3.8116e-02, -4.3061e-02, -6.0427e-02, -8.0273e-02, -9.3116e-02],\n",
       "                        [ 4.5754e-02,  4.9348e-02,  2.4726e-02,  4.3535e-03, -1.9883e-02],\n",
       "                        [ 2.1124e-02,  1.0560e-01,  5.3502e-02,  9.3066e-02,  4.3876e-02],\n",
       "                        [-4.4372e-02,  8.8847e-02,  1.6157e-01,  1.0617e-01, -7.8522e-02]],\n",
       "              \n",
       "                       [[-4.6114e-02,  6.9471e-02,  2.1021e-02,  3.9845e-02, -1.7832e-03],\n",
       "                        [ 1.7717e-02, -3.8937e-02, -9.9994e-02, -9.7764e-02, -1.4852e-01],\n",
       "                        [-9.0843e-02, -9.9355e-02,  2.1249e-03, -1.5820e-01, -2.0984e-02],\n",
       "                        [-7.7775e-02, -1.1715e-01, -4.2872e-02,  1.1133e-02, -1.8183e-02],\n",
       "                        [ 1.8220e-02, -6.6656e-02, -4.5954e-02,  3.6975e-02, -2.9026e-02]],\n",
       "              \n",
       "                       [[ 1.0227e-01,  8.5798e-02, -2.5496e-03, -4.1407e-02,  6.1742e-03],\n",
       "                        [ 3.6876e-03, -1.6028e-01, -1.4256e-01, -8.9991e-02, -1.1410e-01],\n",
       "                        [-1.3536e-02, -8.6745e-03,  1.1814e-03, -6.2235e-02, -9.7301e-02],\n",
       "                        [ 7.5625e-03,  1.5369e-02, -1.3119e-01, -1.1510e-02, -1.7605e-01],\n",
       "                        [-7.0979e-02, -4.2116e-03, -7.4949e-02, -6.2506e-02, -1.8112e-02]],\n",
       "              \n",
       "                       [[ 5.9289e-02,  1.8044e-01,  6.0415e-02,  4.4719e-02, -2.9945e-02],\n",
       "                        [ 6.1332e-02,  6.5816e-02, -7.5752e-02, -9.5994e-02, -3.7051e-02],\n",
       "                        [ 6.6173e-02, -3.4193e-02, -1.3820e-01, -1.5852e-01, -1.5312e-01],\n",
       "                        [-3.0911e-02, -9.4661e-02, -8.6307e-02, -1.2913e-01, -1.3239e-01],\n",
       "                        [ 6.4692e-02,  3.0407e-02,  1.0343e-02, -5.9670e-02, -6.0395e-02]]]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.bias',\n",
       "              tensor([ 0.0825,  0.0316,  0.0627, -0.0144, -0.0370,  0.0057,  0.0078, -0.0621,\n",
       "                      -0.0033, -0.0097, -0.0245, -0.0453, -0.0478, -0.0045,  0.0412,  0.0164],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0645, -0.0720, -0.0496,  ..., -0.0451,  0.0250, -0.0642],\n",
       "                      [ 0.0026,  0.0241, -0.0500,  ...,  0.0030,  0.0060, -0.0569],\n",
       "                      [ 0.0676, -0.0383, -0.0181,  ..., -0.0163,  0.0292,  0.1041],\n",
       "                      ...,\n",
       "                      [-0.0953, -0.0158, -0.0391,  ..., -0.0042, -0.0781, -0.1482],\n",
       "                      [-0.0386,  0.0887, -0.0606,  ...,  0.0185, -0.0683,  0.0310],\n",
       "                      [ 0.0146,  0.0485, -0.0632,  ..., -0.0358,  0.0155,  0.0361]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0126,  0.0579,  0.0505,  0.0546,  0.0252,  0.0472,  0.0304, -0.0039,\n",
       "                      -0.0614, -0.0462,  0.0486,  0.0656, -0.0337, -0.0482,  0.0429,  0.0397,\n",
       "                       0.0528, -0.0588, -0.0448,  0.0005,  0.0029, -0.0554,  0.0231, -0.0397,\n",
       "                       0.0675,  0.0325,  0.0803, -0.0340,  0.0072, -0.0476, -0.0435, -0.0036,\n",
       "                      -0.0108,  0.0341,  0.0361, -0.0306,  0.0193,  0.0326,  0.0229, -0.0300,\n",
       "                       0.0226,  0.0593, -0.0411,  0.0340, -0.0024, -0.0179,  0.0490, -0.0125,\n",
       "                       0.0595, -0.0283, -0.0583,  0.0486,  0.0091,  0.0083, -0.0511, -0.0438,\n",
       "                       0.0345, -0.0597,  0.0638, -0.0333,  0.0032,  0.0192, -0.0456,  0.0675,\n",
       "                      -0.0129,  0.0183, -0.0592,  0.0312,  0.0012, -0.0379, -0.0302, -0.0314,\n",
       "                       0.0168, -0.0034, -0.0441,  0.0212,  0.0087, -0.0101,  0.0562, -0.0543,\n",
       "                      -0.0349, -0.0085,  0.0131, -0.0053,  0.0108, -0.0647, -0.0367, -0.0194,\n",
       "                      -0.0442,  0.0242, -0.0117,  0.0265,  0.0519,  0.0105, -0.0309, -0.0238,\n",
       "                      -0.0075,  0.0530, -0.0488,  0.0840, -0.0077,  0.0716, -0.0266, -0.0508,\n",
       "                       0.0216,  0.0321, -0.0236,  0.0215, -0.0134, -0.0386, -0.0090,  0.0648,\n",
       "                       0.0157, -0.0341, -0.0400,  0.0461, -0.0387,  0.0100,  0.0487,  0.0057],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0292, -0.0741, -0.0581,  ..., -0.1325, -0.1013, -0.0458],\n",
       "                      [-0.0452,  0.0075, -0.0412,  ..., -0.0195, -0.0691, -0.0193],\n",
       "                      [ 0.0389, -0.0799, -0.0498,  ..., -0.0695, -0.0588,  0.0580],\n",
       "                      ...,\n",
       "                      [-0.0051, -0.0754,  0.1073,  ...,  0.0018,  0.1083,  0.0436],\n",
       "                      [-0.0274, -0.0907,  0.0188,  ..., -0.0281, -0.0390,  0.0890],\n",
       "                      [ 0.0388, -0.0023,  0.0269,  ...,  0.0506, -0.0486, -0.0842]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 0.0267, -0.0795,  0.0490,  0.0682, -0.0412,  0.0465,  0.0831,  0.0488,\n",
       "                      -0.0511,  0.0116,  0.0228,  0.0773, -0.0842,  0.0448, -0.1013,  0.1017,\n",
       "                       0.0352, -0.0685, -0.0659,  0.1078, -0.0846,  0.0810,  0.0232,  0.0744,\n",
       "                      -0.0007,  0.0608, -0.0546, -0.0658, -0.0189, -0.0409, -0.0421, -0.0746,\n",
       "                       0.0250,  0.0429,  0.0489,  0.0137,  0.0144, -0.0711,  0.0166,  0.0733,\n",
       "                      -0.0540,  0.0169, -0.0325,  0.0623,  0.0535,  0.0788, -0.0787,  0.1137,\n",
       "                      -0.0728, -0.0566, -0.0332, -0.0536,  0.0112, -0.0534,  0.0450,  0.0577,\n",
       "                       0.0628, -0.0353, -0.0587,  0.0568, -0.0339, -0.0087,  0.1105,  0.0027,\n",
       "                      -0.0841,  0.0689, -0.0767, -0.0571,  0.0785,  0.0148,  0.0302,  0.0769,\n",
       "                       0.1001, -0.0842,  0.0166,  0.0516,  0.1207, -0.0394, -0.0887,  0.0836,\n",
       "                       0.0082,  0.0912,  0.0605, -0.0159], device='cuda:0')),\n",
       "             ('fc3.weight',\n",
       "              tensor([[ 0.1153,  0.1119,  0.1930, -0.1131, -0.2268, -0.2304,  0.1412, -0.0373,\n",
       "                        0.1121, -0.1321,  0.0428,  0.0367, -0.0057, -0.0048, -0.1898, -0.1066,\n",
       "                       -0.1234,  0.1631, -0.0728, -0.1496,  0.0321, -0.0979, -0.1745, -0.1304,\n",
       "                       -0.1672,  0.3353,  0.0375,  0.1997,  0.0242,  0.1219,  0.0428,  0.0571,\n",
       "                       -0.0840,  0.2429,  0.0651,  0.1469,  0.0061, -0.1055, -0.2093, -0.1648,\n",
       "                       -0.0383, -0.0913, -0.0196, -0.0091,  0.2705, -0.1933,  0.1784,  0.1744,\n",
       "                        0.0550, -0.0453, -0.0195,  0.0454,  0.0423, -0.0752, -0.1027,  0.1225,\n",
       "                       -0.0778, -0.1015,  0.0626,  0.1570,  0.0134,  0.0949, -0.0913, -0.1145,\n",
       "                       -0.1327, -0.2001, -0.1004,  0.1084, -0.1388, -0.0623,  0.2830, -0.1907,\n",
       "                        0.3980, -0.0552, -0.0117, -0.0045, -0.3820,  0.0691, -0.0126, -0.1003,\n",
       "                       -0.0444, -0.1320,  0.1774,  0.0347],\n",
       "                      [ 0.1756,  0.0585,  0.1445,  0.1568,  0.2536, -0.2705, -0.0431,  0.1718,\n",
       "                       -0.0399, -0.2172, -0.3215, -0.2162, -0.0322,  0.3715,  0.0569,  0.2429,\n",
       "                        0.0959, -0.0603, -0.0143, -0.0117,  0.1103, -0.1267, -0.1587, -0.0644,\n",
       "                        0.0316, -0.3112, -0.1272,  0.1015,  0.0301, -0.1845,  0.0948,  0.0870,\n",
       "                        0.0290,  0.0480, -0.0965,  0.0317, -0.2214, -0.1767,  0.2013,  0.1200,\n",
       "                        0.1362, -0.0751,  0.0755,  0.2674, -0.1844,  0.1811,  0.0230,  0.1749,\n",
       "                        0.1419,  0.1084,  0.1035,  0.1990, -0.2171, -0.2280,  0.0419, -0.0243,\n",
       "                        0.1389,  0.0668, -0.0547, -0.2254,  0.0203, -0.0954,  0.0319, -0.1949,\n",
       "                        0.1035,  0.1702,  0.2033,  0.0746, -0.1669, -0.0931, -0.2143, -0.2379,\n",
       "                        0.0091, -0.0963, -0.0710,  0.1453,  0.0911, -0.0544, -0.0550,  0.2280,\n",
       "                        0.0657, -0.0984,  0.1154, -0.0465],\n",
       "                      [ 0.0424, -0.0987, -0.0748, -0.2507, -0.1540,  0.0105, -0.0179, -0.1218,\n",
       "                       -0.2569, -0.1879, -0.1261, -0.0824,  0.0027, -0.2419,  0.2470, -0.0845,\n",
       "                        0.2464,  0.2309, -0.0498,  0.0064,  0.0252,  0.3032, -0.1935, -0.0667,\n",
       "                        0.4856,  0.0608, -0.0525,  0.1933,  0.1409, -0.0272,  0.0666,  0.0248,\n",
       "                        0.0547,  0.0174, -0.0724,  0.1435, -0.0969, -0.0728,  0.0031, -0.2386,\n",
       "                       -0.0751,  0.0698,  0.1924, -0.0034,  0.0489, -0.0585,  0.1125,  0.2707,\n",
       "                       -0.2760, -0.0964, -0.0358,  0.1477, -0.1160,  0.2788, -0.0242,  0.1261,\n",
       "                       -0.1902, -0.0171,  0.1785, -0.2556, -0.0672,  0.0093, -0.2627, -0.0813,\n",
       "                       -0.0433,  0.0762,  0.1325, -0.0472, -0.0603, -0.3311,  0.2755,  0.2065,\n",
       "                       -0.1659,  0.0116, -0.0831, -0.1196,  0.0308, -0.1529, -0.2035, -0.1950,\n",
       "                        0.1462,  0.2016, -0.2418,  0.0487],\n",
       "                      [-0.2267,  0.0562, -0.0688,  0.1423,  0.2638,  0.0214,  0.1986,  0.0289,\n",
       "                       -0.0799,  0.0563, -0.0520,  0.2771, -0.0331, -0.1752,  0.1306,  0.1283,\n",
       "                        0.1424,  0.0181, -0.0834, -0.0213, -0.0527, -0.0059, -0.0489,  0.2916,\n",
       "                       -0.0197, -0.2055,  0.0335, -0.0210,  0.0471,  0.0978,  0.0323,  0.0424,\n",
       "                        0.1144,  0.0066,  0.0980,  0.0096,  0.3280,  0.0986, -0.2331,  0.1657,\n",
       "                       -0.0660, -0.0151, -0.0458, -0.2907, -0.2235, -0.1841,  0.1228, -0.1215,\n",
       "                       -0.0282,  0.1126,  0.1059, -0.1587,  0.1524,  0.0768, -0.0920, -0.3431,\n",
       "                       -0.2193,  0.0583,  0.0342,  0.2411, -0.0356,  0.0409,  0.2280,  0.0982,\n",
       "                       -0.1522,  0.2341,  0.1394, -0.0923,  0.1708, -0.2402,  0.0226,  0.1231,\n",
       "                        0.0592,  0.0553, -0.0527, -0.0988,  0.0523, -0.0442,  0.3005, -0.0861,\n",
       "                       -0.0386, -0.2658, -0.0444, -0.0068],\n",
       "                      [-0.3200,  0.0394, -0.1392,  0.0772, -0.2277,  0.2516,  0.1673,  0.1151,\n",
       "                        0.2191,  0.0800,  0.1132,  0.1481,  0.0631,  0.3929, -0.0944, -0.1500,\n",
       "                       -0.0331,  0.0956, -0.1082, -0.1658, -0.0582,  0.1003,  0.0105, -0.0837,\n",
       "                        0.0370, -0.1117,  0.0682, -0.1392, -0.0460,  0.1444, -0.2279, -0.0520,\n",
       "                        0.0592,  0.0300, -0.0476,  0.1005, -0.1882, -0.0508,  0.1035, -0.0794,\n",
       "                        0.0932,  0.1078,  0.2259,  0.2496,  0.1241, -0.0450, -0.2630, -0.1743,\n",
       "                       -0.0933,  0.0007,  0.0556, -0.0238, -0.2165,  0.1002, -0.0364, -0.0638,\n",
       "                        0.1086, -0.0099,  0.0644, -0.1707,  0.0536, -0.0584, -0.2350,  0.0982,\n",
       "                        0.1067, -0.1481, -0.0005,  0.0531, -0.1223,  0.2247,  0.0263, -0.0451,\n",
       "                       -0.2712,  0.0805,  0.0049, -0.3135, -0.1340, -0.1491,  0.0967,  0.3069,\n",
       "                       -0.0421,  0.2770,  0.0288, -0.0650],\n",
       "                      [ 0.1134, -0.0922,  0.1511,  0.1316, -0.1734,  0.1750, -0.1658,  0.0995,\n",
       "                        0.0724, -0.0435,  0.2467,  0.0538, -0.0165, -0.0581, -0.0075, -0.2147,\n",
       "                        0.0832, -0.1587, -0.0822, -0.0548,  0.0085, -0.0152,  0.2468,  0.1033,\n",
       "                       -0.2353, -0.0562, -0.0487, -0.2765, -0.1561, -0.1369, -0.1200, -0.0432,\n",
       "                       -0.0843,  0.2144,  0.2583,  0.1021,  0.0065, -0.1339,  0.1266, -0.0393,\n",
       "                       -0.0603, -0.0205,  0.0697, -0.1801, -0.0936,  0.3071, -0.1409,  0.1777,\n",
       "                        0.1927,  0.0733, -0.0462,  0.0147,  0.0530, -0.2134,  0.0549,  0.1431,\n",
       "                       -0.0820,  0.0119, -0.0602, -0.0433,  0.0907,  0.0435,  0.3595,  0.0229,\n",
       "                        0.0509, -0.1989,  0.0053,  0.1198,  0.2485, -0.1353, -0.0488, -0.1443,\n",
       "                       -0.1235,  0.0653, -0.0545,  0.0752,  0.3529,  0.0421, -0.2399, -0.2095,\n",
       "                        0.0069, -0.3769,  0.2416, -0.0152],\n",
       "                      [-0.2401, -0.0783, -0.0744, -0.2319, -0.0956, -0.4373, -0.0946, -0.0388,\n",
       "                        0.1747,  0.1641,  0.2008,  0.2030,  0.0820, -0.0417, -0.2671,  0.1689,\n",
       "                       -0.0040, -0.2651, -0.1093,  0.2010, -0.0367,  0.1740, -0.2195, -0.2616,\n",
       "                       -0.2058,  0.2764,  0.1134,  0.0319, -0.0465,  0.0304,  0.1050,  0.0533,\n",
       "                       -0.3296,  0.0147,  0.0424, -0.0498,  0.1337,  0.0268, -0.1202, -0.1121,\n",
       "                       -0.1177, -0.0982, -0.0249, -0.2483,  0.1127,  0.1161, -0.2913,  0.2716,\n",
       "                        0.2223,  0.0743, -0.0284, -0.1214, -0.1017, -0.1271, -0.1047,  0.1470,\n",
       "                       -0.1622,  0.0899, -0.0664, -0.0301, -0.0821,  0.0147,  0.1314,  0.1237,\n",
       "                        0.2103, -0.1210,  0.0040, -0.0201, -0.4071,  0.1036, -0.1519,  0.1916,\n",
       "                        0.0360, -0.0927, -0.0022,  0.0776,  0.1785, -0.2719, -0.0177,  0.0042,\n",
       "                        0.0102, -0.0937,  0.1231,  0.0693],\n",
       "                      [ 0.0346, -0.0655,  0.0501,  0.0230,  0.1202,  0.0658,  0.0030, -0.1666,\n",
       "                        0.0621, -0.2902,  0.0271, -0.4560, -0.0058, -0.2556, -0.1761,  0.2420,\n",
       "                       -0.0993, -0.0149,  0.0131,  0.0729,  0.0532, -0.1997,  0.2579, -0.1405,\n",
       "                        0.3625, -0.2253,  0.1580, -0.1879,  0.0644, -0.0453,  0.2660,  0.0508,\n",
       "                        0.1852, -0.1812, -0.0133,  0.0315, -0.0192,  0.0098,  0.4094, -0.1201,\n",
       "                       -0.0898,  0.0464, -0.1097, -0.0601, -0.0088, -0.2580,  0.1372, -0.2565,\n",
       "                        0.0678,  0.1906, -0.0784,  0.1735, -0.2068,  0.2181, -0.0219,  0.0018,\n",
       "                       -0.2515,  0.0746,  0.0542, -0.1055,  0.0673, -0.0884, -0.3822, -0.1308,\n",
       "                       -0.1195,  0.1377, -0.0206,  0.0699,  0.1215,  0.2847, -0.2030,  0.0471,\n",
       "                        0.0595,  0.0016, -0.0537,  0.2484, -0.1914, -0.0046,  0.1847,  0.2480,\n",
       "                       -0.0530, -0.0800,  0.2330, -0.0369],\n",
       "                      [ 0.1861, -0.0274, -0.1285,  0.1450,  0.1607,  0.2070,  0.0438, -0.0828,\n",
       "                       -0.3230,  0.1368, -0.1162,  0.1695,  0.0588, -0.4385, -0.0575, -0.0905,\n",
       "                        0.1054,  0.0249,  0.0713,  0.1947,  0.0872, -0.1295,  0.0117,  0.0096,\n",
       "                       -0.1469,  0.1994, -0.1633,  0.0868, -0.2374, -0.1373, -0.1019, -0.0506,\n",
       "                       -0.0571, -0.1318, -0.0480, -0.1713,  0.4093,  0.1644,  0.0412, -0.2101,\n",
       "                        0.0071, -0.0828, -0.1945, -0.1144, -0.3607,  0.0881, -0.0431, -0.0892,\n",
       "                       -0.0728, -0.2107, -0.0221, -0.1313, -0.0086, -0.1154, -0.1062,  0.1283,\n",
       "                        0.2261, -0.0228,  0.0212, -0.1487, -0.0535, -0.0760,  0.0782,  0.0854,\n",
       "                       -0.0995, -0.1436, -0.2296,  0.0432, -0.1867,  0.2116,  0.1884,  0.3016,\n",
       "                        0.1756,  0.0531,  0.0570, -0.2384,  0.2109,  0.2012,  0.1071, -0.0556,\n",
       "                       -0.1484,  0.4095, -0.2187, -0.0518],\n",
       "                      [-0.0035, -0.0705, -0.2032, -0.0229,  0.0765,  0.1963,  0.1208, -0.1209,\n",
       "                        0.2272,  0.3525,  0.1639, -0.0370,  0.0156,  0.3276,  0.2430, -0.1661,\n",
       "                        0.0332,  0.0778,  0.0248,  0.1554, -0.0337, -0.3677,  0.1105,  0.1309,\n",
       "                       -0.3587,  0.0298, -0.1892, -0.0409,  0.0585, -0.1546, -0.0726,  0.0762,\n",
       "                        0.1544, -0.0691, -0.0866,  0.0630, -0.2591,  0.1606,  0.0452,  0.2904,\n",
       "                        0.0629, -0.0454,  0.1892, -0.0294,  0.0824, -0.0864, -0.0430, -0.2293,\n",
       "                        0.0919,  0.1563, -0.0646,  0.0131,  0.3138,  0.0123,  0.0830, -0.1376,\n",
       "                        0.2329,  0.1078,  0.0035,  0.2102, -0.0995,  0.0211,  0.0153, -0.0544,\n",
       "                       -0.2494, -0.1644, -0.2045, -0.0645,  0.3830,  0.1667, -0.1712, -0.2276,\n",
       "                       -0.1957,  0.0496, -0.0806, -0.0160, -0.2520,  0.0655,  0.0519, -0.1432,\n",
       "                        0.0904,  0.1032, -0.3291,  0.0015]], device='cuda:0')),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 0.0262,  0.0752,  0.0212, -0.1048, -0.0170,  0.1001,  0.0467,  0.0462,\n",
       "                       0.1240,  0.0310], device='cuda:0'))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = Net(q=True)\n",
    "load_model(quantized_model=qnet, model=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare qnet state_dict before and after fuse modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnet.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_modules(model=qnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.0.weight', 'conv1.0.bias', 'conv2.0.weight', 'conv2.0.bias', 'fc1.0.weight', 'fc1.0.bias', 'fc2.0.weight', 'fc2.0.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnet.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB):  0.181262\n",
      "Accuracy of the fused network on the test images: {score}% - FP32\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(model=qnet)\n",
    "score = test(model=qnet, dataloader=testloader, cuda=False)\n",
    "print(\"Accuracy of the fused network on the test images: {score}% - FP32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Post training Quantization Prepare: Inserting Observers\n",
      "\n",
      "Conv1: After observer insertion\n",
      "\n",
      " ConvReLU2d(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      ")\n",
      "Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "Conv1: After fusion and quantization\n",
      "\n",
      " QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.0730319693684578, zero_point=0)\n",
      "Size of model after quantization\n",
      "Size (MB):  0.052102\n"
     ]
    }
   ],
   "source": [
    "qnet.qconfig = torch.quantization.default_qconfig\n",
    "print(qnet.qconfig)\n",
    "torch.quantization.prepare(model=qnet, inplace=True)\n",
    "print(\"Post training Quantization Prepare: Inserting Observers\")\n",
    "print(\"\\nConv1: After observer insertion\\n\\n\", qnet.conv1)\n",
    "\n",
    "# Calibrate with the training set\n",
    "test(model=qnet, dataloader=trainloader, cuda=False)\n",
    "print(\"Post Training Quantization: Calibration done\")\n",
    "torch.quantization.convert(qnet, inplace=True)\n",
    "print(\"Post Training Quantization: Convert done\")\n",
    "print(\"Conv1: After fusion and quantization\\n\\n\", qnet.conv1)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(model=qnet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy of the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the fused and quantized network on the test images: 98.88% - INT8\n"
     ]
    }
   ],
   "source": [
    "score = test(model=qnet, dataloader=testloader, cuda=False)\n",
    "print(f\"Accuracy of the fused and quantized network on the test images: {score}% - INT8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a custom quantization configuration.\n",
    "Replace the default observers and instead of quantizing wrt to max/min we take average of the observed max/min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      "Conv1: After observer insertion\n",
      "\n",
      " ConvReLU2d(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaushik/miniconda3/envs/py3/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      "Conv1: After fusion and quantization\n",
      "\n",
      " QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.07102137058973312, zero_point=0)\n",
      "Size of model after quantization\n",
      "Size (MB):  0.052102\n",
      "Accuracy of the fused and quantized network on the test images: 98.88% - INT8\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization.observer import MovingAverageMinMaxObserver\n",
    "\n",
    "qnet = Net(q=True)\n",
    "load_model(quantized_model=qnet, model=net)\n",
    "fuse_modules(model=qnet)\n",
    "\n",
    "qnet.qconfig = torch.quantization.QConfig(\n",
    "    activation=MovingAverageMinMaxObserver.with_args(reduce_range=True),\n",
    "    weight=MovingAverageMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)\n",
    ")\n",
    "print(qnet.qconfig)\n",
    "torch.quantization.prepare(model=qnet, inplace=True)\n",
    "print(\"Post Training Quantization Prepare: Inserting Observers\")\n",
    "print(\"\\nConv1: After observer insertion\\n\\n\", qnet.conv1)\n",
    "\n",
    "test(model=qnet, dataloader=trainloader, cuda=False)\n",
    "print(\"Post Training Quantization: Calibration done\")\n",
    "torch.quantization.convert(module=qnet, inplace=True)\n",
    "print(\"Post Training Quantization: Convert done\")\n",
    "print(\"\\nConv1: After fusion and quantization\\n\\n\", qnet.conv1)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(model=qnet)\n",
    "score = test(model=qnet, dataloader=testloader, cuda=False)\n",
    "print(f\"Accuracy of the fused and quantized network on the test images: {score}% - INT8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conv1: After fusion and quantization\n",
      "\n",
      " ConvReLU2d(\n",
      "  1, 6, kernel_size=(5, 5), stride=(1, 1)\n",
      "  (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "    (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "  )\n",
      "  (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "qnet = Net(q=True)\n",
    "fuse_modules(model=qnet)\n",
    "\n",
    "# Specify quantization config for QAT\n",
    "qnet.qconfig = torch.quantization.get_default_qat_qconfig(backend=\"fbgemm\")\n",
    "\n",
    "# Prepare QAT\n",
    "torch.quantization.prepare_qat(model=qnet, inplace=True)\n",
    "\n",
    "print(\"\\nConv1: After fusion and quantization\\n\\n\", qnet.conv1)\n",
    "\n",
    "qnet = qnet.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaushik/miniconda3/envs/py3/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/native/ReduceAllOps.cpp:72.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/kaushik/miniconda3/envs/py3/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/native/TensorCompare.cpp:677.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100]  loss 2.277035 (2.302786) train_acc 12.500000 (9.250000)\n",
      "[1,   200]  loss 2.261745 (2.296432) train_acc 37.500000 (10.625000)\n",
      "[1,   300]  loss 2.268169 (2.288824) train_acc 37.500000 (16.270833)\n",
      "[1,   400]  loss 2.255547 (2.280266) train_acc 25.000000 (21.531250)\n",
      "[1,   500]  loss 2.210626 (2.268720) train_acc 50.000000 (28.175000)\n",
      "[1,   600]  loss 2.121386 (2.251543) train_acc 68.750000 (33.604167)\n",
      "[1,   700]  loss 2.051449 (2.227287) train_acc 68.750000 (37.571429)\n",
      "[1,   800]  loss 1.730208 (2.190522) train_acc 81.250000 (41.171875)\n",
      "[1,   900]  loss 1.581596 (2.135772) train_acc 56.250000 (44.229167)\n",
      "[1,  1000]  loss 1.472379 (2.062680) train_acc 56.250000 (46.881250)\n",
      "[1,  1100]  loss 0.928114 (1.973150) train_acc 75.000000 (49.323864)\n",
      "[1,  1200]  loss 0.857227 (1.871526) train_acc 56.250000 (51.812500)\n",
      "[1,  1300]  loss 0.911844 (1.776487) train_acc 68.750000 (53.942308)\n",
      "[1,  1400]  loss 0.374884 (1.684849) train_acc 93.750000 (56.174107)\n",
      "[1,  1500]  loss 0.233549 (1.601560) train_acc 93.750000 (58.187500)\n",
      "[1,  1600]  loss 0.225885 (1.523215) train_acc 87.500000 (60.160156)\n",
      "[1,  1700]  loss 0.302265 (1.453221) train_acc 87.500000 (61.908088)\n",
      "[1,  1800]  loss 0.215903 (1.390754) train_acc 93.750000 (63.479167)\n",
      "[1,  1900]  loss 0.769535 (1.334237) train_acc 81.250000 (64.891447)\n",
      "[1,  2000]  loss 0.300061 (1.280752) train_acc 93.750000 (66.278125)\n",
      "[1,  2100]  loss 0.028734 (1.231816) train_acc 100.000000 (67.505952)\n",
      "[1,  2200]  loss 0.322807 (1.185848) train_acc 81.250000 (68.656250)\n",
      "[1,  2300]  loss 0.165321 (1.142659) train_acc 93.750000 (69.736413)\n",
      "[1,  2400]  loss 0.119641 (1.103905) train_acc 100.000000 (70.718750)\n",
      "[1,  2500]  loss 0.175411 (1.069049) train_acc 93.750000 (71.615000)\n",
      "[1,  2600]  loss 0.310261 (1.035946) train_acc 93.750000 (72.483173)\n",
      "[1,  2700]  loss 0.393395 (1.004408) train_acc 87.500000 (73.303241)\n",
      "[1,  2800]  loss 0.156034 (0.975469) train_acc 93.750000 (74.042411)\n",
      "[1,  2900]  loss 0.305823 (0.948576) train_acc 87.500000 (74.726293)\n",
      "[1,  3000]  loss 0.202864 (0.922822) train_acc 93.750000 (75.379167)\n",
      "[1,  3100]  loss 0.115424 (0.898801) train_acc 93.750000 (75.985887)\n",
      "[1,  3200]  loss 0.089448 (0.876046) train_acc 93.750000 (76.580078)\n",
      "[1,  3300]  loss 0.045108 (0.854390) train_acc 100.000000 (77.125000)\n",
      "[1,  3400]  loss 0.215755 (0.834496) train_acc 93.750000 (77.650735)\n",
      "[1,  3500]  loss 0.040438 (0.814461) train_acc 100.000000 (78.180357)\n",
      "[1,  3600]  loss 0.374722 (0.795771) train_acc 93.750000 (78.673611)\n",
      "[1,  3700]  loss 0.349501 (0.778257) train_acc 93.750000 (79.125000)\n",
      "[2,   100]  loss 0.293419 (0.137480) train_acc 87.500000 (95.500000)\n",
      "[2,   200]  loss 0.057242 (0.130832) train_acc 100.000000 (95.781250)\n",
      "[2,   300]  loss 0.022912 (0.126979) train_acc 100.000000 (95.875000)\n",
      "[2,   400]  loss 0.383197 (0.135675) train_acc 93.750000 (95.796875)\n",
      "[2,   500]  loss 0.237213 (0.134885) train_acc 87.500000 (95.800000)\n",
      "[2,   600]  loss 0.221055 (0.135151) train_acc 87.500000 (95.750000)\n",
      "[2,   700]  loss 0.001782 (0.128219) train_acc 100.000000 (96.017857)\n",
      "[2,   800]  loss 0.020827 (0.130352) train_acc 100.000000 (95.960938)\n",
      "[2,   900]  loss 0.147818 (0.127976) train_acc 93.750000 (96.048611)\n",
      "[2,  1000]  loss 0.065773 (0.126770) train_acc 100.000000 (96.100000)\n",
      "[2,  1100]  loss 0.085134 (0.125742) train_acc 100.000000 (96.136364)\n",
      "[2,  1200]  loss 0.008801 (0.123344) train_acc 100.000000 (96.218750)\n",
      "[2,  1300]  loss 0.093484 (0.122388) train_acc 100.000000 (96.264423)\n",
      "[2,  1400]  loss 0.494715 (0.122385) train_acc 87.500000 (96.223214)\n",
      "[2,  1500]  loss 0.349653 (0.122654) train_acc 87.500000 (96.225000)\n",
      "[2,  1600]  loss 0.019930 (0.121482) train_acc 100.000000 (96.292969)\n",
      "[2,  1700]  loss 0.073527 (0.120696) train_acc 93.750000 (96.319853)\n",
      "[2,  1800]  loss 0.133782 (0.119777) train_acc 93.750000 (96.371528)\n",
      "[2,  1900]  loss 0.189869 (0.119372) train_acc 93.750000 (96.361842)\n",
      "[2,  2000]  loss 0.013762 (0.117915) train_acc 100.000000 (96.400000)\n",
      "[2,  2100]  loss 0.033832 (0.118307) train_acc 100.000000 (96.419643)\n",
      "[2,  2200]  loss 0.123117 (0.117497) train_acc 93.750000 (96.443182)\n",
      "[2,  2300]  loss 0.098441 (0.116163) train_acc 93.750000 (96.480978)\n",
      "[2,  2400]  loss 0.026062 (0.115185) train_acc 100.000000 (96.526042)\n",
      "[2,  2500]  loss 0.018910 (0.114872) train_acc 100.000000 (96.537500)\n",
      "[2,  2600]  loss 0.008916 (0.114818) train_acc 100.000000 (96.526442)\n",
      "[2,  2700]  loss 0.004152 (0.114202) train_acc 100.000000 (96.550926)\n",
      "[2,  2800]  loss 0.087382 (0.114347) train_acc 93.750000 (96.544643)\n",
      "[2,  2900]  loss 0.020427 (0.113899) train_acc 100.000000 (96.577586)\n",
      "[2,  3000]  loss 0.005104 (0.112611) train_acc 100.000000 (96.614583)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,  3100]  loss 0.119964 (0.112254) train_acc 93.750000 (96.622984)\n",
      "[2,  3200]  loss 0.001713 (0.110740) train_acc 100.000000 (96.664062)\n",
      "[2,  3300]  loss 0.086472 (0.110087) train_acc 100.000000 (96.685606)\n",
      "[2,  3400]  loss 0.022003 (0.109245) train_acc 100.000000 (96.704044)\n",
      "[2,  3500]  loss 0.068793 (0.109076) train_acc 93.750000 (96.700000)\n",
      "[2,  3600]  loss 0.033700 (0.108560) train_acc 100.000000 (96.722222)\n",
      "[2,  3700]  loss 0.029272 (0.107796) train_acc 100.000000 (96.743243)\n",
      "[3,   100]  loss 0.108049 (0.076380) train_acc 93.750000 (97.375000)\n",
      "[3,   200]  loss 0.229120 (0.078433) train_acc 93.750000 (97.531250)\n",
      "[3,   300]  loss 0.079726 (0.075637) train_acc 93.750000 (97.562500)\n",
      "[3,   400]  loss 0.013925 (0.075444) train_acc 100.000000 (97.546875)\n",
      "[3,   500]  loss 0.041924 (0.074523) train_acc 100.000000 (97.575000)\n",
      "[3,   600]  loss 0.007033 (0.074922) train_acc 100.000000 (97.604167)\n",
      "[3,   700]  loss 0.023965 (0.074106) train_acc 100.000000 (97.598214)\n",
      "[3,   800]  loss 0.005594 (0.075338) train_acc 100.000000 (97.648438)\n",
      "[3,   900]  loss 0.026916 (0.073886) train_acc 100.000000 (97.708333)\n",
      "[3,  1000]  loss 0.002588 (0.076091) train_acc 100.000000 (97.681250)\n",
      "[3,  1100]  loss 0.003452 (0.074883) train_acc 100.000000 (97.738636)\n",
      "[3,  1200]  loss 0.145513 (0.074126) train_acc 93.750000 (97.765625)\n",
      "[3,  1300]  loss 0.102173 (0.075172) train_acc 93.750000 (97.745192)\n",
      "[3,  1400]  loss 0.100301 (0.075800) train_acc 93.750000 (97.700893)\n",
      "[3,  1500]  loss 0.008010 (0.075205) train_acc 100.000000 (97.708333)\n",
      "[3,  1600]  loss 0.029655 (0.076112) train_acc 100.000000 (97.679688)\n",
      "[3,  1700]  loss 0.148687 (0.075760) train_acc 93.750000 (97.702206)\n",
      "[3,  1800]  loss 0.146142 (0.076211) train_acc 93.750000 (97.680556)\n",
      "[3,  1900]  loss 0.003129 (0.076926) train_acc 100.000000 (97.664474)\n",
      "[3,  2000]  loss 0.058674 (0.076458) train_acc 100.000000 (97.665625)\n",
      "[3,  2100]  loss 0.109504 (0.076565) train_acc 93.750000 (97.645833)\n",
      "[3,  2200]  loss 0.010282 (0.076771) train_acc 100.000000 (97.642045)\n",
      "[3,  2300]  loss 0.032433 (0.075841) train_acc 100.000000 (97.660326)\n",
      "[3,  2400]  loss 0.278249 (0.075592) train_acc 93.750000 (97.658854)\n",
      "[3,  2500]  loss 0.030308 (0.074997) train_acc 100.000000 (97.680000)\n",
      "[3,  2600]  loss 0.065446 (0.075010) train_acc 93.750000 (97.673077)\n",
      "[3,  2700]  loss 0.016949 (0.075157) train_acc 100.000000 (97.673611)\n",
      "[3,  2800]  loss 0.001204 (0.074851) train_acc 100.000000 (97.687500)\n",
      "[3,  2900]  loss 0.003865 (0.074857) train_acc 100.000000 (97.678879)\n",
      "[3,  3000]  loss 0.010176 (0.074595) train_acc 100.000000 (97.691667)\n",
      "[3,  3100]  loss 0.001899 (0.074603) train_acc 100.000000 (97.695565)\n",
      "[3,  3200]  loss 0.032403 (0.074252) train_acc 100.000000 (97.714844)\n",
      "[3,  3300]  loss 0.003845 (0.073817) train_acc 100.000000 (97.725379)\n",
      "[3,  3400]  loss 0.131885 (0.073271) train_acc 93.750000 (97.744485)\n",
      "[3,  3500]  loss 0.008144 (0.073076) train_acc 100.000000 (97.751786)\n",
      "[3,  3600]  loss 0.045869 (0.072497) train_acc 100.000000 (97.772569)\n",
      "[3,  3700]  loss 0.002089 (0.072323) train_acc 100.000000 (97.780405)\n",
      "[4,   100]  loss 0.073276 (0.053873) train_acc 100.000000 (98.375000)\n",
      "[4,   200]  loss 0.116925 (0.061237) train_acc 87.500000 (98.156250)\n",
      "[4,   300]  loss 0.275197 (0.066276) train_acc 93.750000 (98.020833)\n",
      "[4,   400]  loss 0.000766 (0.061230) train_acc 100.000000 (98.109375)\n",
      "[4,   500]  loss 0.025486 (0.058062) train_acc 100.000000 (98.225000)\n",
      "[4,   600]  loss 0.028127 (0.055789) train_acc 100.000000 (98.291667)\n",
      "[4,   700]  loss 0.023920 (0.057560) train_acc 100.000000 (98.187500)\n",
      "[4,   800]  loss 0.000874 (0.058181) train_acc 100.000000 (98.101562)\n",
      "[4,   900]  loss 0.018284 (0.057266) train_acc 100.000000 (98.125000)\n",
      "[4,  1000]  loss 0.000321 (0.055819) train_acc 100.000000 (98.193750)\n",
      "[4,  1100]  loss 0.015003 (0.056731) train_acc 100.000000 (98.159091)\n",
      "[4,  1200]  loss 0.009270 (0.057529) train_acc 100.000000 (98.192708)\n",
      "[4,  1300]  loss 0.001474 (0.057034) train_acc 100.000000 (98.173077)\n",
      "[4,  1400]  loss 0.024820 (0.056927) train_acc 100.000000 (98.183036)\n",
      "[4,  1500]  loss 0.002397 (0.056524) train_acc 100.000000 (98.187500)\n",
      "[4,  1600]  loss 0.171503 (0.057598) train_acc 93.750000 (98.156250)\n",
      "[4,  1700]  loss 0.015124 (0.057293) train_acc 100.000000 (98.158088)\n",
      "[4,  1800]  loss 0.007619 (0.057518) train_acc 100.000000 (98.142361)\n",
      "[4,  1900]  loss 0.026644 (0.058115) train_acc 100.000000 (98.138158)\n",
      "[4,  2000]  loss 0.161051 (0.058612) train_acc 93.750000 (98.128125)\n",
      "[4,  2100]  loss 0.072826 (0.058203) train_acc 93.750000 (98.151786)\n",
      "[4,  2200]  loss 0.011810 (0.057997) train_acc 100.000000 (98.178977)\n",
      "[4,  2300]  loss 0.016017 (0.057627) train_acc 100.000000 (98.198370)\n",
      "[4,  2400]  loss 0.002062 (0.057475) train_acc 100.000000 (98.218750)\n",
      "[4,  2500]  loss 0.238891 (0.057841) train_acc 87.500000 (98.220000)\n",
      "[4,  2600]  loss 0.045515 (0.057226) train_acc 100.000000 (98.235577)\n",
      "[4,  2700]  loss 0.009322 (0.057561) train_acc 100.000000 (98.222222)\n",
      "[4,  2800]  loss 0.002285 (0.057411) train_acc 100.000000 (98.218750)\n",
      "[4,  2900]  loss 0.005408 (0.057487) train_acc 100.000000 (98.206897)\n",
      "[4,  3000]  loss 0.006344 (0.057521) train_acc 100.000000 (98.204167)\n",
      "[4,  3100]  loss 0.042582 (0.057534) train_acc 100.000000 (98.197581)\n",
      "[4,  3200]  loss 0.009699 (0.057252) train_acc 100.000000 (98.208984)\n",
      "[4,  3300]  loss 0.176725 (0.057081) train_acc 93.750000 (98.227273)\n",
      "[4,  3400]  loss 0.008337 (0.056649) train_acc 100.000000 (98.240809)\n",
      "[4,  3500]  loss 0.026615 (0.056672) train_acc 100.000000 (98.237500)\n",
      "[4,  3600]  loss 0.004587 (0.056707) train_acc 100.000000 (98.236111)\n",
      "[4,  3700]  loss 0.001462 (0.056770) train_acc 100.000000 (98.238176)\n",
      "[5,   100]  loss 0.004359 (0.044814) train_acc 100.000000 (98.687500)\n",
      "[5,   200]  loss 0.051259 (0.046799) train_acc 93.750000 (98.750000)\n",
      "[5,   300]  loss 0.013944 (0.043327) train_acc 100.000000 (98.770833)\n",
      "[5,   400]  loss 0.034103 (0.046311) train_acc 100.000000 (98.734375)\n",
      "[5,   500]  loss 0.001726 (0.045703) train_acc 100.000000 (98.700000)\n",
      "[5,   600]  loss 0.016844 (0.049751) train_acc 100.000000 (98.593750)\n",
      "[5,   700]  loss 0.200755 (0.048764) train_acc 93.750000 (98.598214)\n",
      "[5,   800]  loss 0.003464 (0.049550) train_acc 100.000000 (98.578125)\n",
      "[5,   900]  loss 0.023370 (0.048712) train_acc 100.000000 (98.590278)\n",
      "[5,  1000]  loss 0.009649 (0.048443) train_acc 100.000000 (98.581250)\n",
      "[5,  1100]  loss 0.001286 (0.047471) train_acc 100.000000 (98.590909)\n",
      "[5,  1200]  loss 0.028068 (0.048269) train_acc 100.000000 (98.572917)\n",
      "[5,  1300]  loss 0.012833 (0.048086) train_acc 100.000000 (98.557692)\n",
      "[5,  1400]  loss 0.157848 (0.047426) train_acc 93.750000 (98.562500)\n",
      "[5,  1500]  loss 0.000659 (0.046883) train_acc 100.000000 (98.583333)\n",
      "[5,  1600]  loss 0.003976 (0.046845) train_acc 100.000000 (98.589844)\n",
      "[5,  1700]  loss 0.018150 (0.046940) train_acc 100.000000 (98.580882)\n",
      "[5,  1800]  loss 0.009228 (0.046797) train_acc 100.000000 (98.586806)\n",
      "[5,  1900]  loss 0.070007 (0.047015) train_acc 100.000000 (98.575658)\n",
      "[5,  2000]  loss 0.000651 (0.046579) train_acc 100.000000 (98.593750)\n",
      "[5,  2100]  loss 0.005805 (0.046149) train_acc 100.000000 (98.601190)\n",
      "[5,  2200]  loss 0.006228 (0.046343) train_acc 100.000000 (98.602273)\n",
      "[5,  2300]  loss 0.014069 (0.046687) train_acc 100.000000 (98.586957)\n",
      "[5,  2400]  loss 0.003896 (0.047527) train_acc 100.000000 (98.557292)\n",
      "[5,  2500]  loss 0.285942 (0.046953) train_acc 87.500000 (98.572500)\n",
      "[5,  2600]  loss 0.011770 (0.047292) train_acc 100.000000 (98.560096)\n",
      "[5,  2700]  loss 0.001459 (0.047285) train_acc 100.000000 (98.555556)\n",
      "[5,  2800]  loss 0.000744 (0.047313) train_acc 100.000000 (98.566964)\n",
      "[5,  2900]  loss 0.089162 (0.047401) train_acc 93.750000 (98.562500)\n",
      "[5,  3000]  loss 0.002916 (0.046857) train_acc 100.000000 (98.579167)\n",
      "[5,  3100]  loss 0.043846 (0.046793) train_acc 100.000000 (98.576613)\n",
      "[5,  3200]  loss 0.002582 (0.046854) train_acc 100.000000 (98.570312)\n",
      "[5,  3300]  loss 0.008624 (0.047026) train_acc 100.000000 (98.564394)\n",
      "[5,  3400]  loss 0.001461 (0.047150) train_acc 100.000000 (98.560662)\n",
      "[5,  3500]  loss 0.000181 (0.047544) train_acc 100.000000 (98.551786)\n",
      "[5,  3600]  loss 0.010319 (0.047428) train_acc 100.000000 (98.553819)\n",
      "[5,  3700]  loss 0.054011 (0.047623) train_acc 93.750000 (98.547297)\n",
      "[6,   100]  loss 0.003156 (0.031668) train_acc 100.000000 (99.187500)\n",
      "[6,   200]  loss 0.017885 (0.032050) train_acc 100.000000 (99.062500)\n",
      "[6,   300]  loss 0.102856 (0.038529) train_acc 93.750000 (98.833333)\n",
      "[6,   400]  loss 0.008257 (0.038128) train_acc 100.000000 (98.843750)\n",
      "[6,   500]  loss 0.005695 (0.037565) train_acc 100.000000 (98.850000)\n",
      "[6,   600]  loss 0.066220 (0.036050) train_acc 93.750000 (98.885417)\n",
      "[6,   700]  loss 0.170314 (0.036669) train_acc 93.750000 (98.857143)\n",
      "[6,   800]  loss 0.000461 (0.037135) train_acc 100.000000 (98.828125)\n",
      "[6,   900]  loss 0.011508 (0.037305) train_acc 100.000000 (98.833333)\n",
      "[6,  1000]  loss 0.017756 (0.037129) train_acc 100.000000 (98.812500)\n",
      "[6,  1100]  loss 0.081280 (0.037618) train_acc 93.750000 (98.784091)\n",
      "[6,  1200]  loss 0.003042 (0.039190) train_acc 100.000000 (98.776042)\n",
      "[6,  1300]  loss 0.124504 (0.039156) train_acc 87.500000 (98.778846)\n",
      "[6,  1400]  loss 0.009345 (0.040108) train_acc 100.000000 (98.763393)\n",
      "[6,  1500]  loss 0.000769 (0.040093) train_acc 100.000000 (98.750000)\n",
      "[6,  1600]  loss 0.095154 (0.040524) train_acc 93.750000 (98.726562)\n",
      "[6,  1700]  loss 0.005976 (0.040706) train_acc 100.000000 (98.731618)\n",
      "[6,  1800]  loss 0.001100 (0.040746) train_acc 100.000000 (98.722222)\n",
      "[6,  1900]  loss 0.010272 (0.040716) train_acc 100.000000 (98.726974)\n",
      "[6,  2000]  loss 0.000553 (0.040524) train_acc 100.000000 (98.725000)\n",
      "[6,  2100]  loss 0.002330 (0.040972) train_acc 100.000000 (98.720238)\n",
      "[6,  2200]  loss 0.005811 (0.040635) train_acc 100.000000 (98.724432)\n",
      "[6,  2300]  loss 0.048535 (0.040400) train_acc 100.000000 (98.733696)\n",
      "[6,  2400]  loss 0.005833 (0.040397) train_acc 100.000000 (98.726562)\n",
      "[6,  2500]  loss 0.004233 (0.040167) train_acc 100.000000 (98.732500)\n",
      "[6,  2600]  loss 0.010542 (0.039604) train_acc 100.000000 (98.750000)\n",
      "[6,  2700]  loss 0.024561 (0.039683) train_acc 100.000000 (98.743056)\n",
      "[6,  2800]  loss 0.007021 (0.039168) train_acc 100.000000 (98.767857)\n",
      "[6,  2900]  loss 0.001501 (0.038966) train_acc 100.000000 (98.769397)\n",
      "[6,  3000]  loss 0.001320 (0.038732) train_acc 100.000000 (98.764583)\n",
      "[6,  3100]  loss 0.004039 (0.038281) train_acc 100.000000 (98.774194)\n",
      "[6,  3200]  loss 0.005276 (0.038243) train_acc 100.000000 (98.765625)\n",
      "[6,  3300]  loss 0.003669 (0.038499) train_acc 100.000000 (98.757576)\n",
      "[6,  3400]  loss 0.011867 (0.038547) train_acc 100.000000 (98.759191)\n",
      "[6,  3500]  loss 0.004949 (0.039286) train_acc 100.000000 (98.744643)\n",
      "[6,  3600]  loss 0.001721 (0.039389) train_acc 100.000000 (98.748264)\n",
      "[6,  3700]  loss 0.007014 (0.039245) train_acc 100.000000 (98.748311)\n",
      "[7,   100]  loss 0.000532 (0.034479) train_acc 100.000000 (99.062500)\n",
      "[7,   200]  loss 0.000130 (0.029736) train_acc 100.000000 (99.187500)\n",
      "[7,   300]  loss 0.005028 (0.030483) train_acc 100.000000 (99.083333)\n",
      "[7,   400]  loss 0.079018 (0.028731) train_acc 93.750000 (99.109375)\n",
      "[7,   500]  loss 0.025495 (0.029787) train_acc 100.000000 (99.112500)\n",
      "[7,   600]  loss 0.001898 (0.029413) train_acc 100.000000 (99.104167)\n",
      "[7,   700]  loss 0.180338 (0.030171) train_acc 93.750000 (99.089286)\n",
      "[7,   800]  loss 0.080982 (0.029828) train_acc 93.750000 (99.085938)\n",
      "[7,   900]  loss 0.002989 (0.031093) train_acc 100.000000 (99.048611)\n",
      "[7,  1000]  loss 0.002664 (0.031827) train_acc 100.000000 (99.025000)\n",
      "[7,  1100]  loss 0.000873 (0.033072) train_acc 100.000000 (98.965909)\n",
      "[7,  1200]  loss 0.001510 (0.034035) train_acc 100.000000 (98.937500)\n",
      "[7,  1300]  loss 0.015834 (0.034426) train_acc 100.000000 (98.913462)\n",
      "[7,  1400]  loss 0.001586 (0.033979) train_acc 100.000000 (98.955357)\n",
      "[7,  1500]  loss 0.002699 (0.033908) train_acc 100.000000 (98.954167)\n",
      "[7,  1600]  loss 0.040687 (0.033252) train_acc 100.000000 (98.976562)\n",
      "[7,  1700]  loss 0.599456 (0.033550) train_acc 87.500000 (98.963235)\n",
      "[7,  1800]  loss 0.010335 (0.033240) train_acc 100.000000 (98.961806)\n",
      "[7,  1900]  loss 0.037482 (0.033582) train_acc 100.000000 (98.944079)\n",
      "[7,  2000]  loss 0.000379 (0.032935) train_acc 100.000000 (98.962500)\n",
      "[7,  2100]  loss 0.002098 (0.033257) train_acc 100.000000 (98.934524)\n",
      "[7,  2200]  loss 0.001844 (0.033402) train_acc 100.000000 (98.931818)\n",
      "[7,  2300]  loss 0.075290 (0.034126) train_acc 100.000000 (98.926630)\n",
      "[7,  2400]  loss 0.046166 (0.034120) train_acc 100.000000 (98.934896)\n",
      "[7,  2500]  loss 0.014735 (0.034033) train_acc 100.000000 (98.930000)\n",
      "[7,  2600]  loss 0.000777 (0.033991) train_acc 100.000000 (98.935096)\n",
      "[7,  2700]  loss 0.007058 (0.034590) train_acc 100.000000 (98.914352)\n",
      "[7,  2800]  loss 0.021854 (0.035246) train_acc 100.000000 (98.901786)\n",
      "[7,  2900]  loss 0.012721 (0.035394) train_acc 100.000000 (98.896552)\n",
      "[7,  3000]  loss 0.002910 (0.035588) train_acc 100.000000 (98.887500)\n",
      "[7,  3100]  loss 0.001163 (0.035338) train_acc 100.000000 (98.897177)\n",
      "[7,  3200]  loss 0.008135 (0.035295) train_acc 100.000000 (98.902344)\n",
      "[7,  3300]  loss 0.000199 (0.034957) train_acc 100.000000 (98.909091)\n",
      "[7,  3400]  loss 0.035431 (0.034844) train_acc 100.000000 (98.917279)\n",
      "[7,  3500]  loss 0.002662 (0.034631) train_acc 100.000000 (98.925000)\n",
      "[7,  3600]  loss 0.031137 (0.034514) train_acc 100.000000 (98.934028)\n",
      "[7,  3700]  loss 0.012139 (0.034446) train_acc 100.000000 (98.934122)\n",
      "[8,   100]  loss 0.053706 (0.024905) train_acc 93.750000 (99.000000)\n",
      "[8,   200]  loss 0.010507 (0.027877) train_acc 100.000000 (99.031250)\n",
      "[8,   300]  loss 0.003947 (0.027712) train_acc 100.000000 (99.041667)\n",
      "[8,   400]  loss 0.000943 (0.029239) train_acc 100.000000 (99.031250)\n",
      "[8,   500]  loss 0.000264 (0.028595) train_acc 100.000000 (99.050000)\n",
      "[8,   600]  loss 0.001658 (0.026931) train_acc 100.000000 (99.104167)\n",
      "[8,   700]  loss 0.001171 (0.025630) train_acc 100.000000 (99.151786)\n",
      "[8,   800]  loss 0.000414 (0.025468) train_acc 100.000000 (99.156250)\n",
      "[8,   900]  loss 0.006600 (0.025040) train_acc 100.000000 (99.159722)\n",
      "[8,  1000]  loss 0.485867 (0.024466) train_acc 93.750000 (99.187500)\n",
      "[8,  1100]  loss 0.018353 (0.023948) train_acc 100.000000 (99.215909)\n",
      "[8,  1200]  loss 0.013968 (0.023910) train_acc 100.000000 (99.213542)\n",
      "[8,  1300]  loss 0.035216 (0.026630) train_acc 100.000000 (99.149038)\n",
      "[8,  1400]  loss 0.012335 (0.027606) train_acc 100.000000 (99.129464)\n",
      "[8,  1500]  loss 0.000687 (0.028146) train_acc 100.000000 (99.112500)\n",
      "[8,  1600]  loss 0.001589 (0.028036) train_acc 100.000000 (99.117188)\n",
      "[8,  1700]  loss 0.000380 (0.027681) train_acc 100.000000 (99.136029)\n",
      "[8,  1800]  loss 0.001989 (0.027631) train_acc 100.000000 (99.138889)\n",
      "[8,  1900]  loss 0.000536 (0.027597) train_acc 100.000000 (99.138158)\n",
      "[8,  2000]  loss 0.001326 (0.027538) train_acc 100.000000 (99.121875)\n",
      "[8,  2100]  loss 0.005693 (0.027841) train_acc 100.000000 (99.116071)\n",
      "[8,  2200]  loss 0.007616 (0.027692) train_acc 100.000000 (99.125000)\n",
      "[8,  2300]  loss 0.001779 (0.028376) train_acc 100.000000 (99.111413)\n",
      "[8,  2400]  loss 0.008543 (0.028560) train_acc 100.000000 (99.111979)\n",
      "[8,  2500]  loss 0.017492 (0.028983) train_acc 100.000000 (99.090000)\n",
      "[8,  2600]  loss 0.014762 (0.029071) train_acc 100.000000 (99.088942)\n",
      "[8,  2700]  loss 0.089333 (0.029377) train_acc 93.750000 (99.074074)\n",
      "[8,  2800]  loss 0.172560 (0.029449) train_acc 93.750000 (99.066964)\n",
      "[8,  2900]  loss 0.000905 (0.029806) train_acc 100.000000 (99.060345)\n",
      "[8,  3000]  loss 0.002872 (0.030093) train_acc 100.000000 (99.050000)\n",
      "[8,  3100]  loss 0.018898 (0.030222) train_acc 100.000000 (99.042339)\n",
      "[8,  3200]  loss 0.000863 (0.029947) train_acc 100.000000 (99.046875)\n",
      "[8,  3300]  loss 0.003402 (0.029693) train_acc 100.000000 (99.056818)\n",
      "[8,  3400]  loss 0.002384 (0.029796) train_acc 100.000000 (99.053309)\n",
      "[8,  3500]  loss 0.001545 (0.029937) train_acc 100.000000 (99.055357)\n",
      "[8,  3600]  loss 0.194942 (0.029933) train_acc 93.750000 (99.052083)\n",
      "[8,  3700]  loss 0.060825 (0.030151) train_acc 93.750000 (99.052365)\n",
      "[9,   100]  loss 0.004113 (0.017020) train_acc 100.000000 (99.500000)\n",
      "[9,   200]  loss 0.000177 (0.019790) train_acc 100.000000 (99.500000)\n",
      "[9,   300]  loss 0.000578 (0.021884) train_acc 100.000000 (99.416667)\n",
      "[9,   400]  loss 0.003799 (0.022087) train_acc 100.000000 (99.390625)\n",
      "[9,   500]  loss 0.003962 (0.023433) train_acc 100.000000 (99.387500)\n",
      "[9,   600]  loss 0.017597 (0.024102) train_acc 100.000000 (99.364583)\n",
      "[9,   700]  loss 0.000489 (0.024154) train_acc 100.000000 (99.375000)\n",
      "[9,   800]  loss 0.002517 (0.024879) train_acc 100.000000 (99.351562)\n",
      "[9,   900]  loss 0.000532 (0.025732) train_acc 100.000000 (99.333333)\n",
      "[9,  1000]  loss 0.000532 (0.026387) train_acc 100.000000 (99.318750)\n",
      "[9,  1100]  loss 0.025208 (0.025962) train_acc 100.000000 (99.306818)\n",
      "[9,  1200]  loss 0.016705 (0.026017) train_acc 100.000000 (99.265625)\n",
      "[9,  1300]  loss 0.001484 (0.026354) train_acc 100.000000 (99.245192)\n",
      "[9,  1400]  loss 0.011914 (0.027049) train_acc 100.000000 (99.200893)\n",
      "[9,  1500]  loss 0.000608 (0.026889) train_acc 100.000000 (99.204167)\n",
      "[9,  1600]  loss 0.007451 (0.026355) train_acc 100.000000 (99.214844)\n",
      "[9,  1700]  loss 0.005684 (0.026294) train_acc 100.000000 (99.213235)\n",
      "[9,  1800]  loss 0.002849 (0.026502) train_acc 100.000000 (99.204861)\n",
      "[9,  1900]  loss 0.237901 (0.026549) train_acc 93.750000 (99.197368)\n",
      "[9,  2000]  loss 0.006086 (0.026920) train_acc 100.000000 (99.190625)\n",
      "[9,  2100]  loss 0.001199 (0.026823) train_acc 100.000000 (99.187500)\n",
      "[9,  2200]  loss 0.187147 (0.027215) train_acc 93.750000 (99.176136)\n",
      "[9,  2300]  loss 0.045470 (0.027819) train_acc 100.000000 (99.146739)\n",
      "[9,  2400]  loss 0.003040 (0.027954) train_acc 100.000000 (99.145833)\n",
      "[9,  2500]  loss 0.018267 (0.028267) train_acc 100.000000 (99.135000)\n",
      "[9,  2600]  loss 0.018758 (0.028025) train_acc 100.000000 (99.144231)\n",
      "[9,  2700]  loss 0.075545 (0.028477) train_acc 93.750000 (99.134259)\n",
      "[9,  2800]  loss 0.001166 (0.028300) train_acc 100.000000 (99.138393)\n",
      "[9,  2900]  loss 0.004412 (0.028801) train_acc 100.000000 (99.129310)\n",
      "[9,  3000]  loss 0.012718 (0.028479) train_acc 100.000000 (99.133333)\n",
      "[9,  3100]  loss 0.000552 (0.028357) train_acc 100.000000 (99.139113)\n",
      "[9,  3200]  loss 0.000592 (0.028139) train_acc 100.000000 (99.146484)\n",
      "[9,  3300]  loss 0.117033 (0.027865) train_acc 93.750000 (99.153409)\n",
      "[9,  3400]  loss 0.061490 (0.027823) train_acc 100.000000 (99.154412)\n",
      "[9,  3500]  loss 0.004677 (0.027829) train_acc 100.000000 (99.153571)\n",
      "[9,  3600]  loss 0.006490 (0.027633) train_acc 100.000000 (99.157986)\n",
      "[9,  3700]  loss 0.032803 (0.027907) train_acc 100.000000 (99.148649)\n",
      "[10,   100]  loss 0.005630 (0.020233) train_acc 100.000000 (99.250000)\n",
      "[10,   200]  loss 0.003291 (0.020604) train_acc 100.000000 (99.218750)\n",
      "[10,   300]  loss 0.000383 (0.023185) train_acc 100.000000 (99.229167)\n",
      "[10,   400]  loss 0.052109 (0.023690) train_acc 100.000000 (99.234375)\n",
      "[10,   500]  loss 0.004184 (0.024569) train_acc 100.000000 (99.237500)\n",
      "[10,   600]  loss 0.091156 (0.023229) train_acc 93.750000 (99.270833)\n",
      "[10,   700]  loss 0.046366 (0.023518) train_acc 100.000000 (99.285714)\n",
      "[10,   800]  loss 0.007031 (0.023677) train_acc 100.000000 (99.289062)\n",
      "[10,   900]  loss 0.000807 (0.023984) train_acc 100.000000 (99.277778)\n",
      "[10,  1000]  loss 0.000845 (0.024829) train_acc 100.000000 (99.237500)\n",
      "[10,  1100]  loss 0.119333 (0.024275) train_acc 93.750000 (99.238636)\n",
      "[10,  1200]  loss 0.014740 (0.023427) train_acc 100.000000 (99.260417)\n",
      "[10,  1300]  loss 0.000448 (0.023073) train_acc 100.000000 (99.278846)\n",
      "[10,  1400]  loss 0.005946 (0.023523) train_acc 100.000000 (99.267857)\n",
      "[10,  1500]  loss 0.000927 (0.023290) train_acc 100.000000 (99.283333)\n",
      "[10,  1600]  loss 0.000519 (0.023656) train_acc 100.000000 (99.261719)\n",
      "[10,  1700]  loss 0.054627 (0.024090) train_acc 93.750000 (99.264706)\n",
      "[10,  1800]  loss 0.000244 (0.024429) train_acc 100.000000 (99.236111)\n",
      "[10,  1900]  loss 0.027034 (0.024709) train_acc 100.000000 (99.220395)\n",
      "[10,  2000]  loss 0.159429 (0.024786) train_acc 93.750000 (99.221875)\n",
      "[10,  2100]  loss 0.045107 (0.024556) train_acc 93.750000 (99.217262)\n",
      "[10,  2200]  loss 0.005398 (0.024475) train_acc 100.000000 (99.224432)\n",
      "[10,  2300]  loss 0.114195 (0.024762) train_acc 93.750000 (99.214674)\n",
      "[10,  2400]  loss 0.005221 (0.024858) train_acc 100.000000 (99.216146)\n",
      "[10,  2500]  loss 0.000470 (0.024888) train_acc 100.000000 (99.222500)\n",
      "[10,  2600]  loss 0.001767 (0.024702) train_acc 100.000000 (99.235577)\n",
      "[10,  2700]  loss 0.002740 (0.024767) train_acc 100.000000 (99.231481)\n",
      "[10,  2800]  loss 0.000794 (0.024944) train_acc 100.000000 (99.227679)\n",
      "[10,  2900]  loss 0.001531 (0.025041) train_acc 100.000000 (99.234914)\n",
      "[10,  3000]  loss 0.000095 (0.024771) train_acc 100.000000 (99.245833)\n",
      "[10,  3100]  loss 0.372199 (0.024994) train_acc 93.750000 (99.235887)\n",
      "[10,  3200]  loss 0.043729 (0.024924) train_acc 93.750000 (99.238281)\n",
      "[10,  3300]  loss 0.000975 (0.024932) train_acc 100.000000 (99.234848)\n",
      "[10,  3400]  loss 0.000163 (0.024884) train_acc 100.000000 (99.238971)\n",
      "[10,  3500]  loss 0.007254 (0.024670) train_acc 100.000000 (99.244643)\n",
      "[10,  3600]  loss 0.019759 (0.024657) train_acc 100.000000 (99.248264)\n",
      "[10,  3700]  loss 0.000131 (0.024484) train_acc 100.000000 (99.241554)\n",
      "[11,   100]  loss 0.000140 (0.016632) train_acc 100.000000 (99.562500)\n",
      "[11,   200]  loss 0.000415 (0.014626) train_acc 100.000000 (99.562500)\n",
      "[11,   300]  loss 0.000705 (0.016238) train_acc 100.000000 (99.479167)\n",
      "[11,   400]  loss 0.006064 (0.017550) train_acc 100.000000 (99.484375)\n",
      "[11,   500]  loss 0.003480 (0.018774) train_acc 100.000000 (99.412500)\n",
      "[11,   600]  loss 0.001166 (0.018852) train_acc 100.000000 (99.416667)\n",
      "[11,   700]  loss 0.002514 (0.018775) train_acc 100.000000 (99.383929)\n",
      "[11,   800]  loss 0.010059 (0.020055) train_acc 100.000000 (99.343750)\n",
      "[11,   900]  loss 0.025467 (0.020354) train_acc 100.000000 (99.361111)\n",
      "[11,  1000]  loss 0.001480 (0.020301) train_acc 100.000000 (99.356250)\n",
      "[11,  1100]  loss 0.000249 (0.019581) train_acc 100.000000 (99.369318)\n",
      "[11,  1200]  loss 0.002800 (0.020160) train_acc 100.000000 (99.348958)\n",
      "[11,  1300]  loss 0.004328 (0.020845) train_acc 100.000000 (99.331731)\n",
      "[11,  1400]  loss 0.000078 (0.021282) train_acc 100.000000 (99.321429)\n",
      "[11,  1500]  loss 0.088204 (0.020969) train_acc 93.750000 (99.329167)\n",
      "[11,  1600]  loss 0.009594 (0.021251) train_acc 100.000000 (99.324219)\n",
      "[11,  1700]  loss 0.018338 (0.021885) train_acc 100.000000 (99.316176)\n",
      "[11,  1800]  loss 0.001692 (0.021800) train_acc 100.000000 (99.309028)\n",
      "[11,  1900]  loss 0.017087 (0.021921) train_acc 100.000000 (99.302632)\n",
      "[11,  2000]  loss 0.001973 (0.021597) train_acc 100.000000 (99.325000)\n",
      "[11,  2100]  loss 0.022883 (0.021424) train_acc 100.000000 (99.333333)\n",
      "[11,  2200]  loss 0.001111 (0.021466) train_acc 100.000000 (99.338068)\n",
      "[11,  2300]  loss 0.000396 (0.022135) train_acc 100.000000 (99.326087)\n",
      "[11,  2400]  loss 0.001171 (0.021828) train_acc 100.000000 (99.333333)\n",
      "[11,  2500]  loss 0.001847 (0.021869) train_acc 100.000000 (99.327500)\n",
      "[11,  2600]  loss 0.134471 (0.022302) train_acc 93.750000 (99.302885)\n",
      "[11,  2700]  loss 0.000952 (0.022745) train_acc 100.000000 (99.293981)\n",
      "[11,  2800]  loss 0.001624 (0.022583) train_acc 100.000000 (99.294643)\n",
      "[11,  2900]  loss 0.000146 (0.022370) train_acc 100.000000 (99.295259)\n",
      "[11,  3000]  loss 0.002917 (0.022223) train_acc 100.000000 (99.297917)\n",
      "[11,  3100]  loss 0.000262 (0.022150) train_acc 100.000000 (99.300403)\n",
      "[11,  3200]  loss 0.020677 (0.021926) train_acc 100.000000 (99.308594)\n",
      "[11,  3300]  loss 0.000206 (0.021658) train_acc 100.000000 (99.318182)\n",
      "[11,  3400]  loss 0.000437 (0.021556) train_acc 100.000000 (99.325368)\n",
      "[11,  3500]  loss 0.006745 (0.021738) train_acc 100.000000 (99.316071)\n",
      "[11,  3600]  loss 0.001903 (0.021604) train_acc 100.000000 (99.319444)\n",
      "[11,  3700]  loss 0.000271 (0.021522) train_acc 100.000000 (99.324324)\n",
      "[12,   100]  loss 0.000095 (0.014071) train_acc 100.000000 (99.562500)\n",
      "[12,   200]  loss 0.001007 (0.011648) train_acc 100.000000 (99.625000)\n",
      "[12,   300]  loss 0.243243 (0.015665) train_acc 87.500000 (99.458333)\n",
      "[12,   400]  loss 0.012546 (0.017024) train_acc 100.000000 (99.421875)\n",
      "[12,   500]  loss 0.134564 (0.017067) train_acc 93.750000 (99.437500)\n",
      "[12,   600]  loss 0.000874 (0.018988) train_acc 100.000000 (99.375000)\n",
      "[12,   700]  loss 0.000506 (0.017656) train_acc 100.000000 (99.401786)\n",
      "[12,   800]  loss 0.002460 (0.018212) train_acc 100.000000 (99.390625)\n",
      "[12,   900]  loss 0.000262 (0.017889) train_acc 100.000000 (99.409722)\n",
      "[12,  1000]  loss 0.000125 (0.018760) train_acc 100.000000 (99.375000)\n",
      "[12,  1100]  loss 0.001724 (0.018954) train_acc 100.000000 (99.369318)\n",
      "[12,  1200]  loss 0.116790 (0.019357) train_acc 93.750000 (99.348958)\n",
      "[12,  1300]  loss 0.002054 (0.020073) train_acc 100.000000 (99.355769)\n",
      "[12,  1400]  loss 0.001060 (0.019963) train_acc 100.000000 (99.357143)\n",
      "[12,  1500]  loss 0.045200 (0.019735) train_acc 93.750000 (99.366667)\n",
      "[12,  1600]  loss 0.000265 (0.019562) train_acc 100.000000 (99.359375)\n",
      "[12,  1700]  loss 0.001320 (0.020004) train_acc 100.000000 (99.363971)\n",
      "[12,  1800]  loss 0.000220 (0.020097) train_acc 100.000000 (99.350694)\n",
      "[12,  1900]  loss 0.002180 (0.019794) train_acc 100.000000 (99.365132)\n",
      "[12,  2000]  loss 0.002333 (0.019750) train_acc 100.000000 (99.362500)\n",
      "[12,  2100]  loss 0.001645 (0.019632) train_acc 100.000000 (99.363095)\n",
      "[12,  2200]  loss 0.000740 (0.019429) train_acc 100.000000 (99.372159)\n",
      "[12,  2300]  loss 0.000816 (0.019280) train_acc 100.000000 (99.385870)\n",
      "[12,  2400]  loss 0.002515 (0.018807) train_acc 100.000000 (99.406250)\n",
      "[12,  2500]  loss 0.100185 (0.019330) train_acc 93.750000 (99.385000)\n",
      "[12,  2600]  loss 0.000281 (0.019479) train_acc 100.000000 (99.377404)\n",
      "[12,  2700]  loss 0.018461 (0.019772) train_acc 100.000000 (99.358796)\n",
      "[12,  2800]  loss 0.000259 (0.019794) train_acc 100.000000 (99.354911)\n",
      "[12,  2900]  loss 0.003310 (0.019915) train_acc 100.000000 (99.351293)\n",
      "[12,  3000]  loss 0.002773 (0.019639) train_acc 100.000000 (99.362500)\n",
      "[12,  3100]  loss 0.001979 (0.019449) train_acc 100.000000 (99.368952)\n",
      "[12,  3200]  loss 0.022747 (0.019523) train_acc 100.000000 (99.367188)\n",
      "[12,  3300]  loss 0.000927 (0.019831) train_acc 100.000000 (99.357955)\n",
      "[12,  3400]  loss 0.000710 (0.019637) train_acc 100.000000 (99.363971)\n",
      "[12,  3500]  loss 0.000429 (0.019726) train_acc 100.000000 (99.364286)\n",
      "[12,  3600]  loss 0.034297 (0.019675) train_acc 100.000000 (99.368056)\n",
      "[12,  3700]  loss 0.000111 (0.019596) train_acc 100.000000 (99.369932)\n",
      "[13,   100]  loss 0.012078 (0.020244) train_acc 100.000000 (98.937500)\n",
      "[13,   200]  loss 0.003433 (0.019476) train_acc 100.000000 (99.093750)\n",
      "[13,   300]  loss 0.003776 (0.017259) train_acc 100.000000 (99.270833)\n",
      "[13,   400]  loss 0.000158 (0.017418) train_acc 100.000000 (99.375000)\n",
      "[13,   500]  loss 0.004018 (0.015885) train_acc 100.000000 (99.475000)\n",
      "[13,   600]  loss 0.000475 (0.016473) train_acc 100.000000 (99.468750)\n",
      "[13,   700]  loss 0.006143 (0.018372) train_acc 100.000000 (99.410714)\n",
      "[13,   800]  loss 0.000117 (0.019891) train_acc 100.000000 (99.359375)\n",
      "[13,   900]  loss 0.004154 (0.019324) train_acc 100.000000 (99.368056)\n",
      "[13,  1000]  loss 0.002541 (0.019705) train_acc 100.000000 (99.356250)\n",
      "[13,  1100]  loss 0.001887 (0.019362) train_acc 100.000000 (99.375000)\n",
      "[13,  1200]  loss 0.000077 (0.019160) train_acc 100.000000 (99.380208)\n",
      "[13,  1300]  loss 0.001079 (0.019348) train_acc 100.000000 (99.389423)\n",
      "[13,  1400]  loss 0.000032 (0.018945) train_acc 100.000000 (99.401786)\n",
      "[13,  1500]  loss 0.000449 (0.018969) train_acc 100.000000 (99.408333)\n",
      "[13,  1600]  loss 0.045369 (0.019418) train_acc 93.750000 (99.386719)\n",
      "[13,  1700]  loss 0.020833 (0.019071) train_acc 100.000000 (99.400735)\n",
      "[13,  1800]  loss 0.000072 (0.018729) train_acc 100.000000 (99.413194)\n",
      "[13,  1900]  loss 0.001375 (0.018344) train_acc 100.000000 (99.430921)\n",
      "[13,  2000]  loss 0.007991 (0.018280) train_acc 100.000000 (99.421875)\n",
      "[13,  2100]  loss 0.000322 (0.018374) train_acc 100.000000 (99.410714)\n",
      "[13,  2200]  loss 0.002548 (0.018061) train_acc 100.000000 (99.420455)\n",
      "[13,  2300]  loss 0.001464 (0.017937) train_acc 100.000000 (99.423913)\n",
      "[13,  2400]  loss 0.000756 (0.017949) train_acc 100.000000 (99.424479)\n",
      "[13,  2500]  loss 0.000241 (0.017893) train_acc 100.000000 (99.425000)\n",
      "[13,  2600]  loss 0.001329 (0.018169) train_acc 100.000000 (99.425481)\n",
      "[13,  2700]  loss 0.000087 (0.018018) train_acc 100.000000 (99.432870)\n",
      "[13,  2800]  loss 0.000282 (0.017722) train_acc 100.000000 (99.444196)\n",
      "[13,  2900]  loss 0.000975 (0.017544) train_acc 100.000000 (99.448276)\n",
      "[13,  3000]  loss 0.000592 (0.017379) train_acc 100.000000 (99.454167)\n",
      "[13,  3100]  loss 0.021253 (0.017652) train_acc 100.000000 (99.443548)\n",
      "[13,  3200]  loss 0.000124 (0.017766) train_acc 100.000000 (99.439453)\n",
      "[13,  3300]  loss 0.002295 (0.017536) train_acc 100.000000 (99.446970)\n",
      "[13,  3400]  loss 0.015481 (0.017580) train_acc 100.000000 (99.452206)\n",
      "[13,  3500]  loss 0.000482 (0.017480) train_acc 100.000000 (99.455357)\n",
      "[13,  3600]  loss 0.008006 (0.017522) train_acc 100.000000 (99.453125)\n",
      "[13,  3700]  loss 0.000167 (0.017404) train_acc 100.000000 (99.456081)\n",
      "[14,   100]  loss 0.000028 (0.010904) train_acc 100.000000 (99.687500)\n",
      "[14,   200]  loss 0.002445 (0.013213) train_acc 100.000000 (99.593750)\n",
      "[14,   300]  loss 0.001093 (0.013832) train_acc 100.000000 (99.625000)\n",
      "[14,   400]  loss 0.000863 (0.014114) train_acc 100.000000 (99.609375)\n",
      "[14,   500]  loss 0.000733 (0.013684) train_acc 100.000000 (99.625000)\n",
      "[14,   600]  loss 0.026796 (0.012737) train_acc 100.000000 (99.645833)\n",
      "[14,   700]  loss 0.003297 (0.013463) train_acc 100.000000 (99.616071)\n",
      "[14,   800]  loss 0.004169 (0.014442) train_acc 100.000000 (99.570312)\n",
      "[14,   900]  loss 0.059074 (0.014799) train_acc 93.750000 (99.541667)\n",
      "[14,  1000]  loss 0.005484 (0.014829) train_acc 100.000000 (99.556250)\n",
      "[14,  1100]  loss 0.014039 (0.014996) train_acc 100.000000 (99.545455)\n",
      "[14,  1200]  loss 0.000045 (0.015258) train_acc 100.000000 (99.541667)\n",
      "[14,  1300]  loss 0.000181 (0.014800) train_acc 100.000000 (99.552885)\n",
      "[14,  1400]  loss 0.395743 (0.015204) train_acc 93.750000 (99.522321)\n",
      "[14,  1500]  loss 0.000449 (0.014569) train_acc 100.000000 (99.545833)\n",
      "[14,  1600]  loss 0.003014 (0.014586) train_acc 100.000000 (99.531250)\n",
      "[14,  1700]  loss 0.000307 (0.015275) train_acc 100.000000 (99.514706)\n",
      "[14,  1800]  loss 0.003635 (0.015207) train_acc 100.000000 (99.506944)\n",
      "[14,  1900]  loss 0.000152 (0.015717) train_acc 100.000000 (99.486842)\n",
      "[14,  2000]  loss 0.000710 (0.016077) train_acc 100.000000 (99.475000)\n",
      "[14,  2100]  loss 0.001405 (0.016061) train_acc 100.000000 (99.479167)\n",
      "[14,  2200]  loss 0.007735 (0.016048) train_acc 100.000000 (99.480114)\n",
      "[14,  2300]  loss 0.000197 (0.015662) train_acc 100.000000 (99.494565)\n",
      "[14,  2400]  loss 0.000036 (0.015327) train_acc 100.000000 (99.505208)\n",
      "[14,  2500]  loss 0.001332 (0.015529) train_acc 100.000000 (99.505000)\n",
      "[14,  2600]  loss 0.000431 (0.015608) train_acc 100.000000 (99.504808)\n",
      "[14,  2700]  loss 0.085760 (0.015939) train_acc 93.750000 (99.500000)\n",
      "[14,  2800]  loss 0.018289 (0.016736) train_acc 100.000000 (99.470982)\n",
      "[14,  2900]  loss 0.000116 (0.016626) train_acc 100.000000 (99.469828)\n",
      "[14,  3000]  loss 0.000205 (0.016555) train_acc 100.000000 (99.470833)\n",
      "[14,  3100]  loss 0.000485 (0.016466) train_acc 100.000000 (99.473790)\n",
      "[14,  3200]  loss 0.000102 (0.016487) train_acc 100.000000 (99.478516)\n",
      "[14,  3300]  loss 0.000350 (0.016266) train_acc 100.000000 (99.486742)\n",
      "[14,  3400]  loss 0.019714 (0.016142) train_acc 100.000000 (99.492647)\n",
      "[14,  3500]  loss 0.000305 (0.016108) train_acc 100.000000 (99.492857)\n",
      "[14,  3600]  loss 0.000812 (0.015943) train_acc 100.000000 (99.498264)\n",
      "[14,  3700]  loss 0.000622 (0.016042) train_acc 100.000000 (99.489865)\n",
      "[15,   100]  loss 0.023248 (0.009052) train_acc 100.000000 (99.812500)\n",
      "[15,   200]  loss 0.001349 (0.012001) train_acc 100.000000 (99.656250)\n",
      "[15,   300]  loss 0.000062 (0.014111) train_acc 100.000000 (99.520833)\n",
      "[15,   400]  loss 0.000769 (0.012911) train_acc 100.000000 (99.562500)\n",
      "[15,   500]  loss 0.025602 (0.014629) train_acc 100.000000 (99.487500)\n",
      "[15,   600]  loss 0.000564 (0.014504) train_acc 100.000000 (99.500000)\n",
      "[15,   700]  loss 0.000561 (0.015217) train_acc 100.000000 (99.482143)\n",
      "[15,   800]  loss 0.087884 (0.016948) train_acc 93.750000 (99.429688)\n",
      "[15,   900]  loss 0.000016 (0.016214) train_acc 100.000000 (99.458333)\n",
      "[15,  1000]  loss 0.000468 (0.015479) train_acc 100.000000 (99.493750)\n",
      "[15,  1100]  loss 0.000389 (0.015084) train_acc 100.000000 (99.517045)\n",
      "[15,  1200]  loss 0.043828 (0.014859) train_acc 93.750000 (99.520833)\n",
      "[15,  1300]  loss 0.008081 (0.014915) train_acc 100.000000 (99.509615)\n",
      "[15,  1400]  loss 0.041568 (0.014203) train_acc 100.000000 (99.540179)\n",
      "[15,  1500]  loss 0.000688 (0.014530) train_acc 100.000000 (99.537500)\n",
      "[15,  1600]  loss 0.003697 (0.014657) train_acc 100.000000 (99.531250)\n",
      "[15,  1700]  loss 0.001565 (0.014615) train_acc 100.000000 (99.529412)\n",
      "[15,  1800]  loss 0.000065 (0.014670) train_acc 100.000000 (99.531250)\n",
      "[15,  1900]  loss 0.000975 (0.014419) train_acc 100.000000 (99.526316)\n",
      "[15,  2000]  loss 0.041829 (0.014490) train_acc 100.000000 (99.534375)\n",
      "[15,  2100]  loss 0.000189 (0.014759) train_acc 100.000000 (99.526786)\n",
      "[15,  2200]  loss 0.015241 (0.014710) train_acc 100.000000 (99.522727)\n",
      "[15,  2300]  loss 0.004755 (0.014774) train_acc 100.000000 (99.527174)\n",
      "[15,  2400]  loss 0.000405 (0.014767) train_acc 100.000000 (99.528646)\n",
      "[15,  2500]  loss 0.027346 (0.014644) train_acc 100.000000 (99.530000)\n",
      "[15,  2600]  loss 0.002873 (0.014626) train_acc 100.000000 (99.538462)\n",
      "[15,  2700]  loss 0.000285 (0.014702) train_acc 100.000000 (99.541667)\n",
      "[15,  2800]  loss 0.000054 (0.014502) train_acc 100.000000 (99.544643)\n",
      "[15,  2900]  loss 0.000558 (0.014315) train_acc 100.000000 (99.553879)\n",
      "[15,  3000]  loss 0.000935 (0.014231) train_acc 100.000000 (99.550000)\n",
      "[15,  3100]  loss 0.075168 (0.014414) train_acc 93.750000 (99.540323)\n",
      "[15,  3200]  loss 0.002736 (0.014275) train_acc 100.000000 (99.546875)\n",
      "[15,  3300]  loss 0.001863 (0.014394) train_acc 100.000000 (99.543561)\n",
      "[15,  3400]  loss 0.000069 (0.014341) train_acc 100.000000 (99.544118)\n",
      "[15,  3500]  loss 0.000636 (0.014260) train_acc 100.000000 (99.546429)\n",
      "[15,  3600]  loss 0.000439 (0.014200) train_acc 100.000000 (99.541667)\n",
      "[15,  3700]  loss 0.007168 (0.014053) train_acc 100.000000 (99.543919)\n",
      "[16,   100]  loss 0.000486 (0.008809) train_acc 100.000000 (99.687500)\n",
      "[16,   200]  loss 0.002765 (0.009557) train_acc 100.000000 (99.656250)\n",
      "[16,   300]  loss 0.000267 (0.011435) train_acc 100.000000 (99.583333)\n",
      "[16,   400]  loss 0.010770 (0.010953) train_acc 100.000000 (99.609375)\n",
      "[16,   500]  loss 0.002230 (0.010630) train_acc 100.000000 (99.650000)\n",
      "[16,   600]  loss 0.014657 (0.010623) train_acc 100.000000 (99.645833)\n",
      "[16,   700]  loss 0.000152 (0.010233) train_acc 100.000000 (99.651786)\n",
      "[16,   800]  loss 0.000056 (0.010598) train_acc 100.000000 (99.656250)\n",
      "[16,   900]  loss 0.004041 (0.011495) train_acc 100.000000 (99.625000)\n",
      "[16,  1000]  loss 0.013566 (0.011342) train_acc 100.000000 (99.631250)\n",
      "[16,  1100]  loss 0.003031 (0.010957) train_acc 100.000000 (99.642045)\n",
      "[16,  1200]  loss 0.000100 (0.011149) train_acc 100.000000 (99.635417)\n",
      "[16,  1300]  loss 0.000705 (0.010988) train_acc 100.000000 (99.644231)\n",
      "[16,  1400]  loss 0.001916 (0.010928) train_acc 100.000000 (99.633929)\n",
      "[16,  1500]  loss 0.000444 (0.012009) train_acc 100.000000 (99.612500)\n",
      "[16,  1600]  loss 0.006952 (0.012324) train_acc 100.000000 (99.601562)\n",
      "[16,  1700]  loss 0.000580 (0.012384) train_acc 100.000000 (99.602941)\n",
      "[16,  1800]  loss 0.000279 (0.012285) train_acc 100.000000 (99.607639)\n",
      "[16,  1900]  loss 0.002254 (0.012598) train_acc 100.000000 (99.598684)\n",
      "[16,  2000]  loss 0.000352 (0.012782) train_acc 100.000000 (99.596875)\n",
      "[16,  2100]  loss 0.000675 (0.012812) train_acc 100.000000 (99.598214)\n",
      "[16,  2200]  loss 0.006107 (0.012661) train_acc 100.000000 (99.602273)\n",
      "[16,  2300]  loss 0.007104 (0.012698) train_acc 100.000000 (99.600543)\n",
      "[16,  2400]  loss 0.001785 (0.012653) train_acc 100.000000 (99.606771)\n",
      "[16,  2500]  loss 0.002008 (0.012508) train_acc 100.000000 (99.610000)\n",
      "[16,  2600]  loss 0.000747 (0.012394) train_acc 100.000000 (99.615385)\n",
      "[16,  2700]  loss 0.000067 (0.012287) train_acc 100.000000 (99.620370)\n",
      "[16,  2800]  loss 0.004193 (0.012219) train_acc 100.000000 (99.618304)\n",
      "[16,  2900]  loss 0.001260 (0.012260) train_acc 100.000000 (99.609914)\n",
      "[16,  3000]  loss 0.007143 (0.012160) train_acc 100.000000 (99.610417)\n",
      "[16,  3100]  loss 0.000316 (0.012310) train_acc 100.000000 (99.606855)\n",
      "[16,  3200]  loss 0.000409 (0.012604) train_acc 100.000000 (99.593750)\n",
      "[16,  3300]  loss 0.007300 (0.012451) train_acc 100.000000 (99.598485)\n",
      "[16,  3400]  loss 0.000125 (0.012546) train_acc 100.000000 (99.597426)\n",
      "[16,  3500]  loss 0.000082 (0.012535) train_acc 100.000000 (99.598214)\n",
      "[16,  3600]  loss 0.032927 (0.012573) train_acc 100.000000 (99.597222)\n",
      "[16,  3700]  loss 0.011029 (0.012616) train_acc 100.000000 (99.592905)\n",
      "[17,   100]  loss 0.039886 (0.006746) train_acc 100.000000 (99.812500)\n",
      "[17,   200]  loss 0.006336 (0.009355) train_acc 100.000000 (99.687500)\n",
      "[17,   300]  loss 0.000154 (0.010890) train_acc 100.000000 (99.687500)\n",
      "[17,   400]  loss 0.000126 (0.010649) train_acc 100.000000 (99.687500)\n",
      "[17,   500]  loss 0.000360 (0.010042) train_acc 100.000000 (99.700000)\n",
      "[17,   600]  loss 0.000185 (0.009302) train_acc 100.000000 (99.729167)\n",
      "[17,   700]  loss 0.000140 (0.008864) train_acc 100.000000 (99.741071)\n",
      "[17,   800]  loss 0.001098 (0.009047) train_acc 100.000000 (99.726562)\n",
      "[17,   900]  loss 0.007489 (0.009053) train_acc 100.000000 (99.715278)\n",
      "[17,  1000]  loss 0.000078 (0.009139) train_acc 100.000000 (99.712500)\n",
      "[17,  1100]  loss 0.000023 (0.009044) train_acc 100.000000 (99.710227)\n",
      "[17,  1200]  loss 0.000089 (0.009209) train_acc 100.000000 (99.708333)\n",
      "[17,  1300]  loss 0.000097 (0.010185) train_acc 100.000000 (99.658654)\n",
      "[17,  1400]  loss 0.002225 (0.010552) train_acc 100.000000 (99.642857)\n",
      "[17,  1500]  loss 0.000235 (0.010962) train_acc 100.000000 (99.637500)\n",
      "[17,  1600]  loss 0.000376 (0.010788) train_acc 100.000000 (99.640625)\n",
      "[17,  1700]  loss 0.099131 (0.010896) train_acc 93.750000 (99.636029)\n",
      "[17,  1800]  loss 0.001192 (0.010644) train_acc 100.000000 (99.645833)\n",
      "[17,  1900]  loss 0.077420 (0.010380) train_acc 93.750000 (99.651316)\n",
      "[17,  2000]  loss 0.000153 (0.010435) train_acc 100.000000 (99.650000)\n",
      "[17,  2100]  loss 0.000140 (0.010175) train_acc 100.000000 (99.657738)\n",
      "[17,  2200]  loss 0.022702 (0.010186) train_acc 100.000000 (99.664773)\n",
      "[17,  2300]  loss 0.000142 (0.010362) train_acc 100.000000 (99.660326)\n",
      "[17,  2400]  loss 0.000022 (0.010645) train_acc 100.000000 (99.651042)\n",
      "[17,  2500]  loss 0.005114 (0.010531) train_acc 100.000000 (99.655000)\n",
      "[17,  2600]  loss 0.000475 (0.010596) train_acc 100.000000 (99.646635)\n",
      "[17,  2700]  loss 0.001879 (0.010853) train_acc 100.000000 (99.643519)\n",
      "[17,  2800]  loss 0.017284 (0.010744) train_acc 100.000000 (99.647321)\n",
      "[17,  2900]  loss 0.000418 (0.010741) train_acc 100.000000 (99.642241)\n",
      "[17,  3000]  loss 0.025272 (0.010806) train_acc 100.000000 (99.645833)\n",
      "[17,  3100]  loss 0.007089 (0.010870) train_acc 100.000000 (99.645161)\n",
      "[17,  3200]  loss 0.000050 (0.010791) train_acc 100.000000 (99.648438)\n",
      "[17,  3300]  loss 0.006117 (0.010963) train_acc 100.000000 (99.645833)\n",
      "[17,  3400]  loss 0.002538 (0.010882) train_acc 100.000000 (99.650735)\n",
      "[17,  3500]  loss 0.001086 (0.010903) train_acc 100.000000 (99.646429)\n",
      "[17,  3600]  loss 0.000543 (0.010978) train_acc 100.000000 (99.642361)\n",
      "[17,  3700]  loss 0.043541 (0.011111) train_acc 93.750000 (99.640203)\n",
      "[18,   100]  loss 0.000046 (0.015642) train_acc 100.000000 (99.562500)\n",
      "[18,   200]  loss 0.001040 (0.010294) train_acc 100.000000 (99.750000)\n",
      "[18,   300]  loss 0.002861 (0.008600) train_acc 100.000000 (99.791667)\n",
      "[18,   400]  loss 0.000491 (0.011177) train_acc 100.000000 (99.734375)\n",
      "[18,   500]  loss 0.000274 (0.009701) train_acc 100.000000 (99.775000)\n",
      "[18,   600]  loss 0.000294 (0.009060) train_acc 100.000000 (99.791667)\n",
      "[18,   700]  loss 0.000001 (0.008695) train_acc 100.000000 (99.803571)\n",
      "[18,   800]  loss 0.046028 (0.008178) train_acc 100.000000 (99.820312)\n",
      "[18,   900]  loss 0.000001 (0.007748) train_acc 100.000000 (99.826389)\n",
      "[18,  1000]  loss 0.001680 (0.007744) train_acc 100.000000 (99.812500)\n",
      "[18,  1100]  loss 0.000637 (0.007888) train_acc 100.000000 (99.806818)\n",
      "[18,  1200]  loss 0.044064 (0.007968) train_acc 93.750000 (99.807292)\n",
      "[18,  1300]  loss 0.027988 (0.008255) train_acc 100.000000 (99.788462)\n",
      "[18,  1400]  loss 0.000017 (0.008418) train_acc 100.000000 (99.763393)\n",
      "[18,  1500]  loss 0.000134 (0.008665) train_acc 100.000000 (99.754167)\n",
      "[18,  1600]  loss 0.004226 (0.008724) train_acc 100.000000 (99.750000)\n",
      "[18,  1700]  loss 0.002845 (0.008526) train_acc 100.000000 (99.753676)\n",
      "[18,  1800]  loss 0.000505 (0.008464) train_acc 100.000000 (99.753472)\n",
      "[18,  1900]  loss 0.000236 (0.008568) train_acc 100.000000 (99.750000)\n",
      "[18,  2000]  loss 0.016675 (0.008546) train_acc 100.000000 (99.750000)\n",
      "[18,  2100]  loss 0.000077 (0.008616) train_acc 100.000000 (99.747024)\n",
      "[18,  2200]  loss 0.000348 (0.008744) train_acc 100.000000 (99.738636)\n",
      "[18,  2300]  loss 0.004688 (0.008996) train_acc 100.000000 (99.725543)\n",
      "[18,  2400]  loss 0.023323 (0.009347) train_acc 100.000000 (99.708333)\n",
      "[18,  2500]  loss 0.111790 (0.009401) train_acc 93.750000 (99.710000)\n",
      "[18,  2600]  loss 0.000201 (0.009316) train_acc 100.000000 (99.713942)\n",
      "[18,  2700]  loss 0.000353 (0.009234) train_acc 100.000000 (99.717593)\n",
      "[18,  2800]  loss 0.000915 (0.009434) train_acc 100.000000 (99.709821)\n",
      "[18,  2900]  loss 0.000866 (0.009558) train_acc 100.000000 (99.709052)\n",
      "[18,  3000]  loss 0.000293 (0.009723) train_acc 100.000000 (99.706250)\n",
      "[18,  3100]  loss 0.120052 (0.009732) train_acc 93.750000 (99.703629)\n",
      "[18,  3200]  loss 0.005330 (0.009654) train_acc 100.000000 (99.707031)\n",
      "[18,  3300]  loss 0.032145 (0.009915) train_acc 100.000000 (99.704545)\n",
      "[18,  3400]  loss 0.001123 (0.010055) train_acc 100.000000 (99.698529)\n",
      "[18,  3500]  loss 0.003645 (0.010100) train_acc 100.000000 (99.694643)\n",
      "[18,  3600]  loss 0.000377 (0.010076) train_acc 100.000000 (99.697917)\n",
      "[18,  3700]  loss 0.000043 (0.010164) train_acc 100.000000 (99.695946)\n",
      "[19,   100]  loss 0.058410 (0.010537) train_acc 93.750000 (99.750000)\n",
      "[19,   200]  loss 0.002570 (0.007284) train_acc 100.000000 (99.843750)\n",
      "[19,   300]  loss 0.000011 (0.007952) train_acc 100.000000 (99.812500)\n",
      "[19,   400]  loss 0.000159 (0.007264) train_acc 100.000000 (99.812500)\n",
      "[19,   500]  loss 0.002400 (0.007264) train_acc 100.000000 (99.812500)\n",
      "[19,   600]  loss 0.001424 (0.007359) train_acc 100.000000 (99.802083)\n",
      "[19,   700]  loss 0.000846 (0.007044) train_acc 100.000000 (99.821429)\n",
      "[19,   800]  loss 0.000021 (0.007614) train_acc 100.000000 (99.765625)\n",
      "[19,   900]  loss 0.000129 (0.008719) train_acc 100.000000 (99.750000)\n",
      "[19,  1000]  loss 0.011403 (0.008328) train_acc 100.000000 (99.762500)\n",
      "[19,  1100]  loss 0.017432 (0.007870) train_acc 100.000000 (99.784091)\n",
      "[19,  1200]  loss 0.000113 (0.008615) train_acc 100.000000 (99.750000)\n",
      "[19,  1300]  loss 0.061410 (0.008938) train_acc 100.000000 (99.735577)\n",
      "[19,  1400]  loss 0.003720 (0.008639) train_acc 100.000000 (99.745536)\n",
      "[19,  1500]  loss 0.000703 (0.008525) train_acc 100.000000 (99.745833)\n",
      "[19,  1600]  loss 0.000818 (0.008379) train_acc 100.000000 (99.750000)\n",
      "[19,  1700]  loss 0.026334 (0.008262) train_acc 100.000000 (99.757353)\n",
      "[19,  1800]  loss 0.002669 (0.008355) train_acc 100.000000 (99.746528)\n",
      "[19,  1900]  loss 0.000303 (0.008574) train_acc 100.000000 (99.736842)\n",
      "[19,  2000]  loss 0.000135 (0.008548) train_acc 100.000000 (99.737500)\n",
      "[19,  2100]  loss 0.003085 (0.008490) train_acc 100.000000 (99.741071)\n",
      "[19,  2200]  loss 0.000275 (0.008589) train_acc 100.000000 (99.735795)\n",
      "[19,  2300]  loss 0.008139 (0.008453) train_acc 100.000000 (99.741848)\n",
      "[19,  2400]  loss 0.000026 (0.008468) train_acc 100.000000 (99.744792)\n",
      "[19,  2500]  loss 0.000023 (0.008650) train_acc 100.000000 (99.742500)\n",
      "[19,  2600]  loss 0.000029 (0.008425) train_acc 100.000000 (99.750000)\n",
      "[19,  2700]  loss 0.003710 (0.008474) train_acc 100.000000 (99.750000)\n",
      "[19,  2800]  loss 0.007628 (0.008657) train_acc 100.000000 (99.745536)\n",
      "[19,  2900]  loss 0.000248 (0.008589) train_acc 100.000000 (99.747845)\n",
      "[19,  3000]  loss 0.000377 (0.008490) train_acc 100.000000 (99.750000)\n",
      "[19,  3100]  loss 0.000370 (0.008610) train_acc 100.000000 (99.739919)\n",
      "[19,  3200]  loss 0.000052 (0.008688) train_acc 100.000000 (99.734375)\n",
      "[19,  3300]  loss 0.000030 (0.008684) train_acc 100.000000 (99.738636)\n",
      "[19,  3400]  loss 0.000085 (0.008773) train_acc 100.000000 (99.733456)\n",
      "[19,  3500]  loss 0.009708 (0.008693) train_acc 100.000000 (99.735714)\n",
      "[19,  3600]  loss 0.005357 (0.008885) train_acc 100.000000 (99.730903)\n",
      "[19,  3700]  loss 0.002474 (0.008823) train_acc 100.000000 (99.734797)\n",
      "[20,   100]  loss 0.000384 (0.008605) train_acc 100.000000 (99.687500)\n",
      "[20,   200]  loss 0.000985 (0.007813) train_acc 100.000000 (99.750000)\n",
      "[20,   300]  loss 0.008540 (0.006614) train_acc 100.000000 (99.833333)\n",
      "[20,   400]  loss 0.000004 (0.005954) train_acc 100.000000 (99.859375)\n",
      "[20,   500]  loss 0.006478 (0.005533) train_acc 100.000000 (99.887500)\n",
      "[20,   600]  loss 0.000052 (0.006542) train_acc 100.000000 (99.843750)\n",
      "[20,   700]  loss 0.000826 (0.006775) train_acc 100.000000 (99.803571)\n",
      "[20,   800]  loss 0.002688 (0.006639) train_acc 100.000000 (99.796875)\n",
      "[20,   900]  loss 0.000095 (0.006190) train_acc 100.000000 (99.819444)\n",
      "[20,  1000]  loss 0.000005 (0.006505) train_acc 100.000000 (99.806250)\n",
      "[20,  1100]  loss 0.002287 (0.006473) train_acc 100.000000 (99.806818)\n",
      "[20,  1200]  loss 0.000069 (0.006579) train_acc 100.000000 (99.802083)\n",
      "[20,  1300]  loss 0.000002 (0.006412) train_acc 100.000000 (99.802885)\n",
      "[20,  1400]  loss 0.002252 (0.006357) train_acc 100.000000 (99.808036)\n",
      "[20,  1500]  loss 0.098410 (0.006603) train_acc 93.750000 (99.795833)\n",
      "[20,  1600]  loss 0.000336 (0.006630) train_acc 100.000000 (99.789062)\n",
      "[20,  1700]  loss 0.000075 (0.006987) train_acc 100.000000 (99.775735)\n",
      "[20,  1800]  loss 0.000985 (0.007100) train_acc 100.000000 (99.777778)\n",
      "[20,  1900]  loss 0.000037 (0.007262) train_acc 100.000000 (99.769737)\n",
      "[20,  2000]  loss 0.016058 (0.007346) train_acc 100.000000 (99.768750)\n",
      "[20,  2100]  loss 0.016912 (0.007550) train_acc 100.000000 (99.764881)\n",
      "[20,  2200]  loss 0.000003 (0.007857) train_acc 100.000000 (99.755682)\n",
      "[20,  2300]  loss 0.214088 (0.007917) train_acc 93.750000 (99.755435)\n",
      "[20,  2400]  loss 0.002131 (0.007885) train_acc 100.000000 (99.757812)\n",
      "[20,  2500]  loss 0.000192 (0.007867) train_acc 100.000000 (99.750000)\n",
      "[20,  2600]  loss 0.003906 (0.008040) train_acc 100.000000 (99.737981)\n",
      "[20,  2700]  loss 0.000197 (0.008027) train_acc 100.000000 (99.738426)\n",
      "[20,  2800]  loss 0.018189 (0.008124) train_acc 100.000000 (99.736607)\n",
      "[20,  2900]  loss 0.000680 (0.008023) train_acc 100.000000 (99.739224)\n",
      "[20,  3000]  loss 0.001836 (0.008236) train_acc 100.000000 (99.733333)\n",
      "[20,  3100]  loss 0.005557 (0.008278) train_acc 100.000000 (99.731855)\n",
      "[20,  3200]  loss 0.003513 (0.008259) train_acc 100.000000 (99.732422)\n",
      "[20,  3300]  loss 0.000920 (0.008402) train_acc 100.000000 (99.732955)\n",
      "[20,  3400]  loss 0.001228 (0.008423) train_acc 100.000000 (99.731618)\n",
      "[20,  3500]  loss 0.008375 (0.008469) train_acc 100.000000 (99.728571)\n",
      "[20,  3600]  loss 0.000013 (0.008359) train_acc 100.000000 (99.734375)\n",
      "[20,  3700]  loss 0.000081 (0.008565) train_acc 100.000000 (99.731419)\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "train(model=qnet, dataloader=trainloader, cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model after quantization\n",
      "Size (MB):  0.05793\n",
      "Accuracy of the fused and quantized network (trianed quantized) on the test images: 99.11% - INT8\n"
     ]
    }
   ],
   "source": [
    "qnet = qnet.to(device=torch.device(\"cpu\"))\n",
    "torch.quantization.convert(module=qnet, inplace=True)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(model=qnet)\n",
    "\n",
    "score = test(model=qnet, dataloader=testloader, cuda=False)\n",
    "print(f\"Accuracy of the fused and quantized network (trianed quantized) on the test images: {score}% - INT8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the training loop by freezing the quantizer parameters (scale and zero-point) and finetune the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100]  loss 2.288820 (2.298613) train_acc 6.250000 (10.187500)\n",
      "[1,   200]  loss 2.292001 (2.294692) train_acc 6.250000 (11.687500)\n",
      "[1,   300]  loss 2.261161 (2.286984) train_acc 25.000000 (16.229167)\n",
      "[1,   400]  loss 2.262180 (2.277621) train_acc 37.500000 (20.750000)\n",
      "[1,   500]  loss 2.228750 (2.265902) train_acc 18.750000 (26.200000)\n",
      "[1,   600]  loss 2.139659 (2.249977) train_acc 56.250000 (31.125000)\n",
      "[1,   700]  loss 1.966706 (2.226303) train_acc 62.500000 (35.821429)\n",
      "[1,   800]  loss 1.803692 (2.191116) train_acc 87.500000 (39.820312)\n",
      "[1,   900]  loss 1.603228 (2.137844) train_acc 62.500000 (43.208333)\n",
      "[1,  1000]  loss 1.280798 (2.060221) train_acc 68.750000 (46.431250)\n",
      "[1,  1100]  loss 0.863921 (1.963603) train_acc 68.750000 (49.386364)\n",
      "[1,  1200]  loss 0.851023 (1.858339) train_acc 81.250000 (52.031250)\n",
      "[1,  1300]  loss 0.779770 (1.759009) train_acc 81.250000 (54.418269)\n",
      "[1,  1400]  loss 0.899344 (1.669670) train_acc 62.500000 (56.517857)\n",
      "[1,  1500]  loss 0.260186 (1.590217) train_acc 93.750000 (58.404167)\n",
      "[1,  1600]  loss 0.703635 (1.519025) train_acc 75.000000 (60.082031)\n",
      "[1,  1700]  loss 0.218911 (1.453682) train_acc 93.750000 (61.654412)\n",
      "[1,  1800]  loss 0.362300 (1.393992) train_acc 81.250000 (63.163194)\n",
      "[1,  1900]  loss 0.092007 (1.337823) train_acc 100.000000 (64.582237)\n",
      "[1,  2000]  loss 0.222862 (1.286085) train_acc 87.500000 (65.896875)\n",
      "[1,  2100]  loss 0.500014 (1.237763) train_acc 87.500000 (67.092262)\n",
      "[1,  2200]  loss 0.651430 (1.194849) train_acc 68.750000 (68.161932)\n",
      "[1,  2300]  loss 0.039044 (1.154283) train_acc 100.000000 (69.187500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2400]  loss 0.304696 (1.115528) train_acc 87.500000 (70.190104)\n",
      "[1,  2500]  loss 0.574006 (1.080056) train_acc 87.500000 (71.087500)\n",
      "[1,  2600]  loss 0.097433 (1.047876) train_acc 100.000000 (71.923077)\n",
      "[1,  2700]  loss 0.127464 (1.016685) train_acc 93.750000 (72.712963)\n",
      "[1,  2800]  loss 0.264521 (0.987400) train_acc 87.500000 (73.470982)\n",
      "[1,  2900]  loss 0.455620 (0.960537) train_acc 87.500000 (74.146552)\n",
      "[1,  3000]  loss 0.318743 (0.935052) train_acc 87.500000 (74.802083)\n",
      "[1,  3100]  loss 0.050227 (0.911382) train_acc 100.000000 (75.411290)\n",
      "[1,  3200]  loss 0.116966 (0.889287) train_acc 100.000000 (75.982422)\n",
      "[1,  3300]  loss 0.046975 (0.867586) train_acc 100.000000 (76.534091)\n",
      "[1,  3400]  loss 0.057824 (0.847340) train_acc 100.000000 (77.051471)\n",
      "[1,  3500]  loss 0.341893 (0.828908) train_acc 87.500000 (77.539286)\n",
      "[1,  3600]  loss 0.059921 (0.810712) train_acc 100.000000 (78.024306)\n",
      "[1,  3700]  loss 0.096222 (0.793593) train_acc 100.000000 (78.471284)\n",
      "[2,   100]  loss 0.253829 (0.165213) train_acc 93.750000 (94.875000)\n",
      "[2,   200]  loss 0.153562 (0.156834) train_acc 93.750000 (94.906250)\n",
      "[2,   300]  loss 0.038137 (0.153827) train_acc 100.000000 (95.062500)\n",
      "[2,   400]  loss 0.078493 (0.153596) train_acc 100.000000 (94.937500)\n",
      "[2,   500]  loss 0.550218 (0.149150) train_acc 93.750000 (95.125000)\n",
      "[2,   600]  loss 0.074550 (0.145899) train_acc 100.000000 (95.291667)\n",
      "[2,   700]  loss 0.138438 (0.147887) train_acc 93.750000 (95.214286)\n",
      "[2,   800]  loss 0.144928 (0.146805) train_acc 93.750000 (95.281250)\n",
      "[2,   900]  loss 0.201326 (0.142573) train_acc 87.500000 (95.409722)\n",
      "[2,  1000]  loss 0.044121 (0.142136) train_acc 100.000000 (95.375000)\n",
      "[2,  1100]  loss 0.161890 (0.139631) train_acc 93.750000 (95.482955)\n",
      "[2,  1200]  loss 0.006041 (0.139936) train_acc 100.000000 (95.489583)\n",
      "[2,  1300]  loss 0.026004 (0.141343) train_acc 100.000000 (95.475962)\n",
      "[2,  1400]  loss 0.038051 (0.139753) train_acc 100.000000 (95.513393)\n",
      "[2,  1500]  loss 0.150482 (0.137507) train_acc 93.750000 (95.583333)\n",
      "[2,  1600]  loss 0.065320 (0.136373) train_acc 100.000000 (95.632812)\n",
      "[2,  1700]  loss 0.037074 (0.134953) train_acc 100.000000 (95.650735)\n",
      "[2,  1800]  loss 0.019993 (0.133171) train_acc 100.000000 (95.739583)\n",
      "[2,  1900]  loss 0.095822 (0.131995) train_acc 93.750000 (95.809211)\n",
      "[2,  2000]  loss 0.113534 (0.131574) train_acc 93.750000 (95.793750)\n",
      "[2,  2100]  loss 0.025732 (0.130371) train_acc 100.000000 (95.815476)\n",
      "[2,  2200]  loss 0.210673 (0.128929) train_acc 87.500000 (95.860795)\n",
      "[2,  2300]  loss 0.136501 (0.128153) train_acc 93.750000 (95.880435)\n",
      "[2,  2400]  loss 0.024962 (0.128242) train_acc 100.000000 (95.869792)\n",
      "[2,  2500]  loss 0.016290 (0.127628) train_acc 100.000000 (95.897500)\n",
      "[2,  2600]  loss 0.272106 (0.126862) train_acc 81.250000 (95.918269)\n",
      "[2,  2700]  loss 0.078693 (0.126332) train_acc 100.000000 (95.935185)\n",
      "[2,  2800]  loss 0.026603 (0.126044) train_acc 100.000000 (95.937500)\n",
      "[2,  2900]  loss 0.150861 (0.126533) train_acc 93.750000 (95.935345)\n",
      "[2,  3000]  loss 0.093457 (0.125655) train_acc 93.750000 (95.962500)\n",
      "[2,  3100]  loss 0.127496 (0.125196) train_acc 93.750000 (95.985887)\n",
      "[2,  3200]  loss 0.016382 (0.124231) train_acc 100.000000 (96.007812)\n",
      "[2,  3300]  loss 0.161511 (0.123721) train_acc 93.750000 (96.037879)\n",
      "[2,  3400]  loss 0.187846 (0.122996) train_acc 93.750000 (96.055147)\n",
      "[2,  3500]  loss 0.038787 (0.122429) train_acc 100.000000 (96.083929)\n",
      "[2,  3600]  loss 0.025725 (0.122182) train_acc 100.000000 (96.112847)\n",
      "[2,  3700]  loss 0.023951 (0.120972) train_acc 100.000000 (96.153716)\n",
      "[3,   100]  loss 0.074463 (0.097837) train_acc 93.750000 (96.562500)\n",
      "[3,   200]  loss 0.025395 (0.083245) train_acc 100.000000 (97.125000)\n",
      "[3,   300]  loss 0.295110 (0.083550) train_acc 93.750000 (97.187500)\n",
      "[3,   400]  loss 0.008382 (0.086505) train_acc 100.000000 (97.156250)\n",
      "[3,   500]  loss 0.122367 (0.089515) train_acc 93.750000 (97.037500)\n",
      "[3,   600]  loss 0.020850 (0.090047) train_acc 100.000000 (97.010417)\n",
      "[3,   700]  loss 0.005319 (0.089922) train_acc 100.000000 (97.062500)\n",
      "[3,   800]  loss 0.044552 (0.087984) train_acc 100.000000 (97.125000)\n",
      "[3,   900]  loss 0.036796 (0.089493) train_acc 100.000000 (97.159722)\n",
      "[3,  1000]  loss 0.015454 (0.089747) train_acc 100.000000 (97.118750)\n",
      "[3,  1100]  loss 0.008256 (0.089252) train_acc 100.000000 (97.181818)\n",
      "[3,  1200]  loss 0.146228 (0.087932) train_acc 93.750000 (97.171875)\n",
      "[3,  1300]  loss 0.189777 (0.087540) train_acc 93.750000 (97.201923)\n",
      "[3,  1400]  loss 0.016957 (0.087757) train_acc 100.000000 (97.236607)\n",
      "[3,  1500]  loss 0.086123 (0.087526) train_acc 100.000000 (97.270833)\n",
      "[3,  1600]  loss 0.047238 (0.086676) train_acc 93.750000 (97.292969)\n",
      "[3,  1700]  loss 0.273240 (0.085337) train_acc 93.750000 (97.316176)\n",
      "[3,  1800]  loss 0.028539 (0.084631) train_acc 100.000000 (97.326389)\n",
      "[3,  1900]  loss 0.066866 (0.084754) train_acc 100.000000 (97.351974)\n",
      "[3,  2000]  loss 0.000833 (0.084336) train_acc 100.000000 (97.368750)\n",
      "[3,  2100]  loss 0.049985 (0.082840) train_acc 100.000000 (97.422619)\n",
      "[3,  2200]  loss 0.026397 (0.082072) train_acc 100.000000 (97.454545)\n",
      "[3,  2300]  loss 0.003513 (0.081559) train_acc 100.000000 (97.475543)\n",
      "[3,  2400]  loss 0.045627 (0.081217) train_acc 100.000000 (97.484375)\n",
      "[3,  2500]  loss 0.461566 (0.081562) train_acc 87.500000 (97.460000)\n",
      "[3,  2600]  loss 0.015976 (0.081870) train_acc 100.000000 (97.444712)\n",
      "[3,  2700]  loss 0.112999 (0.081654) train_acc 93.750000 (97.458333)\n",
      "[3,  2800]  loss 0.087408 (0.081173) train_acc 93.750000 (97.466518)\n",
      "[3,  2900]  loss 0.022431 (0.081091) train_acc 100.000000 (97.474138)\n",
      "[3,  3000]  loss 0.026703 (0.080951) train_acc 100.000000 (97.460417)\n",
      "[3,  3100]  loss 0.255294 (0.081377) train_acc 87.500000 (97.445565)\n",
      "[3,  3200]  loss 0.056758 (0.080914) train_acc 93.750000 (97.457031)\n",
      "[3,  3300]  loss 0.039286 (0.081741) train_acc 100.000000 (97.441288)\n",
      "[3,  3400]  loss 0.072873 (0.081285) train_acc 93.750000 (97.448529)\n",
      "[3,  3500]  loss 0.004701 (0.080940) train_acc 100.000000 (97.448214)\n",
      "[3,  3600]  loss 0.032463 (0.080858) train_acc 100.000000 (97.451389)\n",
      "[3,  3700]  loss 0.201018 (0.080616) train_acc 93.750000 (97.454392)\n",
      "[4,   100]  loss 0.013557 (0.056377) train_acc 100.000000 (98.375000)\n",
      "[4,   200]  loss 0.037140 (0.059725) train_acc 100.000000 (98.312500)\n",
      "[4,   300]  loss 0.038792 (0.065334) train_acc 100.000000 (98.083333)\n",
      "[4,   400]  loss 0.051834 (0.069774) train_acc 100.000000 (97.906250)\n",
      "[4,   500]  loss 0.088854 (0.070749) train_acc 93.750000 (97.825000)\n",
      "[4,   600]  loss 0.496087 (0.072457) train_acc 93.750000 (97.770833)\n",
      "[4,   700]  loss 0.005120 (0.068749) train_acc 100.000000 (97.892857)\n",
      "[4,   800]  loss 0.029480 (0.067940) train_acc 100.000000 (97.890625)\n",
      "[4,   900]  loss 0.010622 (0.067339) train_acc 100.000000 (97.916667)\n",
      "[4,  1000]  loss 0.013088 (0.068312) train_acc 100.000000 (97.875000)\n",
      "[4,  1100]  loss 0.023191 (0.068243) train_acc 100.000000 (97.863636)\n",
      "[4,  1200]  loss 0.005635 (0.068032) train_acc 100.000000 (97.848958)\n",
      "[4,  1300]  loss 0.153511 (0.068506) train_acc 93.750000 (97.831731)\n",
      "[4,  1400]  loss 0.086226 (0.067852) train_acc 100.000000 (97.821429)\n",
      "[4,  1500]  loss 0.355970 (0.067501) train_acc 93.750000 (97.841667)\n",
      "[4,  1600]  loss 0.026900 (0.068234) train_acc 100.000000 (97.812500)\n",
      "[4,  1700]  loss 0.023022 (0.068779) train_acc 100.000000 (97.801471)\n",
      "[4,  1800]  loss 0.009578 (0.068855) train_acc 100.000000 (97.788194)\n",
      "[4,  1900]  loss 0.003110 (0.068372) train_acc 100.000000 (97.799342)\n",
      "[4,  2000]  loss 0.021380 (0.068615) train_acc 100.000000 (97.796875)\n",
      "[4,  2100]  loss 0.060999 (0.068690) train_acc 93.750000 (97.800595)\n",
      "[4,  2200]  loss 0.017401 (0.069049) train_acc 100.000000 (97.792614)\n",
      "[4,  2300]  loss 0.006596 (0.068377) train_acc 100.000000 (97.828804)\n",
      "[4,  2400]  loss 0.063080 (0.068901) train_acc 100.000000 (97.817708)\n",
      "[4,  2500]  loss 0.010185 (0.068185) train_acc 100.000000 (97.847500)\n",
      "[4,  2600]  loss 0.078459 (0.068168) train_acc 93.750000 (97.848558)\n",
      "[4,  2700]  loss 0.014042 (0.067921) train_acc 100.000000 (97.840278)\n",
      "[4,  2800]  loss 0.143690 (0.067213) train_acc 93.750000 (97.859375)\n",
      "[4,  2900]  loss 0.005970 (0.066498) train_acc 100.000000 (97.872845)\n",
      "[4,  3000]  loss 0.001473 (0.066177) train_acc 100.000000 (97.891667)\n",
      "[4,  3100]  loss 0.089986 (0.065851) train_acc 93.750000 (97.905242)\n",
      "[4,  3200]  loss 0.004884 (0.065111) train_acc 100.000000 (97.935547)\n",
      "[4,  3300]  loss 0.052313 (0.065147) train_acc 93.750000 (97.922348)\n",
      "[4,  3400]  loss 0.023893 (0.064956) train_acc 100.000000 (97.924632)\n",
      "[4,  3500]  loss 0.012157 (0.064913) train_acc 100.000000 (97.932143)\n",
      "[4,  3600]  loss 0.167626 (0.063918) train_acc 87.500000 (97.963542)\n",
      "[4,  3700]  loss 0.007213 (0.064543) train_acc 100.000000 (97.954392)\n",
      "[5,   100]  loss 0.010668 (0.051867) train_acc 100.000000 (98.312500)\n",
      "[5,   200]  loss 0.095935 (0.047237) train_acc 93.750000 (98.531250)\n",
      "[5,   300]  loss 0.046175 (0.047590) train_acc 100.000000 (98.604167)\n",
      "[5,   400]  loss 0.022529 (0.054040) train_acc 100.000000 (98.421875)\n",
      "[5,   500]  loss 0.011735 (0.054811) train_acc 100.000000 (98.350000)\n",
      "[5,   600]  loss 0.084334 (0.054515) train_acc 93.750000 (98.281250)\n",
      "[5,   700]  loss 0.009357 (0.053011) train_acc 100.000000 (98.339286)\n",
      "[5,   800]  loss 0.014846 (0.055288) train_acc 100.000000 (98.304688)\n",
      "[5,   900]  loss 0.160099 (0.055701) train_acc 93.750000 (98.305556)\n",
      "[5,  1000]  loss 0.031266 (0.056654) train_acc 100.000000 (98.268750)\n",
      "[5,  1100]  loss 0.016471 (0.057570) train_acc 100.000000 (98.261364)\n",
      "[5,  1200]  loss 0.103473 (0.057822) train_acc 93.750000 (98.234375)\n",
      "[5,  1300]  loss 0.373171 (0.057647) train_acc 93.750000 (98.245192)\n",
      "[5,  1400]  loss 0.005439 (0.056671) train_acc 100.000000 (98.272321)\n",
      "[5,  1500]  loss 0.384977 (0.056137) train_acc 93.750000 (98.283333)\n",
      "[5,  1600]  loss 0.207199 (0.055578) train_acc 93.750000 (98.289062)\n",
      "[5,  1700]  loss 0.039257 (0.054569) train_acc 100.000000 (98.308824)\n",
      "[5,  1800]  loss 0.000671 (0.054551) train_acc 100.000000 (98.291667)\n",
      "[5,  1900]  loss 0.020791 (0.054414) train_acc 100.000000 (98.296053)\n",
      "[5,  2000]  loss 0.005699 (0.054557) train_acc 100.000000 (98.271875)\n",
      "[5,  2100]  loss 0.001771 (0.054769) train_acc 100.000000 (98.258929)\n",
      "[5,  2200]  loss 0.081049 (0.054767) train_acc 93.750000 (98.255682)\n",
      "[5,  2300]  loss 0.019773 (0.054400) train_acc 100.000000 (98.274457)\n",
      "[5,  2400]  loss 0.054835 (0.054386) train_acc 100.000000 (98.260417)\n",
      "[5,  2500]  loss 0.000972 (0.054491) train_acc 100.000000 (98.245000)\n",
      "[5,  2600]  loss 0.001595 (0.054346) train_acc 100.000000 (98.252404)\n",
      "[5,  2700]  loss 0.011686 (0.053957) train_acc 100.000000 (98.268519)\n",
      "[5,  2800]  loss 0.005334 (0.054081) train_acc 100.000000 (98.270089)\n",
      "[5,  2900]  loss 0.026727 (0.053852) train_acc 100.000000 (98.278017)\n",
      "[5,  3000]  loss 0.013426 (0.053635) train_acc 100.000000 (98.293750)\n",
      "[5,  3100]  loss 0.074317 (0.053422) train_acc 93.750000 (98.298387)\n",
      "[5,  3200]  loss 0.033194 (0.053504) train_acc 100.000000 (98.308594)\n",
      "[5,  3300]  loss 0.082807 (0.053551) train_acc 93.750000 (98.312500)\n",
      "[5,  3400]  loss 0.012595 (0.053608) train_acc 100.000000 (98.314338)\n",
      "[5,  3500]  loss 0.051527 (0.053810) train_acc 100.000000 (98.316071)\n",
      "[5,  3600]  loss 0.202635 (0.053640) train_acc 93.750000 (98.319444)\n",
      "[5,  3700]  loss 0.073086 (0.053877) train_acc 93.750000 (98.317568)\n",
      "[6,   100]  loss 0.031480 (0.044938) train_acc 100.000000 (98.312500)\n",
      "[6,   200]  loss 0.006855 (0.040992) train_acc 100.000000 (98.406250)\n",
      "[6,   300]  loss 0.129461 (0.043007) train_acc 93.750000 (98.458333)\n",
      "[6,   400]  loss 0.034964 (0.042297) train_acc 100.000000 (98.531250)\n",
      "[6,   500]  loss 0.016492 (0.045262) train_acc 100.000000 (98.400000)\n",
      "[6,   600]  loss 0.020179 (0.047100) train_acc 100.000000 (98.354167)\n",
      "[6,   700]  loss 0.120629 (0.045755) train_acc 93.750000 (98.383929)\n",
      "[6,   800]  loss 0.005876 (0.046398) train_acc 100.000000 (98.390625)\n",
      "[6,   900]  loss 0.146087 (0.049044) train_acc 93.750000 (98.368056)\n",
      "[6,  1000]  loss 0.006010 (0.050454) train_acc 100.000000 (98.325000)\n",
      "[6,  1100]  loss 0.001789 (0.049887) train_acc 100.000000 (98.352273)\n",
      "[6,  1200]  loss 0.041925 (0.049124) train_acc 100.000000 (98.375000)\n",
      "[6,  1300]  loss 0.001227 (0.048794) train_acc 100.000000 (98.375000)\n",
      "[6,  1400]  loss 0.174077 (0.048565) train_acc 87.500000 (98.397321)\n",
      "[6,  1500]  loss 0.007012 (0.049209) train_acc 100.000000 (98.370833)\n",
      "[6,  1600]  loss 0.015276 (0.049609) train_acc 100.000000 (98.367188)\n",
      "[6,  1700]  loss 0.013807 (0.049612) train_acc 100.000000 (98.367647)\n",
      "[6,  1800]  loss 0.001532 (0.048833) train_acc 100.000000 (98.395833)\n",
      "[6,  1900]  loss 0.000310 (0.047886) train_acc 100.000000 (98.434211)\n",
      "[6,  2000]  loss 0.047101 (0.047620) train_acc 93.750000 (98.450000)\n",
      "[6,  2100]  loss 0.032409 (0.047570) train_acc 100.000000 (98.455357)\n",
      "[6,  2200]  loss 0.001508 (0.046941) train_acc 100.000000 (98.482955)\n",
      "[6,  2300]  loss 0.031657 (0.046506) train_acc 100.000000 (98.500000)\n",
      "[6,  2400]  loss 0.326930 (0.046641) train_acc 93.750000 (98.502604)\n",
      "[6,  2500]  loss 0.055658 (0.046532) train_acc 100.000000 (98.502500)\n",
      "[6,  2600]  loss 0.055650 (0.047001) train_acc 100.000000 (98.492788)\n",
      "[6,  2700]  loss 0.251724 (0.047041) train_acc 93.750000 (98.504630)\n",
      "[6,  2800]  loss 0.096631 (0.047193) train_acc 93.750000 (98.482143)\n",
      "[6,  2900]  loss 0.005214 (0.047228) train_acc 100.000000 (98.474138)\n",
      "[6,  3000]  loss 0.004435 (0.047023) train_acc 100.000000 (98.485417)\n",
      "[6,  3100]  loss 0.126187 (0.047213) train_acc 93.750000 (98.475806)\n",
      "[6,  3200]  loss 0.000712 (0.046973) train_acc 100.000000 (98.486328)\n",
      "[6,  3300]  loss 0.002579 (0.046668) train_acc 100.000000 (98.501894)\n",
      "[6,  3400]  loss 0.013054 (0.046419) train_acc 100.000000 (98.509191)\n",
      "[6,  3500]  loss 0.097415 (0.045944) train_acc 93.750000 (98.525000)\n",
      "[6,  3600]  loss 0.138924 (0.045999) train_acc 93.750000 (98.513889)\n",
      "[6,  3700]  loss 0.013198 (0.046284) train_acc 100.000000 (98.501689)\n",
      "[7,   100]  loss 0.028777 (0.039531) train_acc 100.000000 (98.562500)\n",
      "[7,   200]  loss 0.020502 (0.042294) train_acc 100.000000 (98.656250)\n",
      "[7,   300]  loss 0.006370 (0.044307) train_acc 100.000000 (98.708333)\n",
      "[7,   400]  loss 0.360698 (0.047583) train_acc 87.500000 (98.531250)\n",
      "[7,   500]  loss 0.008157 (0.045975) train_acc 100.000000 (98.575000)\n",
      "[7,   600]  loss 0.011901 (0.044295) train_acc 100.000000 (98.604167)\n",
      "[7,   700]  loss 0.062374 (0.043580) train_acc 100.000000 (98.580357)\n",
      "[7,   800]  loss 0.046129 (0.043612) train_acc 100.000000 (98.593750)\n",
      "[7,   900]  loss 0.007720 (0.042897) train_acc 100.000000 (98.631944)\n",
      "[7,  1000]  loss 0.009191 (0.044735) train_acc 100.000000 (98.581250)\n",
      "[7,  1100]  loss 0.017207 (0.044911) train_acc 100.000000 (98.573864)\n",
      "[7,  1200]  loss 0.003286 (0.045402) train_acc 100.000000 (98.552083)\n",
      "[7,  1300]  loss 0.002041 (0.045147) train_acc 100.000000 (98.567308)\n",
      "[7,  1400]  loss 0.028916 (0.044462) train_acc 100.000000 (98.580357)\n",
      "[7,  1500]  loss 0.077816 (0.044766) train_acc 93.750000 (98.558333)\n",
      "[7,  1600]  loss 0.014069 (0.045239) train_acc 100.000000 (98.542969)\n",
      "[7,  1700]  loss 0.038532 (0.045264) train_acc 100.000000 (98.562500)\n",
      "[7,  1800]  loss 0.003257 (0.044835) train_acc 100.000000 (98.565972)\n",
      "[7,  1900]  loss 0.009277 (0.044119) train_acc 100.000000 (98.565789)\n",
      "[7,  2000]  loss 0.288360 (0.044035) train_acc 93.750000 (98.578125)\n",
      "[7,  2100]  loss 0.053361 (0.043759) train_acc 93.750000 (98.589286)\n",
      "[7,  2200]  loss 0.307081 (0.043502) train_acc 93.750000 (98.582386)\n",
      "[7,  2300]  loss 0.005373 (0.043144) train_acc 100.000000 (98.592391)\n",
      "[7,  2400]  loss 0.127438 (0.043115) train_acc 93.750000 (98.606771)\n",
      "[7,  2500]  loss 0.008146 (0.042450) train_acc 100.000000 (98.627500)\n",
      "[7,  2600]  loss 0.230054 (0.042530) train_acc 87.500000 (98.625000)\n",
      "[7,  2700]  loss 0.000478 (0.042069) train_acc 100.000000 (98.650463)\n",
      "[7,  2800]  loss 0.000197 (0.041639) train_acc 100.000000 (98.654018)\n",
      "[7,  2900]  loss 0.001461 (0.041002) train_acc 100.000000 (98.672414)\n",
      "[7,  3000]  loss 0.001954 (0.040989) train_acc 100.000000 (98.681250)\n",
      "[7,  3100]  loss 0.006768 (0.040724) train_acc 100.000000 (98.695565)\n",
      "[7,  3200]  loss 0.026118 (0.040094) train_acc 100.000000 (98.718750)\n",
      "[7,  3300]  loss 0.016707 (0.040337) train_acc 100.000000 (98.714015)\n",
      "[7,  3400]  loss 0.048715 (0.040634) train_acc 100.000000 (98.709559)\n",
      "[7,  3500]  loss 0.021359 (0.040334) train_acc 100.000000 (98.716071)\n",
      "[7,  3600]  loss 0.002462 (0.040297) train_acc 100.000000 (98.717014)\n",
      "[7,  3700]  loss 0.001981 (0.040357) train_acc 100.000000 (98.717905)\n",
      "[8,   100]  loss 0.003858 (0.041516) train_acc 100.000000 (98.812500)\n",
      "[8,   200]  loss 0.048004 (0.036520) train_acc 100.000000 (99.000000)\n",
      "[8,   300]  loss 0.008629 (0.036454) train_acc 100.000000 (98.937500)\n",
      "[8,   400]  loss 0.003257 (0.037579) train_acc 100.000000 (98.875000)\n",
      "[8,   500]  loss 0.035217 (0.035677) train_acc 100.000000 (98.987500)\n",
      "[8,   600]  loss 0.018885 (0.036437) train_acc 100.000000 (98.947917)\n",
      "[8,   700]  loss 0.012098 (0.036633) train_acc 100.000000 (98.928571)\n",
      "[8,   800]  loss 0.062707 (0.036425) train_acc 93.750000 (98.921875)\n",
      "[8,   900]  loss 0.006420 (0.035207) train_acc 100.000000 (98.965278)\n",
      "[8,  1000]  loss 0.007502 (0.036792) train_acc 100.000000 (98.912500)\n",
      "[8,  1100]  loss 0.011685 (0.036626) train_acc 100.000000 (98.903409)\n",
      "[8,  1200]  loss 0.001136 (0.035929) train_acc 100.000000 (98.942708)\n",
      "[8,  1300]  loss 0.004699 (0.036380) train_acc 100.000000 (98.918269)\n",
      "[8,  1400]  loss 0.000593 (0.035520) train_acc 100.000000 (98.941964)\n",
      "[8,  1500]  loss 0.045202 (0.035059) train_acc 100.000000 (98.950000)\n",
      "[8,  1600]  loss 0.057566 (0.035539) train_acc 100.000000 (98.921875)\n",
      "[8,  1700]  loss 0.016146 (0.035921) train_acc 100.000000 (98.915441)\n",
      "[8,  1800]  loss 0.004274 (0.035689) train_acc 100.000000 (98.930556)\n",
      "[8,  1900]  loss 0.000402 (0.035843) train_acc 100.000000 (98.934211)\n",
      "[8,  2000]  loss 0.005876 (0.035417) train_acc 100.000000 (98.946875)\n",
      "[8,  2100]  loss 0.004616 (0.035120) train_acc 100.000000 (98.961310)\n",
      "[8,  2200]  loss 0.017861 (0.034955) train_acc 100.000000 (98.960227)\n",
      "[8,  2300]  loss 0.008955 (0.035007) train_acc 100.000000 (98.945652)\n",
      "[8,  2400]  loss 0.000664 (0.034964) train_acc 100.000000 (98.940104)\n",
      "[8,  2500]  loss 0.000802 (0.035545) train_acc 100.000000 (98.932500)\n",
      "[8,  2600]  loss 0.005097 (0.035988) train_acc 100.000000 (98.925481)\n",
      "[8,  2700]  loss 0.113910 (0.036494) train_acc 93.750000 (98.916667)\n",
      "[8,  2800]  loss 0.043169 (0.035981) train_acc 100.000000 (98.928571)\n",
      "[8,  2900]  loss 0.001587 (0.036039) train_acc 100.000000 (98.915948)\n",
      "[8,  3000]  loss 0.003208 (0.035803) train_acc 100.000000 (98.916667)\n",
      "[8,  3100]  loss 0.001312 (0.036052) train_acc 100.000000 (98.909274)\n",
      "[8,  3200]  loss 0.009712 (0.036384) train_acc 100.000000 (98.898438)\n",
      "[8,  3300]  loss 0.000614 (0.036393) train_acc 100.000000 (98.890152)\n",
      "[8,  3400]  loss 0.000652 (0.036482) train_acc 100.000000 (98.878676)\n",
      "[8,  3500]  loss 0.010883 (0.036769) train_acc 100.000000 (98.862500)\n",
      "[8,  3600]  loss 0.033341 (0.036682) train_acc 100.000000 (98.861111)\n",
      "[8,  3700]  loss 0.039950 (0.036584) train_acc 100.000000 (98.866554)\n",
      "[9,   100]  loss 0.097408 (0.023583) train_acc 93.750000 (99.000000)\n",
      "[9,   200]  loss 0.014326 (0.026936) train_acc 100.000000 (99.093750)\n",
      "[9,   300]  loss 0.007894 (0.026706) train_acc 100.000000 (99.041667)\n",
      "[9,   400]  loss 0.032792 (0.027433) train_acc 100.000000 (99.000000)\n",
      "[9,   500]  loss 0.041610 (0.027068) train_acc 100.000000 (99.012500)\n",
      "[9,   600]  loss 0.001539 (0.027054) train_acc 100.000000 (99.020833)\n",
      "[9,   700]  loss 0.041824 (0.029055) train_acc 100.000000 (99.017857)\n",
      "[9,   800]  loss 0.032922 (0.029729) train_acc 100.000000 (99.015625)\n",
      "[9,   900]  loss 0.000206 (0.029008) train_acc 100.000000 (99.020833)\n",
      "[9,  1000]  loss 0.000780 (0.030069) train_acc 100.000000 (99.012500)\n",
      "[9,  1100]  loss 0.001812 (0.030618) train_acc 100.000000 (99.028409)\n",
      "[9,  1200]  loss 0.012385 (0.031001) train_acc 100.000000 (99.010417)\n",
      "[9,  1300]  loss 0.001378 (0.031635) train_acc 100.000000 (98.985577)\n",
      "[9,  1400]  loss 0.049381 (0.031291) train_acc 100.000000 (99.000000)\n",
      "[9,  1500]  loss 0.059941 (0.031592) train_acc 100.000000 (98.979167)\n",
      "[9,  1600]  loss 0.010728 (0.032182) train_acc 100.000000 (98.953125)\n",
      "[9,  1700]  loss 0.267878 (0.032240) train_acc 93.750000 (98.959559)\n",
      "[9,  1800]  loss 0.001031 (0.032087) train_acc 100.000000 (98.968750)\n",
      "[9,  1900]  loss 0.004871 (0.032657) train_acc 100.000000 (98.963816)\n",
      "[9,  2000]  loss 0.008239 (0.033203) train_acc 100.000000 (98.940625)\n",
      "[9,  2100]  loss 0.181564 (0.032787) train_acc 93.750000 (98.952381)\n",
      "[9,  2200]  loss 0.000583 (0.032726) train_acc 100.000000 (98.940341)\n",
      "[9,  2300]  loss 0.000464 (0.032751) train_acc 100.000000 (98.945652)\n",
      "[9,  2400]  loss 0.003933 (0.032667) train_acc 100.000000 (98.947917)\n",
      "[9,  2500]  loss 0.008677 (0.032794) train_acc 100.000000 (98.932500)\n",
      "[9,  2600]  loss 0.024527 (0.032890) train_acc 100.000000 (98.932692)\n",
      "[9,  2700]  loss 0.001786 (0.032910) train_acc 100.000000 (98.935185)\n",
      "[9,  2800]  loss 0.014083 (0.033001) train_acc 100.000000 (98.930804)\n",
      "[9,  2900]  loss 0.018461 (0.033004) train_acc 100.000000 (98.928879)\n",
      "[9,  3000]  loss 0.020705 (0.032754) train_acc 100.000000 (98.941667)\n",
      "[9,  3100]  loss 0.016069 (0.032622) train_acc 100.000000 (98.949597)\n",
      "[9,  3200]  loss 0.131497 (0.033123) train_acc 93.750000 (98.945312)\n",
      "[9,  3300]  loss 0.000408 (0.033193) train_acc 100.000000 (98.945076)\n",
      "[9,  3400]  loss 0.000136 (0.033106) train_acc 100.000000 (98.950368)\n",
      "[9,  3500]  loss 0.055816 (0.032924) train_acc 93.750000 (98.957143)\n",
      "[9,  3600]  loss 0.189593 (0.032857) train_acc 93.750000 (98.960069)\n",
      "[9,  3700]  loss 0.014076 (0.033110) train_acc 100.000000 (98.949324)\n",
      "[10,   100]  loss 0.000454 (0.024520) train_acc 100.000000 (99.187500)\n",
      "[10,   200]  loss 0.002137 (0.025053) train_acc 100.000000 (99.250000)\n",
      "[10,   300]  loss 0.028060 (0.025723) train_acc 100.000000 (99.312500)\n",
      "[10,   400]  loss 0.006695 (0.022438) train_acc 100.000000 (99.406250)\n",
      "[10,   500]  loss 0.412910 (0.023712) train_acc 93.750000 (99.362500)\n",
      "[10,   600]  loss 0.084521 (0.026030) train_acc 93.750000 (99.260417)\n",
      "[10,   700]  loss 0.007549 (0.027154) train_acc 100.000000 (99.160714)\n",
      "[10,   800]  loss 0.003696 (0.026415) train_acc 100.000000 (99.187500)\n",
      "[10,   900]  loss 0.004221 (0.026032) train_acc 100.000000 (99.194444)\n",
      "[10,  1000]  loss 0.126158 (0.026384) train_acc 93.750000 (99.162500)\n",
      "[10,  1100]  loss 0.001594 (0.026730) train_acc 100.000000 (99.147727)\n",
      "[10,  1200]  loss 0.008185 (0.026460) train_acc 100.000000 (99.161458)\n",
      "[10,  1300]  loss 0.000607 (0.026155) train_acc 100.000000 (99.173077)\n",
      "[10,  1400]  loss 0.001084 (0.025946) train_acc 100.000000 (99.169643)\n",
      "[10,  1500]  loss 0.014232 (0.026184) train_acc 100.000000 (99.170833)\n",
      "[10,  1600]  loss 0.004594 (0.026488) train_acc 100.000000 (99.179688)\n",
      "[10,  1700]  loss 0.017294 (0.026094) train_acc 100.000000 (99.194853)\n",
      "[10,  1800]  loss 0.087444 (0.026784) train_acc 93.750000 (99.173611)\n",
      "[10,  1900]  loss 0.000401 (0.026872) train_acc 100.000000 (99.164474)\n",
      "[10,  2000]  loss 0.000108 (0.027241) train_acc 100.000000 (99.146875)\n",
      "[10,  2100]  loss 0.038204 (0.027272) train_acc 100.000000 (99.145833)\n",
      "[10,  2200]  loss 0.000543 (0.027038) train_acc 100.000000 (99.139205)\n",
      "[10,  2300]  loss 0.392423 (0.027158) train_acc 93.750000 (99.138587)\n",
      "[10,  2400]  loss 0.075624 (0.027232) train_acc 93.750000 (99.138021)\n",
      "[10,  2500]  loss 0.001575 (0.027279) train_acc 100.000000 (99.135000)\n",
      "[10,  2600]  loss 0.006266 (0.027473) train_acc 100.000000 (99.146635)\n",
      "[10,  2700]  loss 0.044605 (0.027857) train_acc 93.750000 (99.134259)\n",
      "[10,  2800]  loss 0.001324 (0.028141) train_acc 100.000000 (99.127232)\n",
      "[10,  2900]  loss 0.002500 (0.028443) train_acc 100.000000 (99.118534)\n",
      "[10,  3000]  loss 0.051763 (0.028789) train_acc 93.750000 (99.112500)\n",
      "[10,  3100]  loss 0.034571 (0.029066) train_acc 100.000000 (99.096774)\n",
      "[10,  3200]  loss 0.018281 (0.029025) train_acc 100.000000 (99.101562)\n",
      "[10,  3300]  loss 0.002826 (0.029044) train_acc 100.000000 (99.100379)\n",
      "[10,  3400]  loss 0.007040 (0.028863) train_acc 100.000000 (99.108456)\n",
      "[10,  3500]  loss 0.009665 (0.028861) train_acc 100.000000 (99.094643)\n",
      "[10,  3600]  loss 0.001587 (0.028879) train_acc 100.000000 (99.093750)\n",
      "[10,  3700]  loss 0.007523 (0.028938) train_acc 100.000000 (99.086149)\n",
      "[11,   100]  loss 0.004242 (0.031581) train_acc 100.000000 (98.812500)\n",
      "[11,   200]  loss 0.001794 (0.030705) train_acc 100.000000 (98.906250)\n",
      "[11,   300]  loss 0.009852 (0.028168) train_acc 100.000000 (99.020833)\n",
      "[11,   400]  loss 0.012289 (0.027938) train_acc 100.000000 (99.078125)\n",
      "[11,   500]  loss 0.129152 (0.027190) train_acc 93.750000 (99.125000)\n",
      "[11,   600]  loss 0.005748 (0.026137) train_acc 100.000000 (99.135417)\n",
      "[11,   700]  loss 0.000290 (0.024765) train_acc 100.000000 (99.196429)\n",
      "[11,   800]  loss 0.245092 (0.024751) train_acc 93.750000 (99.195312)\n",
      "[11,   900]  loss 0.016643 (0.023972) train_acc 100.000000 (99.250000)\n",
      "[11,  1000]  loss 0.017892 (0.025158) train_acc 100.000000 (99.200000)\n",
      "[11,  1100]  loss 0.000961 (0.025790) train_acc 100.000000 (99.187500)\n",
      "[11,  1200]  loss 0.000465 (0.026429) train_acc 100.000000 (99.177083)\n",
      "[11,  1300]  loss 0.011371 (0.027046) train_acc 100.000000 (99.158654)\n",
      "[11,  1400]  loss 0.017592 (0.026226) train_acc 100.000000 (99.169643)\n",
      "[11,  1500]  loss 0.000657 (0.026311) train_acc 100.000000 (99.170833)\n",
      "[11,  1600]  loss 0.002039 (0.026398) train_acc 100.000000 (99.160156)\n",
      "[11,  1700]  loss 0.047500 (0.026931) train_acc 100.000000 (99.158088)\n",
      "[11,  1800]  loss 0.000924 (0.027621) train_acc 100.000000 (99.142361)\n",
      "[11,  1900]  loss 0.026911 (0.027558) train_acc 100.000000 (99.144737)\n",
      "[11,  2000]  loss 0.003569 (0.028145) train_acc 100.000000 (99.134375)\n",
      "[11,  2100]  loss 0.003573 (0.028525) train_acc 100.000000 (99.116071)\n",
      "[11,  2200]  loss 0.000305 (0.028310) train_acc 100.000000 (99.127841)\n",
      "[11,  2300]  loss 0.032418 (0.028479) train_acc 100.000000 (99.138587)\n",
      "[11,  2400]  loss 0.000297 (0.028219) train_acc 100.000000 (99.153646)\n",
      "[11,  2500]  loss 0.006330 (0.027827) train_acc 100.000000 (99.170000)\n",
      "[11,  2600]  loss 0.018857 (0.027525) train_acc 100.000000 (99.180288)\n",
      "[11,  2700]  loss 0.025641 (0.027325) train_acc 100.000000 (99.189815)\n",
      "[11,  2800]  loss 0.000063 (0.027223) train_acc 100.000000 (99.189732)\n",
      "[11,  2900]  loss 0.001545 (0.027194) train_acc 100.000000 (99.185345)\n",
      "[11,  3000]  loss 0.414913 (0.027162) train_acc 93.750000 (99.191667)\n",
      "[11,  3100]  loss 0.000999 (0.026947) train_acc 100.000000 (99.205645)\n",
      "[11,  3200]  loss 0.001808 (0.026737) train_acc 100.000000 (99.208984)\n",
      "[11,  3300]  loss 0.141305 (0.026793) train_acc 93.750000 (99.202652)\n",
      "[11,  3400]  loss 0.001901 (0.026760) train_acc 100.000000 (99.202206)\n",
      "[11,  3500]  loss 0.089411 (0.026694) train_acc 93.750000 (99.200000)\n",
      "[11,  3600]  loss 0.003592 (0.026947) train_acc 100.000000 (99.189236)\n",
      "[11,  3700]  loss 0.079309 (0.026757) train_acc 100.000000 (99.192568)\n",
      "[12,   100]  loss 0.002604 (0.021544) train_acc 100.000000 (99.250000)\n",
      "[12,   200]  loss 0.000208 (0.022383) train_acc 100.000000 (99.312500)\n",
      "[12,   300]  loss 0.001625 (0.019597) train_acc 100.000000 (99.395833)\n",
      "[12,   400]  loss 0.009053 (0.022019) train_acc 100.000000 (99.359375)\n",
      "[12,   500]  loss 0.025681 (0.021472) train_acc 100.000000 (99.362500)\n",
      "[12,   600]  loss 0.016882 (0.021525) train_acc 100.000000 (99.343750)\n",
      "[12,   700]  loss 0.003250 (0.023709) train_acc 100.000000 (99.241071)\n",
      "[12,   800]  loss 0.004721 (0.023146) train_acc 100.000000 (99.242188)\n",
      "[12,   900]  loss 0.001319 (0.023167) train_acc 100.000000 (99.229167)\n",
      "[12,  1000]  loss 0.058960 (0.023031) train_acc 100.000000 (99.237500)\n",
      "[12,  1100]  loss 0.001076 (0.023427) train_acc 100.000000 (99.238636)\n",
      "[12,  1200]  loss 0.000501 (0.024044) train_acc 100.000000 (99.208333)\n",
      "[12,  1300]  loss 0.021164 (0.023682) train_acc 100.000000 (99.221154)\n",
      "[12,  1400]  loss 0.003490 (0.023764) train_acc 100.000000 (99.214286)\n",
      "[12,  1500]  loss 0.010073 (0.023868) train_acc 100.000000 (99.200000)\n",
      "[12,  1600]  loss 0.003216 (0.024711) train_acc 100.000000 (99.187500)\n",
      "[12,  1700]  loss 0.033486 (0.024552) train_acc 100.000000 (99.198529)\n",
      "[12,  1800]  loss 0.158442 (0.024965) train_acc 93.750000 (99.187500)\n",
      "[12,  1900]  loss 0.001873 (0.025468) train_acc 100.000000 (99.171053)\n",
      "[12,  2000]  loss 0.003296 (0.025242) train_acc 100.000000 (99.187500)\n",
      "[12,  2100]  loss 0.003212 (0.025009) train_acc 100.000000 (99.196429)\n",
      "[12,  2200]  loss 0.001750 (0.024979) train_acc 100.000000 (99.187500)\n",
      "[12,  2300]  loss 0.000091 (0.025474) train_acc 100.000000 (99.168478)\n",
      "[12,  2400]  loss 0.025884 (0.025300) train_acc 100.000000 (99.171875)\n",
      "[12,  2500]  loss 0.000466 (0.024630) train_acc 100.000000 (99.195000)\n",
      "[12,  2600]  loss 0.001358 (0.024753) train_acc 100.000000 (99.192308)\n",
      "[12,  2700]  loss 0.001028 (0.024691) train_acc 100.000000 (99.194444)\n",
      "[12,  2800]  loss 0.021366 (0.024646) train_acc 100.000000 (99.194196)\n",
      "[12,  2900]  loss 0.000573 (0.024320) train_acc 100.000000 (99.209052)\n",
      "[12,  3000]  loss 0.001847 (0.024497) train_acc 100.000000 (99.208333)\n",
      "[12,  3100]  loss 0.010636 (0.024585) train_acc 100.000000 (99.203629)\n",
      "[12,  3200]  loss 0.004573 (0.024882) train_acc 100.000000 (99.199219)\n",
      "[12,  3300]  loss 0.000617 (0.024703) train_acc 100.000000 (99.210227)\n",
      "[12,  3400]  loss 0.009586 (0.024781) train_acc 100.000000 (99.215074)\n",
      "[12,  3500]  loss 0.030132 (0.024501) train_acc 100.000000 (99.225000)\n",
      "[12,  3600]  loss 0.002383 (0.024258) train_acc 100.000000 (99.232639)\n",
      "[12,  3700]  loss 0.001162 (0.024089) train_acc 100.000000 (99.236486)\n",
      "[13,   100]  loss 0.001005 (0.015784) train_acc 100.000000 (99.500000)\n",
      "[13,   200]  loss 0.003761 (0.017144) train_acc 100.000000 (99.437500)\n",
      "[13,   300]  loss 0.000193 (0.018629) train_acc 100.000000 (99.500000)\n",
      "[13,   400]  loss 0.009678 (0.016615) train_acc 100.000000 (99.578125)\n",
      "[13,   500]  loss 0.000365 (0.016895) train_acc 100.000000 (99.600000)\n",
      "[13,   600]  loss 0.000736 (0.019847) train_acc 100.000000 (99.500000)\n",
      "[13,   700]  loss 0.001081 (0.018630) train_acc 100.000000 (99.526786)\n",
      "[13,   800]  loss 0.025691 (0.020411) train_acc 100.000000 (99.460938)\n",
      "[13,   900]  loss 0.000702 (0.020786) train_acc 100.000000 (99.444444)\n",
      "[13,  1000]  loss 0.001477 (0.020775) train_acc 100.000000 (99.437500)\n",
      "[13,  1100]  loss 0.120684 (0.020384) train_acc 93.750000 (99.448864)\n",
      "[13,  1200]  loss 0.001144 (0.020396) train_acc 100.000000 (99.437500)\n",
      "[13,  1300]  loss 0.023273 (0.020087) train_acc 100.000000 (99.447115)\n",
      "[13,  1400]  loss 0.029538 (0.019939) train_acc 100.000000 (99.446429)\n",
      "[13,  1500]  loss 0.021515 (0.020233) train_acc 100.000000 (99.437500)\n",
      "[13,  1600]  loss 0.070895 (0.020261) train_acc 93.750000 (99.437500)\n",
      "[13,  1700]  loss 0.000278 (0.020249) train_acc 100.000000 (99.430147)\n",
      "[13,  1800]  loss 0.101919 (0.020232) train_acc 93.750000 (99.427083)\n",
      "[13,  1900]  loss 0.000898 (0.020395) train_acc 100.000000 (99.404605)\n",
      "[13,  2000]  loss 0.014575 (0.020528) train_acc 100.000000 (99.390625)\n",
      "[13,  2100]  loss 0.001344 (0.021072) train_acc 100.000000 (99.360119)\n",
      "[13,  2200]  loss 0.001550 (0.021565) train_acc 100.000000 (99.332386)\n",
      "[13,  2300]  loss 0.000140 (0.021396) train_acc 100.000000 (99.334239)\n",
      "[13,  2400]  loss 0.000175 (0.021426) train_acc 100.000000 (99.328125)\n",
      "[13,  2500]  loss 0.064578 (0.021218) train_acc 93.750000 (99.335000)\n",
      "[13,  2600]  loss 0.021316 (0.021164) train_acc 100.000000 (99.341346)\n",
      "[13,  2700]  loss 0.001390 (0.021561) train_acc 100.000000 (99.333333)\n",
      "[13,  2800]  loss 0.021853 (0.021823) train_acc 100.000000 (99.314732)\n",
      "[13,  2900]  loss 0.002650 (0.021719) train_acc 100.000000 (99.321121)\n",
      "[13,  3000]  loss 0.029950 (0.021634) train_acc 100.000000 (99.320833)\n",
      "[13,  3100]  loss 0.000492 (0.021670) train_acc 100.000000 (99.314516)\n",
      "[13,  3200]  loss 0.036941 (0.021481) train_acc 100.000000 (99.322266)\n",
      "[13,  3300]  loss 0.055260 (0.021582) train_acc 100.000000 (99.320076)\n",
      "[13,  3400]  loss 0.004999 (0.021874) train_acc 100.000000 (99.319853)\n",
      "[13,  3500]  loss 0.030885 (0.021809) train_acc 100.000000 (99.321429)\n",
      "[13,  3600]  loss 0.133135 (0.022082) train_acc 93.750000 (99.314236)\n",
      "[13,  3700]  loss 0.004790 (0.022203) train_acc 100.000000 (99.305743)\n",
      "[14,   100]  loss 0.000244 (0.020762) train_acc 100.000000 (99.312500)\n",
      "[14,   200]  loss 0.001304 (0.017745) train_acc 100.000000 (99.312500)\n",
      "[14,   300]  loss 0.021478 (0.020407) train_acc 100.000000 (99.270833)\n",
      "[14,   400]  loss 0.000297 (0.019562) train_acc 100.000000 (99.265625)\n",
      "[14,   500]  loss 0.051505 (0.017478) train_acc 93.750000 (99.337500)\n",
      "[14,   600]  loss 0.000290 (0.017575) train_acc 100.000000 (99.364583)\n",
      "[14,   700]  loss 0.011702 (0.018375) train_acc 100.000000 (99.339286)\n",
      "[14,   800]  loss 0.018175 (0.017433) train_acc 100.000000 (99.398438)\n",
      "[14,   900]  loss 0.040003 (0.017340) train_acc 100.000000 (99.430556)\n",
      "[14,  1000]  loss 0.015336 (0.017474) train_acc 100.000000 (99.412500)\n",
      "[14,  1100]  loss 0.000699 (0.017146) train_acc 100.000000 (99.426136)\n",
      "[14,  1200]  loss 0.029758 (0.017383) train_acc 100.000000 (99.416667)\n",
      "[14,  1300]  loss 1.069211 (0.018257) train_acc 93.750000 (99.403846)\n",
      "[14,  1400]  loss 0.001533 (0.018304) train_acc 100.000000 (99.401786)\n",
      "[14,  1500]  loss 0.000741 (0.017759) train_acc 100.000000 (99.437500)\n",
      "[14,  1600]  loss 0.001173 (0.018501) train_acc 100.000000 (99.410156)\n",
      "[14,  1700]  loss 0.003997 (0.018985) train_acc 100.000000 (99.393382)\n",
      "[14,  1800]  loss 0.000057 (0.018604) train_acc 100.000000 (99.399306)\n",
      "[14,  1900]  loss 0.000557 (0.018332) train_acc 100.000000 (99.407895)\n",
      "[14,  2000]  loss 0.000157 (0.018169) train_acc 100.000000 (99.409375)\n",
      "[14,  2100]  loss 0.048667 (0.018042) train_acc 100.000000 (99.419643)\n",
      "[14,  2200]  loss 0.000604 (0.017971) train_acc 100.000000 (99.423295)\n",
      "[14,  2300]  loss 0.128540 (0.018385) train_acc 93.750000 (99.407609)\n",
      "[14,  2400]  loss 0.000317 (0.018511) train_acc 100.000000 (99.398438)\n",
      "[14,  2500]  loss 0.001253 (0.018766) train_acc 100.000000 (99.380000)\n",
      "[14,  2600]  loss 0.001688 (0.018385) train_acc 100.000000 (99.394231)\n",
      "[14,  2700]  loss 0.027591 (0.018874) train_acc 100.000000 (99.386574)\n",
      "[14,  2800]  loss 0.003286 (0.019232) train_acc 100.000000 (99.386161)\n",
      "[14,  2900]  loss 0.003954 (0.019403) train_acc 100.000000 (99.383621)\n",
      "[14,  3000]  loss 0.000660 (0.019367) train_acc 100.000000 (99.385417)\n",
      "[14,  3100]  loss 0.250484 (0.019281) train_acc 93.750000 (99.391129)\n",
      "[14,  3200]  loss 0.001577 (0.019340) train_acc 100.000000 (99.382812)\n",
      "[14,  3300]  loss 0.013560 (0.019633) train_acc 100.000000 (99.373106)\n",
      "[14,  3400]  loss 0.000634 (0.019697) train_acc 100.000000 (99.375000)\n",
      "[14,  3500]  loss 0.007967 (0.019633) train_acc 100.000000 (99.383929)\n",
      "[14,  3600]  loss 0.276244 (0.019918) train_acc 87.500000 (99.375000)\n",
      "[14,  3700]  loss 0.001941 (0.019896) train_acc 100.000000 (99.369932)\n",
      "[15,   100]  loss 0.002857 (0.012389) train_acc 100.000000 (99.562500)\n",
      "[15,   200]  loss 0.004914 (0.012024) train_acc 100.000000 (99.625000)\n",
      "[15,   300]  loss 0.006305 (0.014230) train_acc 100.000000 (99.541667)\n",
      "[15,   400]  loss 0.006859 (0.014069) train_acc 100.000000 (99.562500)\n",
      "[15,   500]  loss 0.000468 (0.015723) train_acc 100.000000 (99.512500)\n",
      "[15,   600]  loss 0.000043 (0.014856) train_acc 100.000000 (99.541667)\n",
      "[15,   700]  loss 0.021447 (0.014622) train_acc 100.000000 (99.553571)\n",
      "[15,   800]  loss 0.000257 (0.015295) train_acc 100.000000 (99.554688)\n",
      "[15,   900]  loss 0.000529 (0.014998) train_acc 100.000000 (99.548611)\n",
      "[15,  1000]  loss 0.000080 (0.015036) train_acc 100.000000 (99.518750)\n",
      "[15,  1100]  loss 0.000231 (0.015234) train_acc 100.000000 (99.511364)\n",
      "[15,  1200]  loss 0.000285 (0.015816) train_acc 100.000000 (99.505208)\n",
      "[15,  1300]  loss 0.004411 (0.016340) train_acc 100.000000 (99.495192)\n",
      "[15,  1400]  loss 0.001302 (0.016609) train_acc 100.000000 (99.491071)\n",
      "[15,  1500]  loss 0.000157 (0.016672) train_acc 100.000000 (99.479167)\n",
      "[15,  1600]  loss 0.000102 (0.016258) train_acc 100.000000 (99.503906)\n",
      "[15,  1700]  loss 0.002538 (0.016289) train_acc 100.000000 (99.481618)\n",
      "[15,  1800]  loss 0.000639 (0.016788) train_acc 100.000000 (99.451389)\n",
      "[15,  1900]  loss 0.002343 (0.016845) train_acc 100.000000 (99.444079)\n",
      "[15,  2000]  loss 0.000165 (0.016838) train_acc 100.000000 (99.446875)\n",
      "[15,  2100]  loss 0.001181 (0.016989) train_acc 100.000000 (99.440476)\n",
      "[15,  2200]  loss 0.004210 (0.017118) train_acc 100.000000 (99.437500)\n",
      "[15,  2300]  loss 0.000783 (0.017449) train_acc 100.000000 (99.423913)\n",
      "[15,  2400]  loss 0.000722 (0.017253) train_acc 100.000000 (99.424479)\n",
      "[15,  2500]  loss 0.024341 (0.017540) train_acc 100.000000 (99.425000)\n",
      "[15,  2600]  loss 0.032762 (0.017581) train_acc 100.000000 (99.423077)\n",
      "[15,  2700]  loss 0.000395 (0.017960) train_acc 100.000000 (99.416667)\n",
      "[15,  2800]  loss 0.000328 (0.017988) train_acc 100.000000 (99.419643)\n",
      "[15,  2900]  loss 0.000955 (0.017840) train_acc 100.000000 (99.426724)\n",
      "[15,  3000]  loss 0.008877 (0.017752) train_acc 100.000000 (99.422917)\n",
      "[15,  3100]  loss 0.012480 (0.017894) train_acc 100.000000 (99.425403)\n",
      "[15,  3200]  loss 0.001401 (0.018221) train_acc 100.000000 (99.412109)\n",
      "[15,  3300]  loss 0.010500 (0.018288) train_acc 100.000000 (99.414773)\n",
      "[15,  3400]  loss 0.020122 (0.018482) train_acc 100.000000 (99.411765)\n",
      "[15,  3500]  loss 0.000353 (0.018667) train_acc 100.000000 (99.407143)\n",
      "[15,  3600]  loss 0.105677 (0.018665) train_acc 93.750000 (99.406250)\n",
      "[15,  3700]  loss 0.001065 (0.018988) train_acc 100.000000 (99.396959)\n",
      "[16,   100]  loss 0.004971 (0.013743) train_acc 100.000000 (99.500000)\n",
      "[16,   200]  loss 0.010398 (0.015056) train_acc 100.000000 (99.500000)\n",
      "[16,   300]  loss 0.080173 (0.019858) train_acc 93.750000 (99.354167)\n",
      "[16,   400]  loss 0.006644 (0.016823) train_acc 100.000000 (99.484375)\n",
      "[16,   500]  loss 0.000356 (0.016264) train_acc 100.000000 (99.500000)\n",
      "[16,   600]  loss 0.000209 (0.016019) train_acc 100.000000 (99.510417)\n",
      "[16,   700]  loss 0.039045 (0.015874) train_acc 100.000000 (99.535714)\n",
      "[16,   800]  loss 0.002125 (0.015356) train_acc 100.000000 (99.554688)\n",
      "[16,   900]  loss 0.045951 (0.014980) train_acc 100.000000 (99.583333)\n",
      "[16,  1000]  loss 0.000706 (0.015464) train_acc 100.000000 (99.562500)\n",
      "[16,  1100]  loss 0.003025 (0.015573) train_acc 100.000000 (99.545455)\n",
      "[16,  1200]  loss 0.004267 (0.015712) train_acc 100.000000 (99.510417)\n",
      "[16,  1300]  loss 0.009624 (0.015617) train_acc 100.000000 (99.514423)\n",
      "[16,  1400]  loss 0.017465 (0.015365) train_acc 100.000000 (99.513393)\n",
      "[16,  1500]  loss 0.001830 (0.015324) train_acc 100.000000 (99.525000)\n",
      "[16,  1600]  loss 0.001672 (0.015321) train_acc 100.000000 (99.531250)\n",
      "[16,  1700]  loss 0.007484 (0.015753) train_acc 100.000000 (99.507353)\n",
      "[16,  1800]  loss 0.001302 (0.015839) train_acc 100.000000 (99.503472)\n",
      "[16,  1900]  loss 0.025452 (0.015852) train_acc 100.000000 (99.503289)\n",
      "[16,  2000]  loss 0.002044 (0.015935) train_acc 100.000000 (99.503125)\n",
      "[16,  2100]  loss 0.000302 (0.015804) train_acc 100.000000 (99.502976)\n",
      "[16,  2200]  loss 0.004377 (0.015700) train_acc 100.000000 (99.508523)\n",
      "[16,  2300]  loss 0.010303 (0.015749) train_acc 100.000000 (99.510870)\n",
      "[16,  2400]  loss 0.001331 (0.015830) train_acc 100.000000 (99.500000)\n",
      "[16,  2500]  loss 0.000575 (0.015802) train_acc 100.000000 (99.500000)\n",
      "[16,  2600]  loss 0.000296 (0.015859) train_acc 100.000000 (99.500000)\n",
      "[16,  2700]  loss 0.000327 (0.015925) train_acc 100.000000 (99.500000)\n",
      "[16,  2800]  loss 0.017885 (0.015894) train_acc 100.000000 (99.500000)\n",
      "[16,  2900]  loss 0.000247 (0.015868) train_acc 100.000000 (99.495690)\n",
      "[16,  3000]  loss 0.000843 (0.015777) train_acc 100.000000 (99.500000)\n",
      "[16,  3100]  loss 0.002160 (0.015708) train_acc 100.000000 (99.502016)\n",
      "[16,  3200]  loss 0.002732 (0.015976) train_acc 100.000000 (99.503906)\n",
      "[16,  3300]  loss 0.002901 (0.015929) train_acc 100.000000 (99.501894)\n",
      "[16,  3400]  loss 0.010349 (0.016013) train_acc 100.000000 (99.496324)\n",
      "[16,  3500]  loss 0.011481 (0.016237) train_acc 100.000000 (99.489286)\n",
      "[16,  3600]  loss 0.007184 (0.016265) train_acc 100.000000 (99.493056)\n",
      "[16,  3700]  loss 0.003117 (0.016517) train_acc 100.000000 (99.486486)\n",
      "[17,   100]  loss 0.000947 (0.026059) train_acc 100.000000 (99.437500)\n",
      "[17,   200]  loss 0.006776 (0.023132) train_acc 100.000000 (99.343750)\n",
      "[17,   300]  loss 0.000147 (0.019420) train_acc 100.000000 (99.395833)\n",
      "[17,   400]  loss 0.000446 (0.017345) train_acc 100.000000 (99.437500)\n",
      "[17,   500]  loss 0.270371 (0.017227) train_acc 93.750000 (99.412500)\n",
      "[17,   600]  loss 0.000536 (0.017204) train_acc 100.000000 (99.416667)\n",
      "[17,   700]  loss 0.007576 (0.017089) train_acc 100.000000 (99.437500)\n",
      "[17,   800]  loss 0.004042 (0.016892) train_acc 100.000000 (99.460938)\n",
      "[17,   900]  loss 0.009018 (0.016411) train_acc 100.000000 (99.465278)\n",
      "[17,  1000]  loss 0.000511 (0.016075) train_acc 100.000000 (99.487500)\n",
      "[17,  1100]  loss 0.000625 (0.015802) train_acc 100.000000 (99.500000)\n",
      "[17,  1200]  loss 0.008577 (0.015627) train_acc 100.000000 (99.500000)\n",
      "[17,  1300]  loss 0.001238 (0.016603) train_acc 100.000000 (99.475962)\n",
      "[17,  1400]  loss 0.017870 (0.016886) train_acc 100.000000 (99.455357)\n",
      "[17,  1500]  loss 0.000174 (0.016311) train_acc 100.000000 (99.479167)\n",
      "[17,  1600]  loss 0.043329 (0.016176) train_acc 100.000000 (99.484375)\n",
      "[17,  1700]  loss 0.005082 (0.015990) train_acc 100.000000 (99.477941)\n",
      "[17,  1800]  loss 0.001265 (0.016141) train_acc 100.000000 (99.465278)\n",
      "[17,  1900]  loss 0.001002 (0.016061) train_acc 100.000000 (99.476974)\n",
      "[17,  2000]  loss 0.000195 (0.016145) train_acc 100.000000 (99.487500)\n",
      "[17,  2100]  loss 0.012274 (0.015915) train_acc 100.000000 (99.500000)\n",
      "[17,  2200]  loss 0.000153 (0.015947) train_acc 100.000000 (99.488636)\n",
      "[17,  2300]  loss 0.015441 (0.015934) train_acc 100.000000 (99.486413)\n",
      "[17,  2400]  loss 0.000067 (0.015664) train_acc 100.000000 (99.494792)\n",
      "[17,  2500]  loss 0.000920 (0.015525) train_acc 100.000000 (99.505000)\n",
      "[17,  2600]  loss 0.011841 (0.015429) train_acc 100.000000 (99.516827)\n",
      "[17,  2700]  loss 0.001951 (0.015452) train_acc 100.000000 (99.518519)\n",
      "[17,  2800]  loss 0.001375 (0.015517) train_acc 100.000000 (99.522321)\n",
      "[17,  2900]  loss 0.000198 (0.015349) train_acc 100.000000 (99.532328)\n",
      "[17,  3000]  loss 0.000072 (0.015610) train_acc 100.000000 (99.522917)\n",
      "[17,  3100]  loss 0.000185 (0.015543) train_acc 100.000000 (99.522177)\n",
      "[17,  3200]  loss 0.014614 (0.015522) train_acc 100.000000 (99.521484)\n",
      "[17,  3300]  loss 0.000472 (0.015543) train_acc 100.000000 (99.517045)\n",
      "[17,  3400]  loss 0.001667 (0.015742) train_acc 100.000000 (99.505515)\n",
      "[17,  3500]  loss 0.001017 (0.015603) train_acc 100.000000 (99.505357)\n",
      "[17,  3600]  loss 0.004825 (0.015614) train_acc 100.000000 (99.508681)\n",
      "[17,  3700]  loss 0.001096 (0.015472) train_acc 100.000000 (99.515203)\n",
      "[18,   100]  loss 0.020988 (0.010916) train_acc 100.000000 (99.625000)\n",
      "[18,   200]  loss 0.009543 (0.013942) train_acc 100.000000 (99.531250)\n",
      "[18,   300]  loss 0.034579 (0.014277) train_acc 100.000000 (99.541667)\n",
      "[18,   400]  loss 0.000180 (0.013307) train_acc 100.000000 (99.578125)\n",
      "[18,   500]  loss 0.001722 (0.012618) train_acc 100.000000 (99.600000)\n",
      "[18,   600]  loss 0.010143 (0.012778) train_acc 100.000000 (99.583333)\n",
      "[18,   700]  loss 0.006878 (0.013273) train_acc 100.000000 (99.598214)\n",
      "[18,   800]  loss 0.002873 (0.013137) train_acc 100.000000 (99.617188)\n",
      "[18,   900]  loss 0.005610 (0.012433) train_acc 100.000000 (99.645833)\n",
      "[18,  1000]  loss 0.000415 (0.013016) train_acc 100.000000 (99.631250)\n",
      "[18,  1100]  loss 0.066613 (0.014052) train_acc 100.000000 (99.585227)\n",
      "[18,  1200]  loss 0.031723 (0.014487) train_acc 100.000000 (99.557292)\n",
      "[18,  1300]  loss 0.018808 (0.014184) train_acc 100.000000 (99.576923)\n",
      "[18,  1400]  loss 0.000406 (0.014306) train_acc 100.000000 (99.571429)\n",
      "[18,  1500]  loss 0.001248 (0.013907) train_acc 100.000000 (99.579167)\n",
      "[18,  1600]  loss 0.002335 (0.013861) train_acc 100.000000 (99.585938)\n",
      "[18,  1700]  loss 0.000266 (0.013965) train_acc 100.000000 (99.573529)\n",
      "[18,  1800]  loss 0.000205 (0.014251) train_acc 100.000000 (99.562500)\n",
      "[18,  1900]  loss 0.004410 (0.014238) train_acc 100.000000 (99.565789)\n",
      "[18,  2000]  loss 0.002816 (0.013961) train_acc 100.000000 (99.575000)\n",
      "[18,  2100]  loss 0.002535 (0.013847) train_acc 100.000000 (99.580357)\n",
      "[18,  2200]  loss 0.000190 (0.013529) train_acc 100.000000 (99.588068)\n",
      "[18,  2300]  loss 0.002259 (0.013618) train_acc 100.000000 (99.584239)\n",
      "[18,  2400]  loss 0.001821 (0.014048) train_acc 100.000000 (99.583333)\n",
      "[18,  2500]  loss 0.267059 (0.014291) train_acc 93.750000 (99.575000)\n",
      "[18,  2600]  loss 0.002151 (0.014176) train_acc 100.000000 (99.574519)\n",
      "[18,  2700]  loss 0.090297 (0.014317) train_acc 93.750000 (99.564815)\n",
      "[18,  2800]  loss 0.003607 (0.014463) train_acc 100.000000 (99.558036)\n",
      "[18,  2900]  loss 0.000166 (0.014772) train_acc 100.000000 (99.547414)\n",
      "[18,  3000]  loss 0.030957 (0.014659) train_acc 100.000000 (99.552083)\n",
      "[18,  3100]  loss 0.023828 (0.014549) train_acc 100.000000 (99.560484)\n",
      "[18,  3200]  loss 0.000083 (0.014469) train_acc 100.000000 (99.564453)\n",
      "[18,  3300]  loss 0.005690 (0.014453) train_acc 100.000000 (99.570076)\n",
      "[18,  3400]  loss 0.010625 (0.014225) train_acc 100.000000 (99.579044)\n",
      "[18,  3500]  loss 0.060460 (0.014413) train_acc 93.750000 (99.571429)\n",
      "[18,  3600]  loss 0.000112 (0.014271) train_acc 100.000000 (99.576389)\n",
      "[18,  3700]  loss 0.003060 (0.014087) train_acc 100.000000 (99.579392)\n",
      "[19,   100]  loss 0.021400 (0.014594) train_acc 100.000000 (99.625000)\n",
      "[19,   200]  loss 0.001623 (0.011920) train_acc 100.000000 (99.750000)\n",
      "[19,   300]  loss 0.002908 (0.011491) train_acc 100.000000 (99.708333)\n",
      "[19,   400]  loss 0.007958 (0.011011) train_acc 100.000000 (99.703125)\n",
      "[19,   500]  loss 0.001125 (0.010018) train_acc 100.000000 (99.737500)\n",
      "[19,   600]  loss 0.009469 (0.009831) train_acc 100.000000 (99.729167)\n",
      "[19,   700]  loss 0.014253 (0.011500) train_acc 100.000000 (99.696429)\n",
      "[19,   800]  loss 0.005228 (0.012773) train_acc 100.000000 (99.656250)\n",
      "[19,   900]  loss 0.000217 (0.012486) train_acc 100.000000 (99.652778)\n",
      "[19,  1000]  loss 0.000500 (0.012266) train_acc 100.000000 (99.662500)\n",
      "[19,  1100]  loss 0.000348 (0.011952) train_acc 100.000000 (99.664773)\n",
      "[19,  1200]  loss 0.004057 (0.011526) train_acc 100.000000 (99.671875)\n",
      "[19,  1300]  loss 0.003230 (0.011284) train_acc 100.000000 (99.682692)\n",
      "[19,  1400]  loss 0.002642 (0.011410) train_acc 100.000000 (99.678571)\n",
      "[19,  1500]  loss 0.003141 (0.011573) train_acc 100.000000 (99.666667)\n",
      "[19,  1600]  loss 0.005650 (0.011255) train_acc 100.000000 (99.679688)\n",
      "[19,  1700]  loss 0.022063 (0.011433) train_acc 100.000000 (99.676471)\n",
      "[19,  1800]  loss 0.060765 (0.012097) train_acc 93.750000 (99.645833)\n",
      "[19,  1900]  loss 0.001462 (0.012284) train_acc 100.000000 (99.641447)\n",
      "[19,  2000]  loss 0.001209 (0.012225) train_acc 100.000000 (99.643750)\n",
      "[19,  2100]  loss 0.003600 (0.012145) train_acc 100.000000 (99.639881)\n",
      "[19,  2200]  loss 0.018141 (0.012391) train_acc 100.000000 (99.627841)\n",
      "[19,  2300]  loss 0.001689 (0.012582) train_acc 100.000000 (99.627717)\n",
      "[19,  2400]  loss 0.005235 (0.012539) train_acc 100.000000 (99.627604)\n",
      "[19,  2500]  loss 0.022204 (0.012787) train_acc 100.000000 (99.625000)\n",
      "[19,  2600]  loss 0.018629 (0.012910) train_acc 100.000000 (99.625000)\n",
      "[19,  2700]  loss 0.002290 (0.012704) train_acc 100.000000 (99.636574)\n",
      "[19,  2800]  loss 0.003328 (0.012605) train_acc 100.000000 (99.640625)\n",
      "[19,  2900]  loss 0.063219 (0.012547) train_acc 93.750000 (99.640086)\n",
      "[19,  3000]  loss 0.000494 (0.012652) train_acc 100.000000 (99.633333)\n",
      "[19,  3100]  loss 0.005162 (0.012723) train_acc 100.000000 (99.633065)\n",
      "[19,  3200]  loss 0.007474 (0.012903) train_acc 100.000000 (99.625000)\n",
      "[19,  3300]  loss 0.134595 (0.012873) train_acc 93.750000 (99.625000)\n",
      "[19,  3400]  loss 0.005491 (0.012823) train_acc 100.000000 (99.628676)\n",
      "[19,  3500]  loss 0.003195 (0.012714) train_acc 100.000000 (99.632143)\n",
      "[19,  3600]  loss 0.020646 (0.012744) train_acc 100.000000 (99.630208)\n",
      "[19,  3700]  loss 0.000799 (0.012830) train_acc 100.000000 (99.623311)\n",
      "[20,   100]  loss 0.000377 (0.009288) train_acc 100.000000 (99.875000)\n",
      "[20,   200]  loss 0.296435 (0.013485) train_acc 93.750000 (99.718750)\n",
      "[20,   300]  loss 0.000212 (0.011664) train_acc 100.000000 (99.770833)\n",
      "[20,   400]  loss 0.049009 (0.011883) train_acc 100.000000 (99.703125)\n",
      "[20,   500]  loss 0.000147 (0.010586) train_acc 100.000000 (99.762500)\n",
      "[20,   600]  loss 0.045727 (0.010560) train_acc 100.000000 (99.750000)\n",
      "[20,   700]  loss 0.000858 (0.009951) train_acc 100.000000 (99.767857)\n",
      "[20,   800]  loss 0.002418 (0.009555) train_acc 100.000000 (99.789062)\n",
      "[20,   900]  loss 0.050120 (0.009088) train_acc 93.750000 (99.798611)\n",
      "[20,  1000]  loss 0.004006 (0.008775) train_acc 100.000000 (99.818750)\n",
      "[20,  1100]  loss 0.001664 (0.009308) train_acc 100.000000 (99.789773)\n",
      "[20,  1200]  loss 0.000146 (0.010261) train_acc 100.000000 (99.755208)\n",
      "[20,  1300]  loss 0.036537 (0.009912) train_acc 100.000000 (99.759615)\n",
      "[20,  1400]  loss 0.001096 (0.009609) train_acc 100.000000 (99.767857)\n",
      "[20,  1500]  loss 0.006499 (0.009806) train_acc 100.000000 (99.762500)\n",
      "[20,  1600]  loss 0.000909 (0.010598) train_acc 100.000000 (99.738281)\n",
      "[20,  1700]  loss 0.000887 (0.010681) train_acc 100.000000 (99.731618)\n",
      "[20,  1800]  loss 0.007688 (0.010971) train_acc 100.000000 (99.711806)\n",
      "[20,  1900]  loss 0.000151 (0.011022) train_acc 100.000000 (99.703947)\n",
      "[20,  2000]  loss 0.000264 (0.010821) train_acc 100.000000 (99.706250)\n",
      "[20,  2100]  loss 0.030789 (0.010752) train_acc 100.000000 (99.702381)\n",
      "[20,  2200]  loss 0.012715 (0.010814) train_acc 100.000000 (99.693182)\n",
      "[20,  2300]  loss 0.064322 (0.011024) train_acc 93.750000 (99.682065)\n",
      "[20,  2400]  loss 0.008684 (0.010951) train_acc 100.000000 (99.690104)\n",
      "[20,  2500]  loss 0.014372 (0.010956) train_acc 100.000000 (99.690000)\n",
      "[20,  2600]  loss 0.000581 (0.010902) train_acc 100.000000 (99.694712)\n",
      "[20,  2700]  loss 0.100215 (0.011122) train_acc 93.750000 (99.694444)\n",
      "[20,  2800]  loss 0.005149 (0.011164) train_acc 100.000000 (99.687500)\n",
      "[20,  2900]  loss 0.001148 (0.011325) train_acc 100.000000 (99.681034)\n",
      "[20,  3000]  loss 0.006473 (0.011505) train_acc 100.000000 (99.675000)\n",
      "[20,  3100]  loss 0.003606 (0.011317) train_acc 100.000000 (99.679435)\n",
      "[20,  3200]  loss 0.000540 (0.011210) train_acc 100.000000 (99.681641)\n",
      "[20,  3300]  loss 0.000371 (0.011318) train_acc 100.000000 (99.676136)\n",
      "[20,  3400]  loss 0.006049 (0.011332) train_acc 100.000000 (99.678309)\n",
      "[20,  3500]  loss 0.009542 (0.011431) train_acc 100.000000 (99.671429)\n",
      "[20,  3600]  loss 0.001259 (0.011552) train_acc 100.000000 (99.670139)\n",
      "[20,  3700]  loss 0.000741 (0.011602) train_acc 100.000000 (99.667230)\n",
      "Finished training\n",
      "Size of model after quantization\n",
      "Size (MB):  0.05793\n",
      "Accuracy of the fused and quantized network (trianed quantized) on the test images: 98.47% - INT8\n"
     ]
    }
   ],
   "source": [
    "qnet = Net(q=True)\n",
    "\n",
    "fuse_modules(model=qnet)\n",
    "\n",
    "# Specify quantization config for QAT\n",
    "qnet.qconfig = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "\n",
    "# Prepare QAT\n",
    "torch.quantization.prepare_qat(model=qnet, inplace=True)\n",
    "\n",
    "qnet = qnet.to(device)\n",
    "\n",
    "train(model=qnet, dataloader=trainloader, cuda=torch.cuda.is_available(), q=True)\n",
    "\n",
    "qnet = qnet.to(torch.device(\"cpu\"))\n",
    "\n",
    "torch.quantization.convert(module=qnet, inplace=True)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(model=qnet)\n",
    "\n",
    "score = test(model=qnet, dataloader=testloader, cuda=False)\n",
    "print(f\"Accuracy of the fused and quantized network (trianed quantized) on the test images: {score}% - INT8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
